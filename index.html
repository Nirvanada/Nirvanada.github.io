<!doctype html>



  


<html class="theme-next muse use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.1" />






<meta property="og:type" content="website">
<meta property="og:title" content="Andante">
<meta property="og:url" content="https://nirvanada.github.io/index.html">
<meta property="og:site_name" content="Andante">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Andante">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Muse',
    sidebar: {"position":"right","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: '博主'
    }
  };
</script>




  <link rel="canonical" href="https://nirvanada.github.io/"/>

  <title> Andante </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-right 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">Andante</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle"></p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/09/15/AFM/" itemprop="url">
                  未命名
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-09-15T14:07:19+08:00" content="2017-09-15">
              2017-09-15
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>title: Attentional FM及源码解析<br>date: 2017-09-18 18:17:15<br>categories: Machine Learning</p>
<h4 id="原理部分"><a href="#原理部分" class="headerlink" title="原理部分"></a>原理部分</h4><p>书接上回，FM模型无论是对于交叉特征的捕捉能力上，还是在工业界大规模数据下的运用方面上，都具有出色的表现。因此，在深度学习火热的大背景下，各种基于FM模型和神经网络模型相结合的方法也开始频频出现。这里要说的是SIGIR 2017和IJCAI 2017的两篇文章<a href="http://www.comp.nus.edu.sg/~xiangnan/papers/sigir17-nfm.pdf" target="_blank" rel="external">Neural Factorization Machine</a>和<a href="http://www.comp.nus.edu.sg/~xiangnan/papers/ijcai17-afm.pdf，无一例外，它们都是在FM的基础上衍生出与深度学习相关的研究。由于两篇文章的是同一团队所著，所以工作有重叠和改进部分，前一篇网络结构相对简单，因此本文主要针对Attentional Facotrization Machine展开，但也会在其中涉及Neural Factorization Machine。不仅如此，作者也同样给出了开源实现，源码解析也会在此基础上进行（[Neural FM](https://github.com/hexiangnan/attentional_factorization_machine" target="_blank" rel="external">Attentional Facotrization Machine</a> and <a href="https://github.com/hexiangnan/neural_factorization_machine" target="_blank" rel="external">Attention FM</a>）。</p>
<p>FM模型虽然存在了所有可能的二阶特征交叉项，但本质上，并不是所有的特征都是有效的，比如一个垃圾特征a和一个牛逼特征b做交叉可能会起到一些作用，但a和垃圾特征c再做交叉基本上不会有任何作用，因此Attentional Facotrization Machine解决的问题主要就是——利用目前比较流行的attention的概念，对每个特征交叉分配不同的attention，使没用的交叉特征权重降低，有用的交叉特征权重提高，加强模型预测能力。原始的FM模型，如果忽略一阶项，可以表示为</p>
<script type="math/tex; mode=display">
y_{fm} = \sum_i \sum_{j = i + 1} <v_i, v_j> x_i x_j</script><p>这里我们可以把$v_i$理解为特征$x_i$的feature embedding，而上式中交叉项的表达为embedding向量的内积，因此输入的$y$直接就为标量，不需要再次的映射；但是这种内积操作如果变换为哈达玛积的操作（element-wise product）$(v_i \odot v_j) x_i x_j$，这样从Neural Network的角度去看，每个特征都交叉之后上层就变成了一个embedding layer，然后再对这个layer做一次sum pooling和映射，便可以得到输出值，即</p>
<script type="math/tex; mode=display">
y = p^T\sum_i \sum_{j = i + 1} (v_i \odot v_j) x_i x_j</script><p>至此，Neural Factorization Machine中所描述的已经叙述完毕，就是这么简单。而来到了Attention这篇，思想一致，只是在embedding layer的sum pooling时为每个embedding加上对应的attention，也就是权重，即</p>
<script type="math/tex; mode=display">
y = p^T\sum_i \sum_{j = i + 1} \alpha_{ij}(v_i \odot v_j) x_i x_j</script><p>其中$\alpha$通过对embedding空间做了一次encoding并标准化得到，</p>
<script type="math/tex; mode=display">
\alpha_{ij}^{'} = h^TReLU(W(v_i \odot v_j) x_i x_j + b) \\
\alpha_{ij} = \frac{exp(\alpha_{ij}^{'})}{\sum exp(\alpha_{ij}^{'})}</script><p>另外，由于这种attention的方式对训练数据的拟合表达更充分，但也更容易过拟合，因此作者除了在loss中加入正则项之外，在attention部分加入了dropout。具体模型结构如下图：</p>
<img src="/images/AFM/net.jpeg">
<p>作者在Frappe和MovieLens数据集上进行了实验，AFM的表现beat掉了所有fine tuned的对比模型（LibFm、HOFM、Wide&amp;Deep、DeepCross）。而且，使用这套框架的传统FM也要比LibFM的效果好，原因之一在于dropout的引入降低了过拟合的风险，第二在于LibFM在迭代优化时使用的是SGD方法，对每个参数的学习步长一致，很容易使后期的学习很快的止步，而Adagrad的引入使得学习率可变，因此性能更加优良。另外，attention的引入，一方面加速了收敛速度，另一方面也具有对交叉特征的可解释性（因为就是权重），类似于特征重要性。</p>
<h4 id="代码部分"><a href="#代码部分" class="headerlink" title="代码部分"></a>代码部分</h4><p>截取了一部分关键代码，解析在注释中体现</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_init_graph</span><span class="params">(self)</span>:</span></div><div class="line">    self.graph = tf.Graph()</div><div class="line">    <span class="keyword">with</span> self.graph.as_default(): </div><div class="line">        <span class="comment"># Input data.</span></div><div class="line">        self.train_features = tf.placeholder(tf.int32, shape=[<span class="keyword">None</span>, <span class="keyword">None</span>], name=<span class="string">"train_features_afm"</span>)  <span class="comment"># None * features_M</span></div><div class="line">        self.train_labels = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, <span class="number">1</span>], name=<span class="string">"train_labels_afm"</span>)  <span class="comment"># None * 1</span></div><div class="line">        self.dropout_keep = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>], name=<span class="string">"dropout_keep_afm"</span>)</div><div class="line"></div><div class="line">        <span class="comment"># Variables.</span></div><div class="line">        self.weights = self._initialize_weights()</div><div class="line"></div><div class="line">        <span class="comment"># Model.</span></div><div class="line">         <span class="comment"># _________ Embedding part _____________</span></div><div class="line">        <span class="comment"># data为libsvm格式，且非none特征共有M‘个，且特征值均为1。data中每个feature都对应一个K维embedding</span></div><div class="line">        self.nonzero_embeddings = tf.nn.embedding_lookup(self.weights[<span class="string">'feature_embeddings'</span>], self.train_features) <span class="comment"># None * M' * K</span></div><div class="line">        </div><div class="line">        <span class="comment"># 交叉项embedding layer生成，对应equation(2)</span></div><div class="line">        element_wise_product_list = []</div><div class="line">        count = <span class="number">0</span></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, self.valid_dimension):</div><div class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(i+<span class="number">1</span>, self.valid_dimension):</div><div class="line">               element_wise_product_list.append(tf.multiply(self.nonzero_embeddings[:,i,:], self.nonzero_embeddings[:,j,:]))</div><div class="line">                count += <span class="number">1</span></div><div class="line">        self.element_wise_product = tf.stack(element_wise_product_list) <span class="comment"># (M'*(M'-1))/2 * None * K</span></div><div class="line">        self.element_wise_product = tf.transpose(self.element_wise_product, perm=[<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>], name=<span class="string">"element_wise_product"</span>) <span class="comment"># None * (M'*(M'-1))/2 * K</span></div><div class="line">        </div><div class="line">        <span class="comment"># _________ MLP Layer / attention part _____________</span></div><div class="line">        <span class="comment"># self.attention == True时，模型为Attention FM;</span></div><div class="line">        <span class="comment"># self.attention == False时，就退化为Neural FM</span></div><div class="line">        num_interactions = self.valid_dimension*(self.valid_dimension<span class="number">-1</span>)/<span class="number">2</span></div><div class="line">        <span class="keyword">if</span> self.attention:</div><div class="line">            <span class="comment"># 对应equation(4)-1</span></div><div class="line">            self.attention_mul = tf.reshape(tf.matmul(tf.reshape(self.element_wise_product, shape=[<span class="number">-1</span>, self.hidden_factor[<span class="number">1</span>]]),self.weights[<span class="string">'attention_W'</span>]), shape=[<span class="number">-1</span>, num_interactions, self.hidden_factor[<span class="number">0</span>]])</div><div class="line">            self.attention_exp = tf.exp(tf.reduce_sum(tf.multiply(self.weights[<span class="string">'attention_p'</span>], tf.nn.relu(self.attention_mul + self.weights[<span class="string">'attention_b'</span>])), <span class="number">2</span>, keep_dims=<span class="keyword">True</span>)) <span class="comment"># None * (M'*(M'-1)) * 1</span></div><div class="line">            self.attention_sum = tf.reduce_sum(self.attention_exp, <span class="number">1</span>, keep_dims=<span class="keyword">True</span>) <span class="comment"># None * 1 * 1</span></div><div class="line">            <span class="comment"># 对应equation(4)-2</span></div><div class="line">            self.attention_out = tf.div(self.attention_exp, self.attention_sum, name=<span class="string">"attention_out"</span>) <span class="comment"># None * (M'*(M'-1)) * 1</span></div><div class="line">            <span class="comment"># 在attention部分引入dropout</span></div><div class="line">            self.attention_out = tf.nn.dropout(self.attention_out, self.dropout_keep[<span class="number">0</span>]) <span class="comment"># dropout</span></div><div class="line">            self.AFM = tf.reduce_sum(tf.multiply(self.attention_out, self.element_wise_product), <span class="number">1</span>, name=<span class="string">"afm"</span>) <span class="comment"># None * K</span></div><div class="line">        <span class="keyword">else</span>:              <span class="comment"># Neural FM</span></div><div class="line">            self.AFM = tf.reduce_sum(self.element_wise_product, <span class="number">1</span>, name=<span class="string">"afm"</span>) <span class="comment"># None * K</span></div><div class="line">        </div><div class="line">        <span class="comment"># 在全连接（映射）部分二次引入dropout</span></div><div class="line">        self.AFM = tf.nn.dropout(self.AFM, self.dropout_keep[<span class="number">1</span>]) <span class="comment"># dropout</span></div><div class="line"></div><div class="line">        <span class="comment"># _________ out _____________</span></div><div class="line">        self.prediction = tf.matmul(self.AFM, self.weights[<span class="string">'prediction'</span>]) <span class="comment"># None * 1</span></div><div class="line">        <span class="comment"># 引入传统FM中的一阶项和常数项</span></div><div class="line">        Bilinear = tf.reduce_sum(self.prediction, <span class="number">1</span>, keep_dims=<span class="keyword">True</span>)  <span class="comment"># None * 1</span></div><div class="line">        self.Feature_bias = tf.reduce_sum(tf.nn.embedding_lookup(self.weights[<span class="string">'feature_bias'</span>], self.train_features) , <span class="number">1</span>)  <span class="comment"># None * 1</span></div><div class="line">        Bias = self.weights[<span class="string">'bias'</span>] * tf.ones_like(self.train_labels)  <span class="comment"># None * 1</span></div><div class="line">        self.out = tf.add_n([Bilinear, self.Feature_bias, Bias], name=<span class="string">"out_afm"</span>)  <span class="comment"># None * 1</span></div><div class="line"></div><div class="line">        <span class="comment"># Compute the loss.</span></div><div class="line">        <span class="comment"># 在loss中加入L2正则项</span></div><div class="line">        self.loss = tf.nn.l2_loss(tf.subtract(self.train_labels, self.out)) + tf.contrib.layers.l2_regularizer(self.lamda_attention)(self.weights[<span class="string">'attention_W'</span>])  <span class="comment"># regulizer</span></div><div class="line"></div><div class="line">        <span class="comment"># Optimizer.</span></div><div class="line">        <span class="comment"># 传统FM为SGD方式迭代训练，可能会导致训练不充分的情况。Ada系列的迭代方式learning rate可变，会一定程度上缓解这个现象</span></div><div class="line">        <span class="keyword">if</span> self.optimizer_type == <span class="string">'AdamOptimizer'</span>:</div><div class="line">            self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate, beta1=<span class="number">0.9</span>, beta2=<span class="number">0.999</span>, epsilon=<span class="number">1e-8</span>).minimize(self.loss)</div><div class="line">        <span class="keyword">elif</span> self.optimizer_type == <span class="string">'AdagradOptimizer'</span>:</div><div class="line">            self.optimizer = tf.train.AdagradOptimizer(learning_rate=self.learning_rate, initial_accumulator_value=<span class="number">1e-8</span>).minimize(self.loss)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate).minimize(self.loss)</div><div class="line"></div><div class="line">        <span class="comment"># init</span></div><div class="line">        self.saver = tf.train.Saver()</div><div class="line">        init = tf.global_variables_initializer()</div><div class="line">        self.sess = self._init_session()</div><div class="line">        self.sess.run(init)</div></pre></td></tr></table></figure>
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/07/14/FM and FFM/" itemprop="url">
                  FM and FFM
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-07-14T17:11:02+08:00" content="2017-07-14">
              2017-07-14
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>在CTR预估中，LR模型有着举足轻重的地位，而近几年FM（Factorization Machine）和FFM（Field Factorization Machine）开始在各大比赛和互联网公司广泛运用并取得了不俗的成绩和效果。实际上，FM算是LR的扩展，FFM算是FM的扩展，或者说FMM的一种特例是FM，而FM的一种特例就是LR，下面主要围绕着这两个算法的原理部分展开介绍。</p>
<h4 id="FM"><a href="#FM" class="headerlink" title="FM"></a>FM</h4><p>相对于LR模型来说，FM就是考虑了每个特征之间的交互关系，而有些时候在实际场景中这些特征交互往往起到了关键性作用，这也是为什么现在深度学习火的原因之一。打个比方，在广告推荐场景下，一个男性用户可能会有2%的概率点击一条足球属性的广告，而一个年龄在20~40岁之间的男性用户可能会有3%的概率点击，更进一步，满足上面条件的用户在一个给定的上下文条件下，比如世界杯期间、傍晚、甚至在搜索query高度相关的条件下就会有10%的点击概率，这都说明了特征之间的交互作用会对最后的预测结果产生很重要的影响。回到LR模型，它只对每个特征单独赋予一个权重，即</p>
<script type="math/tex; mode=display">
\phi(x) = \sigma(w_0 + \sum_i w_i x_i)</script><p>如果把$\sigma$拿掉的话，只看内部的线性部分，FM模型所能带来的特征交互形式即为</p>
<script type="math/tex; mode=display">
\begin{aligned}
y(x) =& w_0 + \sum_i w_i x_i +\sum_i \sum_{j = i + 1} w_{ij} x_i x_j\\
= &w_0 + \sum_i w_i x_i +\sum_i \sum_{j = i + 1} <v_i, v_j> x_i x_j
\end{aligned}</script><p>实际上，仅仅依靠$w_{ij}$就可以完成对特征的交互，但这里对$W$矩阵的构造并没有采用直接学习的方式，而是借鉴了矩阵分解的形式，构成形式为由$V \cdot V^T$。这样做的好处有两点，第一点是学习的参数量会显著减少。由于在CTR预估的任务中，会存在非常sparse的特征，百万千万维都有可能，如果直接学习$w_{ij}$的参数量是$N^2$，而如果$V\in R^{N*k}$，学习的参数量就是$kN$，而一般来说$k$的选取并不会太大，一方面是为了减少计算量，另一方面则是为了考虑在高维稀疏特征条件下的泛化性能；第二点是如果直接学习$w_{ij}$，会存在“没什么可学的”的窘境，因为如果是采用SGD的优化方式，$w_{ij}$的更新会依赖那些交互项都不为0的特征，而这样的交互极少，参数可能会学习不充分，但以这种矩阵分解的间接构建$w_{ij}$更新参数就不会出现这种问题。</p>
<p>由于上式前半部分和LR一致，所以我们将重点放在后半部分特征交互项上，经过推导，最后一项可以化简为</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\sum_i \sum_{j = i+1} <v_i, v_j> x_i x_j \\
=&\frac{1}{2} \sum_i \sum_{j = 1} <v_i, v_j> x_i x_j - 1/2 \sum_i <v_i, v_i> x_i x_i \\
=&\frac{1}{2}(\sum_i \sum_j \sum_{f=1}^{k} v_{i,f} v_{j, f} x_{i} x_{j} - \sum_i \sum_{f=1}^{k} v_{i, f} v_{i, f} x_i x_i) \\
=&\frac{1}{2}\sum_{f = 1}^{k}((\sum_i v_{i,f} x_i)(\sum_j v_{j,f} x_j)) - \sum_i v_{i,f}^2 x_i^2))  \\
=&\frac{1}{2}\sum_{f = 1}^{k}(( \sum_{i=1}^{N} v_{i,f} x_i)^2 -  \sum_{i=1}^{N} v_{i,f}^2 x_i^2))
\end{aligned}</script><p>这里虽然看上去很绕，但只要自己动手写一写就一目了然了。最最重要的是，上面的式子的计算复杂度是线性的$O(kN)$，相比较于直接计算$W$矩阵的复杂度$O(kN^2)$，明显要高效许多。在优化参数时，采用传统的SGD方法，式中各项参数的梯度可求</p>
<script type="math/tex; mode=display">
\frac {\partial y(x)}{\partial \theta} = \begin{cases} 1 & \text {if $\theta$ is $w_0$}\\
x_i & \text {if $\theta$ is $w_i$}\\
x_i \sum_j v_{j,f}x_j - v_{i,f}x_i^2 & \text {if $\theta$ is $v_{i,f}$} \end{cases}</script><p>最后，假设对于一个具体的分类问题，最终带正则的优化目标就变成了</p>
<script type="math/tex; mode=display">
\begin{aligned}
\theta^* &= argmin_\theta \sum (loss(y(x_i), \hat y_i) + \sum_\theta \lambda \theta^2) \\ 
&= argmin_\theta \sum -\hat y_i log\sigma(y_i) - (1 - \hat y_i) log(1 - \sigma(y_i)) + \sum_\theta \lambda \theta^2)
\end{aligned}</script><h4 id="FFM"><a href="#FFM" class="headerlink" title="FFM"></a>FFM</h4><p>FFM是在FM的基础上引入了Field的概念。从上面的公式中可以看到，FM在处理$x_i$和$x_j$的特征交互时，所使用的隐向量$V$是$v_i$和$v_j$，而在处理$x_i$和$x_k$的特征交互时，所使用的隐向量$V$是$v_i$和$v_k$，这样的局限就是$x_i$和其他所有特征交互时用的都是一个隐向量$v_i$，而FFM对这个问题的处理方法就是引入field的概念，或者说把特征分组，比如年龄特征是一个field，性别特征是一个field，职业特征又是一个field，在FM模型中年龄与性别、职业特征交互项的隐因子$v_{age}$是一样的，但在FFM中性别职业的交互隐因子为$v_{age, sex}$，而性别职业的交互隐因子为$v_{age, prof}$。更一般地，之前的交互项系数的表达为&lt;$v_i, v_j$&gt;，而现在又加入了field这一因素，表达为&lt;$v_{i, f_j}, v_{j, f_i}$&gt;，也就是说在模型的表达式上只需要修改这一项就可以，其余都不变。在实际应用中，我们一般指定分类特征中的每一类特征为一个field，而连续特征每个都为一个field。</p>
<p>我们可以看到，加入field概念之后对特征的交互考虑得更加周全，不同类型的特征交互体现出了多样性，但是由于有这样一个因素的存在，使得我们模型的参数量也随之提高。FM的参数量为$kN$，又由于公式可以化简，因此计算复杂度由初始的$O(kN^2)$变为$O(kN)$，然而FM的参数量则变为了$kfN$，其中$f$为field个数，公式无法化简，计算复杂度为$O(kN^2)$。虽然说由于有field带来的多样性，在$k$的选取上FFM要比FM小，但总体上看FFM的学习速率还是要比FM慢一截。</p>
<p>从FM和FFM在公开数据集上的表现来看，FM和FFM各有千秋，但相比于原始未进行特征交互的LM和直接计算特征交互矩阵$W$的Poly2方法还是有一定程度的提升。</p>
<img src="/images/FM/fm_ffm.png">
<h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><p><a href="http://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf" target="_blank" rel="external">Factorization Machines</a><br><a href="http://www.csie.ntu.edu.tw/~cjlin/papers/ffm.pdf" target="_blank" rel="external">Field-aware Factorization Machines for CTR Prediction</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/07/14/Tensorflow/" itemprop="url">
                  Tensorflow简介
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-07-14T17:08:42+08:00" content="2017-07-14">
              2017-07-14
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>接触Tensorflow也有好一段时间了，但总是对这个项目庞大的代码量和复杂的设计结构望而生畏，一直觉得它可以和某些原生的编程语言相提并论，所以大多时候也都是照着一些现成的模板去套用然后再不断调试，对内部的一些基本概念其实并没有太多了解。最近想着好好把官方给的get started和tutorial过一遍，对以后使用应该也更有帮助。</p>
<h3 id="Tensors"><a href="#Tensors" class="headerlink" title="Tensors"></a>Tensors</h3><p>张量就是高维数组的另一种表达形式，数组是几维的，张量的rank就是几。比如[1,2,3]是一维向量，rank=1；[[1,2,3], [2,3,4]]是二维矩阵，rank=2；[[[1,2,3], [2,3,4]]]是三维张量，rank=3，就这么简单。</p>
<h3 id="Computational-Graph"><a href="#Computational-Graph" class="headerlink" title="Computational Graph"></a>Computational Graph</h3><p>有了tensor，要形成flow，就必须借助图，张量和图起来就基本构成了Tensorflow的基本框架。图中的每个node可以是tensor，也可以是加减乘除之类的算子。其实回想一下，神经网络本质上就是这种神经元之间互相有向连接的结构，所以在我们定义好每个node后把这些node连接成图，理论上就可以搭建任何模式的结构。但是如果要查看某个图最终node的输出结果，还必须要通过一个session的运行环境来run。下面举个简单的小例子就会一目了然：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">node1 = tf.constant(<span class="number">3.0</span>, tf.float32)</div><div class="line">node2 = tf.constant(<span class="number">4.0</span>) <span class="comment"># also tf.float32 implicitly</span></div><div class="line">print(node1, node2)</div><div class="line"><span class="comment">#Output: Tensor("Const:0", shape=(), dtype=float32) Tensor("Const_1:0", shape=(), dtype=float32)</span></div><div class="line">sess = tf.Session()</div><div class="line">print(sess.run([node1, node2]))</div><div class="line"><span class="comment">#Output:[3.0, 4.0]</span></div><div class="line">node3 = tf.add(node1, node2)</div><div class="line">print(<span class="string">"node3: "</span>, node3)</div><div class="line">print(<span class="string">"sess.run(node3): "</span>,sess.run(node3))</div><div class="line"><span class="comment">#Output:7.0</span></div></pre></td></tr></table></figure>
<p>上面的code只是把node1和node2定义为tf.constant()的常量形式，更普遍的我们将输入定义为类似于函数形式的变量，这时就需要利用到tf.placeholder()方法，可以理解为占位符，这类似于python中lambda匿名函数的形式，占位符就是传入匿名函数的输入参数，再看例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">a = tf.placeholder(tf.float32)</div><div class="line">b = tf.placeholder(tf.float32)</div><div class="line">adder_node = a + b  <span class="comment"># + provides a shortcut for tf.add(a, b)</span></div><div class="line">print(sess.run(adder_node, feed_dict = &#123;a: <span class="number">3</span>, b:<span class="number">4.5</span>&#125;))</div><div class="line"><span class="comment">#Output:7.5</span></div><div class="line">print(sess.run(adder_node, feed_dict = &#123;a: [<span class="number">1</span>,<span class="number">3</span>], b: [<span class="number">2</span>, <span class="number">4</span>]&#125;))</div><div class="line"><span class="comment">#Output:[ 3.  7.]</span></div><div class="line">add_and_triple = adder_node * <span class="number">3.</span></div><div class="line">print(sess.run(add_and_triple, feed_dict = &#123;a: <span class="number">3</span>, b:<span class="number">4.5</span>&#125;))</div><div class="line"><span class="comment">#Output:22.5</span></div></pre></td></tr></table></figure>
<p>上面提到的tensor都是不可变，但是在大多machine learning的应用中需要通过不断的迭代来调整参数，比如网络中的weight和bias，这时需要利用tf.Variable()来给定初始值并定义这些需要train的变量。有了trainable的参数和输入数据，最后一步就是定义loss，我们以linear regression为例，定义square loss 为$loss  = (Wx + b - \hat y)^2$，大多数常见的loss在Tensorflow中都已经被封装好，直接调用即可。接下来的代码相对来说复杂一点，但逻辑在Tensorflow的框架下十分清晰：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">W = tf.Variable([<span class="number">.3</span>], tf.float32) <span class="comment">#Trainable param</span></div><div class="line">b = tf.Variable([<span class="number">-.3</span>], tf.float32) <span class="comment">#Trainable param</span></div><div class="line">x = tf.placeholder(tf.float32) <span class="comment">#Input placeholder</span></div><div class="line">y = tf.placeholder(tf.float32) <span class="comment">#Input placeholder</span></div><div class="line"></div><div class="line">linear_model = W * x + b <span class="comment"># Linear Regression</span></div><div class="line">squared_deltas = tf.square(linear_model - y) <span class="comment">#Define square loss</span></div><div class="line">loss = tf.reduce_sum(squared_deltas) <span class="comment">#Add loss on all samples</span></div><div class="line"></div><div class="line">init = tf.global_variables_initializer() <span class="comment"># You must explicitly initilize the variables</span></div><div class="line">sess.run(init) <span class="comment"># Until we call sess.run, the variables are uninitialized</span></div><div class="line">print(sess.run(loss, &#123;x:[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>], y:[<span class="number">0</span>,<span class="number">-1</span>,<span class="number">-2</span>,<span class="number">-3</span>]&#125;))</div><div class="line"><span class="comment">#Output:23.66</span></div></pre></td></tr></table></figure>
<h3 id="Train"><a href="#Train" class="headerlink" title="Train"></a>Train</h3><p>到现在为止，我们还只是简单的手动构建了图，做了几个简单运算，还没有涉及到神经网路的梯度反向传播、参数更新过程。Tensorflow的强大之处在于，在用户构建完成网络结构（图）后，就可以通过简单的函数调用来自动计算梯度。这里我们以最常见的梯度下降为例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">optimizer = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>) <span class="comment"># define gradient descent method</span></div><div class="line">train = optimizer.minimize(loss) <span class="comment"># minize loss using gradient descent</span></div><div class="line"></div><div class="line">sess.run(init) <span class="comment"># reset values to incorrect defaults.</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>): <span class="comment"># run gradient descent for 1000 steps</span></div><div class="line">  sess.run(train, &#123;x:[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>], y:[<span class="number">0</span>,<span class="number">-1</span>,<span class="number">-2</span>,<span class="number">-3</span>]&#125;)</div><div class="line"></div><div class="line">print(sess.run([W, b]))</div><div class="line"><span class="comment">#Output:[array([-0.9999969], dtype=float32), array([ 0.99999082],dtype=float32)]</span></div></pre></td></tr></table></figure>
<h3 id="Reading-Data"><a href="#Reading-Data" class="headerlink" title="Reading Data"></a>Reading Data</h3><p>Tensorflow读取数据的方式大体上说有三种，第一种是和上述例子中一样的方式，先在图中定义好placeholder，再通过feed_dict方式在run session时传入placeholder对应的数据；第二种适用于小数据量，直接将数据定义为constant或variable，个人感觉本质上和第一种没有太大区别；第三种在数据量比较大时候十分常用，就是直接从文件中按照队列的方式进行读取，以目前一般企业级机器学习的数据量来看，前两种方式会受限于内存和图的大小，因此简单介绍下这种比较通用的方式。</p>
<p>首先将要读取的小文件名形成一个列表，然后通过<a href="https://www.tensorflow.org/api_docs/python/tf/train/string_input_producer" target="_blank" rel="external">tf.train.string_input_producer</a>函数建立一个关于这些文件名的队列queue。接下来会有若干个reader实体去从这个queue中读取文件名，然后对于不同形式的文件（比如csv、二进制等）再通过不同的decoder对当前文件名下的内容进行解析，这样就可以不断的从文件列表中的每个文件读数，下面是一个涉及到batch read的例子。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_my_file_format</span><span class="params">(filename_queue)</span>:</span></div><div class="line">  reader = tf.SomeReader() <span class="comment"># define reader</span></div><div class="line">  key, record_string = reader.read(filename_queue) <span class="comment"># read file from file_queue using reader, each execution of read reads a single line from the file</span></div><div class="line">  example, label = tf.some_decoder(record_string) <span class="comment"># decode file</span></div><div class="line">  processed_example = some_processing(example) <span class="comment"># do some preprocessing</span></div><div class="line">  <span class="keyword">return</span> processed_example, label</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">input_pipeline</span><span class="params">(filenames, batch_size, num_epochs=None)</span>:</span></div><div class="line">  filename_queue = tf.train.string_input_producer(</div><div class="line">      filenames, num_epochs=num_epochs, shuffle=<span class="keyword">True</span>)</div><div class="line">  example, label = read_my_file_format(filename_queue)</div><div class="line">  min_after_dequeue = <span class="number">10000</span> <span class="comment"># define how big a buffer we will randomly sample from</span></div><div class="line">  capacity = min_after_dequeue + <span class="number">3</span> * batch_size <span class="comment"># larger than min_after_dequeue</span></div><div class="line">  example_batch, label_batch = tf.train.shuffle_batch(</div><div class="line">      [example, label], batch_size=batch_size, capacity=capacity,</div><div class="line">      min_after_dequeue=min_after_dequeue) <span class="comment"># make batch sample</span></div><div class="line">  <span class="keyword">return</span> example_batch, label_batch</div></pre></td></tr></table></figure>
<p>一般来说牵扯到了queue，我们就必须要在tf.Session()中在任务run之前利用tf.train.start_queue_runners和tf.train.Coordinator来填充队列和管理线程，不然read进程就会因为从queue中等待读取文件名而发生阻塞。</p>
<p>上面只是对tf做了一个非常非常浅显的介绍，但烂熟于心之后基本上也可以无障碍阅读github上大多数的tensorflow tutorial，而在实际的程序中依然有大量的坑和N多封装好的方法等待被发掘。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/05/05/Session_rec/" itemprop="url">
                  利用RNN做session-based推荐
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-05-05T15:05:42+08:00" content="2017-05-05">
              2017-05-05
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/Paper/" itemprop="url" rel="index">
                    <span itemprop="name">Paper</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>最近在做站内新用户推荐相关的项目，由于新用户历史行为相对较少，传统的neighborhood method和factor method models上对于user profile的利用少之又少，而如果当做一个传统的二分类问题，即预测购买/不购买，同样也存在相似的问题——特征不足，model能力上限有瓶颈，因此想着挖掘一下会话的time series pattern，从而变成一个基于session的预测问题，主要参考了<a href="https://arxiv.org/abs/1511.06939" target="_blank" rel="external">ICLR 2016的一篇文章</a>，个人感觉这篇文章可以算是利用RNN做基于session推荐比较早期的方法，而且作者也开源了<a href="https://github.com/hidasib/GRU4Rec" target="_blank" rel="external">基于theano的实现</a>，后续一些improved版本，包括网易在考拉上的session推荐实践也都借鉴了这篇文章的一些思想，接下来对这篇文章进行一个简单的梳理。</p>
<p>文章中解决的问题是：给定一个session中的item点击流（click streaming events），来预测这个点击流后的下一个点击会是什么item，并最终由目标item完成推荐。需要注意的是，作者虽然用到了recsys 2015的数据来做model evaluation，但实际上并没有用到购买数据，因此这篇文章所解决的问题只是通过session的item序列模式挖掘可能感兴趣的item，并不是最终会发生购买行为的item。按照这个模型结构来看，输入数据就是某个session的当前状态，即前N个item click sequences所携带的序列信息，输出就是当前session第N+1个item。所以在一个点击流中的一个特定点击事件的网络结构可以表达为如下图所示，输入为已点击item的one-hot编码，经过一层embedding层来进行降维，再通过多层GRU（Gated Recurrent Unit）单元，最后通过一个前馈映射层来输出下一个item的likelihood。</p>
<p>简要描述一下GRU的原理（这里作者用GRU的原因是在GRU、LSTM和传统RNN单元中发现GRU效果最好）。传统RNN的hidden state数学表达为</p>
<script type="math/tex; mode=display">
h_t = g(Wx_t + Uh_{t-1})</script><p>也就是某一个时间点上的hidden state与当前时间点的输入和上一个时间点的hidden state有关，并且是二者线性组合的形式，但当时间序列过长时，会面临严重的梯度消失问题，而GRU和LSTM由于网络结构的变化可以有效的解决传统RNN中的时间维度梯度消失问题。其中GRU引入了update gate $z_t$和reset gate $r_t$的概念，两个gate控制当前时刻输入和上一时刻的记忆信息哪部分需要保留，哪部分需要丢弃，表达形式一致，分别表示为</p>
<script type="math/tex; mode=display">
z_t = \sigma(W_z x_t + U_z h_{t-1}) \\
r_t = \sigma(W_r x_t + U_r h_{t-1})</script><p>而输出的hidden state为</p>
<script type="math/tex; mode=display">
\hat h_t = tanh(W x_t + U(r_t \odot h_{t-1}))\\
h_t = (1 - z_t)h_{t-1} + z_t \hat h_t</script><p>在构造数据集时，文章采用了session后补齐再划分mini-batch的方法（如下图）。具体来讲，就是当一个session结束时，用一个新的session接到当前session的末尾，当这个事件发生时，hidden state就被重新置零。</p>
<img src="/images/Session_Rec/fig1.png">
<p>虽然可以把上述问题作为一个多分类问题，但最后文章在实验中发现用cross-entropy做loss在100个左右hidden units时效果不错，但一旦units数量增大效果就不如ranking loss的形式，因此采用了pairwise ranking loss作为损失函数，并分别列举了BPR（Bayesian Personalized Ranking）和TOP1，其中某一个特定session中的一个点击预测loss为</p>
<script type="math/tex; mode=display">
L_S^{BPR} = -\frac{1}{N_S} \sum_{j=1}^{N_S} log(\sigma(\hat{r}_{S,i} - \hat{r}_{S,j})) \\
L_S^{TOP1} = -\frac{1}{N_S} \sum_{j=1}^{N_S} log(\sigma(\hat{r}_{S,i} - \hat{r}_{S,j})) + \sigma(\hat{r}_{S,j}^2)</script><p>，这里$\hat {r}_{S,j}$为第$j$个negative sample item的score，$\hat {r}_{S,i}$为postive sample item的score，值得注意的是这里对负样本做了一定程度的负采样，以消除一些用户可能感兴趣但并没有展示商品的影响。</p>
<p>实验在Recsys 2015数据集和另一个视频点击数据集上进行，以MRR@20和Recall@20作为指标评估，分别比较了不同mini-batch大小、dropout大小、learning-rate大小和momentum和loss形式下的效果，并与一些传统流行的推荐算法baseline进行对比，均有提升，细节不详谈，有兴趣的可以自己去看一下。</p>
<img src="/images/Session_Rec/fig2.png">
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/04/10/DSSM/" itemprop="url">
                  Multi-Rate Deep Learning for Temporal Recommendation解读
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-04-10T20:47:53+08:00" content="2017-04-10">
              2017-04-10
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/Paper/" itemprop="url" rel="index">
                    <span itemprop="name">Paper</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>在推荐系统里有一个比较重要的问题：如何能把一个user的长期兴趣和短期兴趣综合起来考虑进行内容推荐？一个人的兴趣总会随着时间发生变化，特定日期、事件都会使长期兴趣发生波动从而促生短期兴趣。一个比较直观的例子：如果在一个世界杯期间的新闻推荐场景下，推足球新闻很有可能比推荐长期兴趣的内容更使人满意。而目前的很多推荐方法都没有考虑与时间有关的短期信息，因此这篇文章主要针对将长短兴趣结合来提高推荐效果。</p>
<p>模型的基础利用deep semantic structured model (DSSM)。简言之，DSSM可以视为把从两个或多个角度所构建的神经网络模型整合到一个角度进行学习。假设一个两个角度的DSSM，其中一个网络表示query，另一个网络表示document，两个网络的输入可以是由各自角度所代表的特征（query-based features和document-based features），输出为比输入维度低的embedding vectors，而两个网络综合学习的目标是最大化两个输出vectors的cosine相似度。在具体的训练过程中，在每个mini-batch中随机负采样后再与正样本组合，然后再最小化正样本上的cosine loss（使正样本中的两个网络输出vectors最匹配）。在推荐场景里，可以认为其中一个网络是user的query history，即用户特征，另一个网络为系统中item的隠反馈，比如新闻的点击或是app的下载。</p>
<p>上面所说的是DSSM的基本概念，但是这里有一个问题，就是user相关的features都是不随时间发生改变的，于是作者引入了Temporal的概念，也就是将user的feautures进行拆分和细化，分为static和temporal，分别代表长期兴趣和短期兴趣。 下图可以很好的表示整个推荐系统的架构，其中item features和user static features全都利用全连接的神经网络构造embeddings，而user temporal features则利用LSTM构造embedding（文中的实验表明GRU的效果并不理想），然后将static和temporal的vectors通过函数$f$做成组合向量（$f$可以是multiplication或concatenation），再与item的向量做cosine similarity生成一个user-item相似度，预测的时候取与user相似度最大的topK item进行推荐。（这里有个问题，如果考虑item的temporal features会是什么样？）</p>
<img src="/images/DSSM/1.png">
<p>给出的优化目标为似然形式，即使给定user和时刻t时item概率最大,</p>
<script type="math/tex; mode=display">
min_{W_{user}, W_{item}} -log \prod_{user, item^+, t_i} p(item^+|user, t_i) \\
p(item^+|user, t_i) = \frac{exp(cos(E(user, t_i), E_{item^+}))}{\sum_{item} exp(cos(E(user, t_i), E_{item}))}</script><p>这里概率依然还是由softmax得到，只不过用user的temporal+static向量和item向量之间的相似度来表现。</p>
<p>不过仔细想一下，模型在细节上还是有些问题，那就是如何选择时间窗口t。选大了，兴趣不够“短期”，选小了，模型参数太多，训练不来，因此作者又引入了multi-rate的概念，也就是选择几个窗口，分别代表短期兴趣和中短期兴趣，然后再训练不同的LSTM，称为“Fast-RNNs”和“Slow-RNNs”，然后将几个LSTMs用全连接层串到一起就OK了。不过这样RNN所带来的训练参数还是太多，文中采用的方法是在训练之前先用上文提到基本的DSSM做pre-train。</p>
<p>理论部分基本就是这些，就是在DSSM基础上引入了Temporal的概念——解决用户短期兴趣的问题，再引入了multi-rate的概念——对短期兴趣的粒度和模型训练效率做trade-off，因此称为MR-TDSSM。说实话，这篇文章并没有DSSM那篇惊艳，只能算是前者的进一步扩展，但实验结果确实很不赖，在新闻数据上多个指标上能够碾压传统推荐算法和DSSM。值得一提的是，实现工具是keras（一直以为微软的工程师都会用自己的轮子），一些传统方法的baseline用的都是LibRec，可以看到这个开源工具还是挺流行的。</p>
<img src="/images/DSSM/2.png">
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/03/22/w_n_d/" itemprop="url">
                  Wide & Deep Learning for Recommender Systems解读
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-03-22T22:59:53+08:00" content="2017-03-22">
              2017-03-22
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/Paper/" itemprop="url" rel="index">
                    <span itemprop="name">Paper</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Google在去年6月份在arxiv上放出了”Wide &amp; Deep Learning for Recommender Systems”这篇文章，应用场景是Google Play上App安装预测，线上效果提升显著（相比较于Wide only和Deep only的model）。与此同时，Google也开源了这一模型框架并将其集成在Tensorflow的高级封装Tflearn中，今年开年的谷歌开发者大会上也专门有一个section是讲wide and deep model的，作者的意图也很明显，一方面推广tensorflow，另一方面是显示模型的强大。</p>
<p>通读文章后，发现其实模型的基本原理很简单，就是wide model + deep model。分别来讲，wide model就是一个LR，在特征方面除了原始特征外还有分类特征稀疏表达后的交叉特征，例如将分类特征做完one-hot后再进行cross product。个人理解，这里一方面是利用类似于FM模型原理来增强分类特征的特征交互（co-occurrence），另一方面是利用LR对高维稀疏特征的学习能力，而作者把wide model所具备的能力称为“memorization”；而deep model则是一个DNN，特征上除了原始特征还增加了分类特征的embedding，这个embedding在模型中属于独立的一层，embedding后的向量也是通过不断迭代学习出来的。将高维稀疏分类特征映射到低维embedding特征这种方式有助于模型进行“generalization”。Memerization和Generalization这两个概念中文还真没找到特别合适的诠释，如果非要翻译一下，我觉得应该是推理和演绎，一个是通过特征交互关系来训练浅层模型，另一个则是通过特征在映射空间中的信息训练深层模型。</p>
<p>模型结构采用joint的方式而非传统的ensemble方式。如果是ensemble方式，那么这两个模型就针对label进行单独训练，然后再加权到一起；而joint方式则是将这两个模型的输出加起来，然后再针对label进行联合训练。这样的好处是在train model的时候可以同时最优化两个model的参数，而且两个model可以起到互相补充的作用。下面的公式也很好的解释了wide and deep model的结构原理，即两个model的output在通过sigmoid函数之前把结果相加，然后再经过sigmoid实现分类。这里$x$指原始特征，$\phi(x)$分别表示wide模型的cross product feature和deep模型的embedding feature，而$w_{deep}$则泛指DNN各层weights和bias集合表示。</p>
<script type="math/tex; mode=display">
 P(Y=1|x) = \sigma(w_{wide}[x, \phi_1(x)] + w_{deep}[x, \phi_2(x)])</script><img src="/images/w_n_d/1.png">
<p>最终模型在离线评测上效果并不明显，但在在线评测上提升还算显著。前几天在电梯里听到广告部同事说他们也在搞这个模型，离线效果提升了10%+，但在线serving技术上目前比较头疼，但也不知道具体是什么指标提升了10%+。但在我们团队内部客户拉新预测问题的离线应用上，同样的数据效果只和xgboost持平。另外，官方提供的tutorial原生代码并不能很好的应用于大数据量，于是进行了改写，读取数据方式变成了队列读取，也是参考了stackoverflow上的一些反馈。核心就是wide_and_deep函数，cross product的实现方式是直接在预处理数据时对分类特征进行字符串拼接，然后再做one-hot，相对contrib中crossed_column的实现方式略显复杂，但以个人能力也只能先这样做，日后再去探索。另外，由于目前团队业务只涉及线下模型，因此对于模型的线上更新并没有太多关注，但大的技术框架来看应该也是用tf serving的方式实现。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">import</span> tensorflow.contrib.learn <span class="keyword">as</span> tf_learn</div><div class="line"><span class="keyword">import</span> tensorflow.contrib.layers <span class="keyword">as</span> tf_layers</div><div class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</div><div class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</div><div class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</div><div class="line"><span class="keyword">import</span> os</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</div><div class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">from</span> feat <span class="keyword">import</span> CONTINUOUS_COLUMNS</div><div class="line"><span class="keyword">from</span> feat <span class="keyword">import</span> CATEGORICAL_COLUMNS</div><div class="line"><span class="keyword">from</span> feat <span class="keyword">import</span> COLUMNS</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_columns</span><span class="params">(x)</span>:</span></div><div class="line">    <span class="keyword">return</span> <span class="string">':'</span>.join(x)</div><div class="line"></div><div class="line"><span class="comment"># Define the column names for the data sets.</span></div><div class="line">LABEL_COLUMN = <span class="string">'target'</span></div><div class="line"><span class="comment">#second order</span></div><div class="line">CROSSED_COLUMNS = []</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(CATEGORICAL_COLUMNS) <span class="number">-1</span>):</div><div class="line">  <span class="keyword">for</span> j <span class="keyword">in</span> range(i+<span class="number">1</span>, len(CATEGORICAL_COLUMNS)):</div><div class="line">     CROSSED_COLUMNS.append([CATEGORICAL_COLUMNS[i], CATEGORICAL_COLUMNS[j]])</div><div class="line"></div><div class="line">CATEGORICAL_COLUMNS_DNN = CATEGORICAL_COLUMNS[:]</div><div class="line">CATEGORICAL_COLUMNS += map(add_columns, CROSSED_COLUMNS)</div><div class="line">CATEGORICAL_ID_COLUMNS = [col + <span class="string">'_ids'</span> <span class="keyword">for</span> col <span class="keyword">in</span> CATEGORICAL_COLUMNS]</div><div class="line"></div><div class="line"></div><div class="line">HIDDEN_UNITS = [<span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>]</div><div class="line">CATEGORICAL_EMBED_SIZE = <span class="number">10</span></div><div class="line"></div><div class="line">LABEL_ENCODERS = &#123;&#125;</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">pandas_input_fn</span><span class="params">(X, y=None, batch_size=<span class="number">1024</span>, num_epochs=None)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">input_fn</span><span class="params">()</span>:</span></div><div class="line">        <span class="keyword">if</span> y <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">            X[<span class="string">'target'</span>] = y</div><div class="line">        queue = tf_learn.dataframe.queues.feeding_functions.enqueue_data(</div><div class="line">            X, <span class="number">1000</span>, shuffle=num_epochs <span class="keyword">is</span> <span class="keyword">None</span>, num_epochs=num_epochs)</div><div class="line">        <span class="keyword">if</span> num_epochs <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">            features = queue.dequeue_many(batch_size)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            features = queue.dequeue_up_to(batch_size)</div><div class="line"></div><div class="line">        features = dict(zip([<span class="string">'index'</span>] + list(X.columns), features))</div><div class="line"></div><div class="line">        <span class="keyword">if</span> y <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">            target = features.pop(<span class="string">'target'</span>)</div><div class="line">            <span class="keyword">return</span> features, target</div><div class="line">        <span class="keyword">return</span> features</div><div class="line"></div><div class="line">    <span class="keyword">return</span> input_fn</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">encode_categorical_cross</span><span class="params">(df)</span>:</span></div><div class="line">    <span class="keyword">global</span> LABEL_ENCODERS</div><div class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> CATEGORICAL_COLUMNS:</div><div class="line">        <span class="keyword">if</span> <span class="string">":"</span> <span class="keyword">in</span> col:</div><div class="line">            df[col] = df[col.split(<span class="string">":"</span>)[<span class="number">0</span>]].fillna(<span class="number">-1</span>).astype(str) + <span class="string">":"</span> + df[col.split(<span class="string">":"</span>)[<span class="number">1</span>]].fillna(<span class="number">-1</span>).astype(str)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            df[col] = df[col].fillna(<span class="number">-1</span>).astype(str)</div><div class="line">        encoder = LabelEncoder().fit(df[col])</div><div class="line">        df[col + <span class="string">'_ids'</span>] = encoder.transform(df[col])</div><div class="line">        LABEL_ENCODERS[col] = encoder</div><div class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> CATEGORICAL_COLUMNS:</div><div class="line">        df.pop(col)</div><div class="line">    <span class="keyword">return</span> df, LABEL_ENCODERS</div><div class="line"></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_input_df</span><span class="params">(df)</span>:</span></div><div class="line">    df, label_encoders = encode_categorical_cross(df)</div><div class="line"><span class="keyword">for</span> col <span class="keyword">in</span> CATEGORICAL_COLUMNS:</div><div class="line">    y = df.pop(LABEL_COLUMN)</div><div class="line">    X = df[CATEGORICAL_ID_COLUMNS + CONTINUOUS_COLUMNS].fillna(<span class="number">0</span>)</div><div class="line">    <span class="keyword">return</span> X, y</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">wide_and_deep</span><span class="params">(features, target, hidden_units=HIDDEN_UNITS)</span>:</span></div><div class="line">    <span class="keyword">global</span> LABEL_ENCODERS</div><div class="line">    target = tf.one_hot(target, <span class="number">2</span>, <span class="number">1.0</span>, <span class="number">0.0</span>)</div><div class="line"></div><div class="line">    <span class="comment"># DNN</span></div><div class="line">    final_features_nn = [tf.expand_dims(tf.cast(features[col], tf.float32), <span class="number">1</span>) <span class="keyword">for</span></div><div class="line">                      col <span class="keyword">in</span> CONTINUOUS_COLUMNS]</div><div class="line"></div><div class="line">    <span class="comment"># Embed categorical variables into distributed representation.</span></div><div class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> CATEGORICAL_COLUMNS_DNN:</div><div class="line">        feature_tmp = tf_learn.ops.categorical_variable(</div><div class="line">            features[col + <span class="string">'_ids'</span>],</div><div class="line">            len(LABEL_ENCODERS[col].classes_),</div><div class="line">            embedding_size=CATEGORICAL_EMBED_SIZE,</div><div class="line">            name=col)</div><div class="line">        final_features_nn.append(feature_tmp)</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">    <span class="comment"># Concatenate all features into one vector.</span></div><div class="line">    features_nn = tf.concat(<span class="number">1</span>, final_features_nn)</div><div class="line"></div><div class="line">    logits_nn = tf_layers.stack(features_nn,</div><div class="line">                             tf_layers.fully_connected,</div><div class="line">                             stack_args=hidden_units,</div><div class="line">                             activation_fn=tf.nn.relu)</div><div class="line"></div><div class="line">    <span class="comment"># LR</span></div><div class="line">    final_features_lr = [tf.expand_dims(tf.cast(features[col], tf.float32), <span class="number">1</span>) <span class="keyword">for</span></div><div class="line">                      col <span class="keyword">in</span> CONTINUOUS_COLUMNS]</div><div class="line"></div><div class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> CATEGORICAL_COLUMNS:</div><div class="line">        final_features_lr.append(tf.one_hot(features[col + <span class="string">'_ids'</span>],</div><div class="line">            len(LABEL_ENCODERS[col].classes_),</div><div class="line">            on_value = <span class="number">1.0</span>,</div><div class="line">            off_value = <span class="number">0.0</span>))</div><div class="line"></div><div class="line"></div><div class="line">    logits_lr = tf_layers.stack(tf.concat(<span class="number">1</span>, final_features_lr),</div><div class="line">                             tf_layers.fully_connected,</div><div class="line">                             stack_args=[<span class="number">1</span>],</div><div class="line">                             activation_fn=<span class="keyword">None</span>)</div><div class="line"></div><div class="line">    <span class="comment"># add logits</span></div><div class="line">    logits = logits_lr + logits_nn</div><div class="line"></div><div class="line">    prediction, loss = tf_learn.models.logistic_regression(logits, target)</div><div class="line">    train_op = tf_layers.optimize_loss(loss,</div><div class="line">                                       tf.contrib.framework.get_global_step(),</div><div class="line">                                       optimizer=<span class="string">'Adam'</span>,</div><div class="line">                                       learning_rate=<span class="number">0.001</span>)</div><div class="line">    <span class="keyword">return</span> prediction[:,<span class="number">1</span>], loss, train_op</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(X, y, steps=<span class="number">100</span>)</span>:</span></div><div class="line">    print(<span class="string">"model dir: "</span>, model_dir)</div><div class="line">    classifier = tf_learn.Estimator(model_fn=wide_and_deep, model_dir=model_dir)</div><div class="line">    classifier.fit(input_fn=pandas_input_fn(X, y), steps=steps)</div><div class="line"></div><div class="line">    <span class="keyword">return</span> classifier</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(classifier, X)</span>:</span></div><div class="line">    <span class="keyword">return</span> list(classifier.predict(input_fn=pandas_input_fn(X, num_epochs=<span class="number">1</span>),</div><div class="line">                                   as_iterable=<span class="keyword">True</span>))</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    model_dir = <span class="string">"./wnd"</span></div><div class="line">    os.system(<span class="string">"rm -rf ./wnd"</span>)</div><div class="line"></div><div class="line">    trainFile = <span class="string">'train.csv'</span></div><div class="line">    testFile = <span class="string">'test.csv'</span></div><div class="line"></div><div class="line">    data_train = pd.read_csv(trainFile, names=COLUMNS)  <span class="comment"># LOAD DATA</span></div><div class="line">    data_test = pd.read_csv(testFile, names=COLUMNS)  <span class="comment"># LOAD DATA</span></div><div class="line"></div><div class="line">    train_size = data_train.shape[<span class="number">0</span>]</div><div class="line"></div><div class="line">    X_train, y_train = process_input_df(data_train)</div><div class="line">    X_test, y_test = process_input_df(data_test)</div><div class="line"></div><div class="line">    <span class="comment"># data scale</span></div><div class="line">    data_continuous = pd.concat([X_train[CONTINUOUS_COLUMNS], X_test[CONTINUOUS_COLUMNS]])</div><div class="line">    scaler = StandardScaler()</div><div class="line">    data_continuous_scale = scaler.fit_transform(data_continuous)</div><div class="line">    X_train[CONTINUOUS_COLUMNS] = pd.DataFrame(data_continuous_scale[:train_size])</div><div class="line">    X_test[CONTINUOUS_COLUMNS] = pd.DataFrame(data_continuous_scale[train_size:])</div><div class="line"></div><div class="line">    classifier = train(X_train, y_train, steps=<span class="number">5000</span>)</div><div class="line">    pred = predict(classifier, X_test)</div><div class="line">    print(<span class="string">"auc"</span>, roc_auc_score(y_test, np.array(pred)))</div></pre></td></tr></table></figure>
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/03/15/ts/" itemprop="url">
                  关于时间序列的一些概念
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-03-15T22:33:53+08:00" content="2017-03-15">
              2017-03-15
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/Data-Analysis/" itemprop="url" rel="index">
                    <span itemprop="name">Data Analysis</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h4 id="平稳性"><a href="#平稳性" class="headerlink" title="平稳性"></a>平稳性</h4><p>平稳性是大多数时间序列分析问题中的一个大前提，而通常我们讨论的都是弱平稳，需要满足以下几个条件：</p>
<p>1）在任意时间点变量的均值函数是一个常数</p>
<p>2）在任意时间点变量的方差函数是一个常数</p>
<p>3）在任意两个时间点的自协方差函数只与两点时间间隔有关，而与这两点具体的时间点无关</p>
<p>典型的平稳时间序列：白噪声 $N(t)$</p>
<p>典型的非平稳时间序列：random walk（$R(t) = \sum_1^t N(i)$，方差随时间改变）</p>
<p>如果时间序列不满足平稳性，可以做N阶差分运算。</p>
<h4 id="自相关"><a href="#自相关" class="headerlink" title="自相关"></a>自相关</h4><p>时间序列的自相关性，从字面上就可以看出来，就是看这个序列平移一段距离后与这个原始信号有多相似。</p>
<script type="math/tex; mode=display">
R(\tau) = \sum x(t) x(t-\tau)dt</script><p>有点类似于卷积，但符号刚好相反。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">t = np.linspace(<span class="number">0</span>, <span class="number">20</span>, <span class="number">10000</span>)</div><div class="line">ts = np.sin(<span class="number">2</span>*np.pi*<span class="number">0.2</span>*t)</div><div class="line">ts = ts - ts.mean()</div><div class="line">autocorr = np.correlate(ts, ts, mode = <span class="string">'full'</span>)</div><div class="line">autocorr = autocorr[ts.size:] / autocorr.max() <span class="comment">#归一化</span></div><div class="line">plt.plot(autocorr)</div></pre></td></tr></table></figure>
<img src="/images/ts/autocorr.png">
<p>一般来说，平稳时间序列的自相关函数会随时间快速衰减。</p>
<p>其实本来是想看看关于时间序列分析的资料来应用到KDD CUP 2017的比赛上，而且之前天池上O2O口碑商家销量预测也是一个类似的时间序列问题。但是反复琢磨了一下，题目最终的预测只是在rush hours中的一两组数据，真正关于时序的信息可能真的用不大，而且之前口碑比赛里也有相应的反应，像ARIMA这种模型效果并不理想，所以下一步可能会考虑模式匹配的方法来做。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/03/12/w2v/" itemprop="url">
                  Word2Vec原理简介
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-03-12T18:42:00+08:00" content="2017-03-12">
              2017-03-12
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Word2vec，顾名思义，就是把词语料中的所有词转化为向量形式，这样自然语言就有了数学表达形式，向量化后可以聚类、近义词等运算，举一个论文中的例子，vec(“Madrid”) - vec(“Spain”) + vec(“France”)所生成的向量与vec(“Paris”)的距离是最近的。同样对于机器学习模型来说，也可以将词作为embedding的特征输入。另外，由于作者Tomas Mikolov在深度学习领域下比较出名，所以word2vec也自然而然的被归到“深度”网络了，然而了解原理后就会发现其实就是一个2-layer的浅层模型。</p>
<p>主要建模的方法有CBOW和Skip-gram两种，每种方法下面又可以分为Hierarchical Softmax和Negative Sampling方法。虽然看上去有点复杂，但是大致的原理很相似，下面就尽量以简单的语言和逻辑给出word2vec的原理。</p>
<h3 id="CBOW"><a href="#CBOW" class="headerlink" title="CBOW"></a>CBOW</h3><img src="/images/w2v/CBOW.png">
<p>首先来看一下CBOW（Continuous Bag of Words）模型的基本结构。假设要预测词$w(t)$的向量，那么输入层是与$w(t)$相邻的几个上下文词，投影层是对这些上下文进行了向量求和运算，而输出层就是$w(t)$的概率表示。整体上来看，假设临近的词都具有一定的相似性，那么对于CBOW模型而言，目的就是要在给定词$w$上下文$Context(w)$的条件下使$w$的概率最大，这里就可以用到最大似然的思想，目标函数可以定义为对数似然函数，即</p>
<script type="math/tex; mode=display">
L= \sum log p(w|Context(w))</script><p>显然，对于输入层已经有了明确的表示，可以看做是监督训练中的feature，而现在的问题就是如何去构造输出层的形式，使这个两层网络能够拥有(feature, label)的形式进行迭代训练。这里就要引入另一个概念——<a href="https://zh.wikipedia.org/wiki/%E9%9C%8D%E5%A4%AB%E6%9B%BC%E7%BC%96%E7%A0%81" target="_blank" rel="external">霍夫曼编码</a>，目的是对语料中的所有词基于出现的频率进行不等长编码。</p>
<p>简单举个例子就可以很轻松的理解霍夫曼编码的概念。假设我们的语料为{a, b, c, d, e}，这些词在语料中出现的频率为{4, 5, 6, 8, 10}，现在我们就可以基于语料来建立一棵霍夫曼树，如下图。构建过程也很简单，即将目前集合中所有元素的两个最小值进行合并，然后用这两个最小值的求和项来代替两个最小值产生新的集合，重复这一过程直至集合中没有元素，构建过程也在下图中清晰的画了出来（这里人为规定左子节点的值大于右子节点的值）。这样看来，语料中的所有词语都会是一个单独的叶子节点（个数为size(语料)），而中间的非叶子节点就是每次的求和项（个数为size(语料)-1）。如果定义霍夫曼树中的左子节点编码为1，右子节点编码为0的话，那么每个词语都可以被唯一且不等长的编码。例如a可以被编码为100，d可以被编码为01。可以看到频率越高的词语就越出于根节点的位置，编码长度也就越短，在通信里传输数据量也就越小，这也是通信里不等长编码的奥妙所在（把本科的东西再捡起来）。</p>
<img src="/images/w2v/huffman.png">
<h4 id="Hierarchical-Softmax"><a href="#Hierarchical-Softmax" class="headerlink" title="Hierarchical Softmax"></a>Hierarchical Softmax</h4><p>OK，现在我们已经有了每个词语的霍夫曼编码，该考虑如何构建$p(w|Context(w))$了。以词语d为例，观察之前构建的霍夫曼树结构可以发现这个词语进行了两次叶子分叉：第一次从根节点分到了右子节点，第二次从当前根节点分到了左子节点。这里Hierarchical Softmax的思想就体现了出来，即把每一次非叶子节点的分裂都视为是一次二分类问题，对应的类别就是0/1，也就是节点上的编码值。如果定义1为负类，0为正类（反过来也可以），并在每次分类时采用逻辑回归模型，那么词语d的两次分类概率分别为</p>
<script type="math/tex; mode=display">
\sigma(v(d)^T \theta_1(d))\\
1 - \sigma(v(d)^T \theta_2(d))\</script><p>其中$\sigma$为逻辑回归函数$\frac{1}{1+e^{-y}}，$ $v(d)$是词语d上下文的向量求和，$\theta$为每个非叶子节点所对应的辅助向量，维度与词语的向量维度一致。这样一来，把上面两个概率公式相乘后就可以以霍夫曼编码的形式得到$p(w|Context(w))$了，而每个词语的条件概率通式就可以写出来了，即</p>
<script type="math/tex; mode=display">
p(w|Context(w)) = \prod_j p(d_w^j|x_w, \theta_w^{j-1})</script><p>这里的$j$是每个词$w$所经过的霍夫曼树路径，$d\in {0,1}$，为在路径上的每一次分类结果。我们的目标就是取上式的似然函数$L(w,j)$最大值，未知参数只有$x_w$和$\theta_w^{j-1}$（每个词对应霍夫曼树路径上的每一个非叶子节点辅助向量），使用随机梯度上升法，不具体推导，直接给出参数的梯度：</p>
<script type="math/tex; mode=display">
\frac{\partial L(w,j)}{\partial \theta_w^{j-1}} = [1 - d_w^j - \sigma(x_w^T \theta_w^{j-1})]x_w \\
\frac{\partial L(w,j)}{\partial x_w} = [1 - d_w^j - \sigma(x_w^T \theta_w^{j-1})] \theta_w^{j-1}</script><p>这里的参数$\theta$可以直接利用梯度进行更新，但前面我们提到过，$x_w$实际上是$w$上下文的向量求和，而我们真正需要的是$w$的向量$v(w)$，所以在更新$v(w)$的时候需要将梯度贡献到这个词所包含的所有上下文中，并最终生成语料中每个词的词向量，学习结束。</p>
<h4 id="Negative-Sampling"><a href="#Negative-Sampling" class="headerlink" title="Negative Sampling"></a>Negative Sampling</h4><p>看完Hierarchical Softmax方法后，最直观的印象可能就是霍夫曼树的构造过于麻烦，而且当语料库规模很大时，树的复杂度也随之提高，从而导致一定的训练效率问题。Negative Sampling则不采用霍夫曼树进行概率的层次计算，而是采用一种更直观、更简洁的方法。</p>
<p>事实上Negative Sampling可以分为两部分，一部分为Negative，另一部分为Sampling，我们先从Negative说起。如果给定词$w$及其上下文$Context(w)$，我们将此$w$视为一个正样本，其余的所有词都可以看做是负样本，除此之外再直接为每个词分配一个类似于Hierarchical Softmax方法中的辅助向量$\theta$。借助逻辑回归的思想，对于一个词$u$在$Context(w)$条件下分类后的概率为</p>
<script type="math/tex; mode=display">
p(u|context(w)) = \sigma(x_w^T \theta_u)^{L_w(u)} (1-\sigma(x_w^T \theta_u))^{1 - L_w(u)}</script><p>其中$L_w(u) \in {0,1}$，表示$u$是否等于$w$（需要注意的是由于我们现在讨论的还是CBOW的方法，因此这里的$x_w$和Hierarchical Softmax中的定义一样，依然是$context(w)$的向量求和）。显然，正样本只有一个，而负样本量过大，因此需要采样处理，用$Neg$表示语料中对于$w$采样后的负样本，我们的目标就是最大化</p>
<script type="math/tex; mode=display">
l(w) = \prod p(u|context(w)) = \sigma(x_w^T \theta_w) \prod_{u \in Neg}1 - \sigma(x_w^T \theta_u)</script><p>换句话说，就是使正样本概率尽可能高，负样本概率尽可能低。上面的式子只是针对一个词，如果是对于整个语料库$A$，依然是最大似然的思想</p>
<script type="math/tex; mode=display">
max\prod_{w \in A} l(w)</script><p>待更新参数依然为$x_w$和$\theta_w$，更新依然是梯度上升法，与Hierarchical Softmax基本一致，只不过后者在$\theta$的更新是基于霍夫曼树非叶子节点路径的更新，而前者则是基于$w$的正负样本做更新。</p>
<p>说完了Negative，还有一个重要的Sampling。从上面的基本原理中我们可以看到，负样本相对于正样本要多很多，因此需要对负样本做一定程度的采样，而采样的原则需要保证语料中词频大的词采样概率大，词频小的词采样概率小，以符合原始分布。这样看来就是一个带权采样的问题，具体解决方法可以定义每个词为其词频长度的线段，然后将所有词线段首尾相接连在一起，词频大的词自然线段就长。在采样时随机选取（0，线段总长度）的数字，采样就选择它落在范围内的词，这样就保证了采样后的词与语料中原始词的词频基本同分布。</p>
<h3 id="Skip-gram"><a href="#Skip-gram" class="headerlink" title="Skip-gram"></a>Skip-gram</h3><img src="/images/w2v/SG.png">
<p>Skip-gram与CBOW的唯一区别就在于：CBOW是对$p(w|Context(w))$进行建模，而Skip-gram则是对$p(Context(w)|w)$进行建模。上面一张图可以很好的诠释二者的区别。这里的投影层只是为了和CBOW形势保持一致才加上去的，实际上不起任何作用。</p>
<p>针对Hierarchical Softmax方法，只需要将条件概率形式稍加改变即可</p>
<script type="math/tex; mode=display">
p(Context(w)|w) = \prod_{u \in Context(w)}\prod_j p(d_u^j|v_w, \theta_u^{j-1})</script><p>接下来流程完全一致，最大似然，梯度更新，迭代学习。</p>
<p>而对于Negative Sampling也是同样，当给定$w$时，最大化$context(w)$中所有词的似然函数，只需要将优化目标变为</p>
<script type="math/tex; mode=display">
l(u) = \prod p(z|w) = \sigma(v_w^T \theta_z) \prod_{z \in Neg(u)}1 - \sigma(v_w^T \theta_z)\\
max \prod_{w \in A} \prod_{u \in Context(w)}l(u)</script><p>接下来也都是老套路。</p>
<h3 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h3><p>关于方法的优劣和数据集、超参数以及语义评价标准有着密切的关系，因此在实际应用中也需要不断的尝试。word2vec的应用也有很多，比如将一个用户的购物历史、app浏览历史等作为一个句子，便可以学习出商品、品类或app的语义特征。google的单机多线程的code执行效率也很高。最近的一个工作是将用户的浏览的品类历史作为句子，学习出每个品类的vec，然后喂给后续的模型作为特征进行训练，只可惜特征重要性并不出众。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/03/03/gcForest/" itemprop="url">
                  挑战深度学习——Deep Forest
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-03-03T21:17:02+08:00" content="2017-03-03">
              2017-03-03
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/Paper/" itemprop="url" rel="index">
                    <span itemprop="name">Paper</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>两天前南大周志华教授在arXiv上放了一篇文章：Deep Forest: Towards An Alternative to Deep Neural Networks。国内机器学习界瞬间爆炸，业内著名非著名人士纷纷前来解读，有说即将取代DNN的，有说其实没什么新玩意就是那么回事的。我们这里只阐述这个model的基本结构和原理，不做任何评价（个人认为在相同资历和学术水平上才真正有资格去评价，反正我是没资格）。</p>
<h3 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h3><p>首先，Deep Forest，换一种叫法，gcForest(multi-Grained Cascade forest)，可以翻译为多粒度级联随机森林。通俗的来讲就是把若干个随机森林合并起来作为类似于神经网络中的一个layer，然后将这些layer串联起来，形成一个基于组合随机森林的、相对deep的模型。通过这个描述，模型的主体结构就已经基本形成了，如下图<br><img src="/images/gcForest/1.png"></p>
<p>这里level就是layer的概念，而我们发现Forest有黑色和蓝色两种，其中黑色代表传统意义上的随机森林，即每次选择sqrt(#Features)数量的特征作为候选特征进行分裂；蓝色代表“完全随机森林”，其中每个森林中包含1000棵树，每棵树随机选择特征进行节点的分裂使树一直生长到叶节点只包含同类样本或小于是个样本。因为随机森林本质上就一种ensemble method，而每一个layer又引入了若干个两种随机形式的随机森林，因此在layer层面上可以认为是“ensemble of ensemble”。<br>现在具体到一个layer，如下图，假设现在是个三分类问题，那么每个随机森林的输出都是这三个类别的概率值，因此对于任何一个样本而言，在每一个layer的每一个forest输的都是3维的向量，如果每一个layer有4个forest，那么在这个layer上就输出4<em>3维的新特征灌到下一个layer里。注意，如果不是最后一层，那么每一层接收的特征还要加上raw feature，也就是第N+1层比第N层多出了4</em>3个特征。如果是最后一层，就不再接收原始特征，把上一层的输出结果做平均再取最大值，作为最后的prediction。还有一点很重要，也是区别于DNN的最显著因素，就是gcForest每一层都是监督学习，都利用到了label的信息，而非像DNN一样只在最有一层才是有监督而进行误差传播。这样的好处就是可以每向后训练一层就用validation set评估一下accuracy和loss，因为每一层的输出结果都是可解释的，都可以拿来当做预测得分，因此如果训练N+1层时验证效果提高不大或有降低时，可以自适应的终止训练。在这点上DNN必须在训练前就指定layer的层数，相当于变相多了一个超参数。<br><img src="/images/gcForest/2.png"></p>
<h3 id="Multi-Grained-Scanning"><a href="#Multi-Grained-Scanning" class="headerlink" title="Multi-Grained Scanning"></a>Multi-Grained Scanning</h3><p>这算是论文的另一个亮点吧，主要描述了如何由原始数据生成样本和特征，依然还是先看下图。对于序列数据，假设特征是400维，现在我们用一个100维的滑动窗口（实际上窗口也是可变的，并不是只有一个窗）来做window-slide（比较类似于1D-convolution），每滑动一次生成100维向量就作为一个新的样本，这里相当于把原始数据做了滑动拆分，最终形成301个100维特征的子样本。接下来把这些样本灌到Deep Forest的第一层，一个Random Forest为301个子样本生成3个新特征，一个Complete Random Forest也为301个子样本生成3个新特征，然后再将这些特征做一个concat，这样一来一个真正的400维特的样本在第一层输出的高阶特征就是2<em>3</em>301维。后面的训练过程就和上面写的一样，一层接一层，直到模型精度改善不明显的时候就终止。而对于图像数据也同理，只不过window-slide由一维变为二维（类似于2D-convolution），不再赘述。整体流程如下图，画的非常明白。<br><img src="/images/gcForest/3.png"></p>
<h3 id="DF-vs-DNN"><a href="#DF-vs-DNN" class="headerlink" title="DF vs DNN"></a>DF vs DNN</h3><p>再来看DF与对标对象DNN的参数对比，也是文章比较骄傲的一点。在训练DNN时候我们需要指定一大堆超参数，最常见的比如layer数、neuron数、dropout rate、batch size等等。但DF完全不用考虑这些超参，只用默认的参数就很好了。<br><img src="/images/gcForest/4.png"></p>
<h3 id="Performance"><a href="#Performance" class="headerlink" title="Performance"></a>Performance</h3><p>不多说，直接上图。<br><img src="/images/gcForest/5.png"></p>
<p>总的来说，这些评估结果都是基于小数据集的，在大数据集上还没测试过。但不可否认的是，相对于DL来讲效率还是要快不少，而且也没用到GPU资源。要知道现在普遍大公司里计算资源已经不是瓶颈了，而是如何把这些复杂的算法在线或是移动端应用起来，所以在这种趋势下，文章提出的这种方法还是极有意义的。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/03/01/mf/" itemprop="url">
                  矩阵分解：从入门到高级入门
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-03-01T21:48:42+08:00" content="2017-03-01">
              2017-03-01
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>矩阵分解可以视为无监督学习的一种形式，成名于Netflix Prize推荐比赛，由Yehuda Koren引申出诸多变种，在推荐系统中可谓是简单粗暴，非常有效。</p>
<p>矩阵分解，顾名思义，就是把已有的目标矩阵分解为两个矩阵相乘的形式，那么目标矩阵是什么？在推荐系统里一般我们可以理解为是用户（user）-商品（item）-评分（rating）所构成的矩阵。矩阵的每一行代表一个user，每一列代表item，矩阵元素$r_{ij}$代表第i个user为第j个item打得分数，可以是浏览数、加购数、购买数、评论数等等，如果第i个user对第j个item没有任何行为，我们令该矩阵元素为空。而我们的目标就是将这些空元素补齐，用以推断每个user对没有行为的这些item的兴趣度大小究竟是多少，从而找到用户可能最感兴趣的top-K个商品进行推荐。</p>
<h3 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h3><p>假设矩阵$R$维度为$M  \times N$，也就是用户为$M$个，item为$N$个，目标是将$R$分解为$X \cdot Y$的形式，其中$X$维度为$M  \times K$，$Y$维度为$K \times N$，$K &lt;&lt; M,N$，这里的$K$称为隐因子维度。我们可以把user的隐因子解释为用户的兴趣维度，把item的隐因子解释为商品的属性维度，分解出的结果同样也可以基于cosine similarity计算用户之间的相似度和商品之间的相似度。其实传统的SVD同样也是这种思想，但是由于这里我们的场景有大量的缺失值SVD无法操作，盲目的填补缺失值也会造成模型的过拟合，因此也衍生出了大量诸如SVD++，RSVD等SVD加强版。事实上最基础和原始的矩阵分解优化目标形式非常简洁，思想就是尽可能的可以复原出已有矩阵中非空元素，使预测元素与原始元素保持一致，即</p>
<script type="math/tex; mode=display">
min \sum_{u,i \in ratings} (r_{ui} - \hat r_{ui})^2 = min \sum (r_{ui} - x_u^T y_i)^2</script><p>当然，还少不了regulation term，以防止overfitting，这样目标就变得复杂一点</p>
<script type="math/tex; mode=display">
min \sum (r_{ui} - x_u^T y_i)^2 + \lambda(\sum_{u} \parallel x_u\parallel ^2  + \sum_{i} \parallel y_i \parallel^2 )</script><h4 id="Bias"><a href="#Bias" class="headerlink" title="Bias"></a>Bias</h4><p>有了这个基础版本的目标函数，其实就可以在上面进行修修改改，逐步完善。考虑到在实际推荐系统中user和item的多样性，我们还可以在此基础上加上user、item的bias项。举个直观例子来解释这个概念，假如现在要预测用户A对商品a的打分，已经知道的是在所有的商品里均值得分为4分，而a相比所有商品来说质量和口碑要相对好一些，因此均值要高出1分，但用户A又是一个比较较真和挑剔的用户，所以他对每个商品的打分要比所有人打分均值低0.5分，因此如果暂且不考虑兴趣因素，最终A对a的打分可以粗略估计为4+1-0.5=4.5，这里这些均值偏移数据就是所谓的bias项修正。如此目标就又复杂一些</p>
<script type="math/tex; mode=display">
min \sum (r_{ui} - \beta_u - \gamma_i - x_u^T y_i)^2 + \lambda(\sum_{u} (\parallel x_u\parallel ^2  + \beta_u^2)+ \sum_{i} (\parallel y_i \parallel^2 + \gamma_i^2)</script><h4 id="Implicit-Feedback"><a href="#Implicit-Feedback" class="headerlink" title="Implicit Feedback"></a>Implicit Feedback</h4><p>上面说了一大堆，但是有比较核心的一点被忽略了，那就是在现实的应用场景中，“rating”，也就是打分这个概念我们几乎是获取不到的。这相当于是用户给出最积极的反馈，比如豆瓣电影的主动打分，但是大多数用户在网站网行的行为也不过是点击、浏览，在电商场景中可能更多一些，比如关注、加购等，在一些音乐、影视平台上可能有收听、观看和评论等。但总的说来，这些都不能够让我们具体的概化为一个“打分”，或者称为显示反馈，所以需要引入隐式反馈这个概念来进一步刻画矩阵分解的优化目标。</p>
<p>首先引入一个二值变量，定义</p>
<script type="math/tex; mode=display">
p_{ui}= \begin{cases} 1, & \text {if $r_{ui}$ > 0} \\ 0, & \text{if $r_{ui}$ = 0} \end{cases}</script><p>这里的$r_{ui}$就不再是打分数据了，而是用户对商品有过任意行为的次数，而二值变量可以理解为一个用户对一个商品有过是/否有过任何行为。接下来优化目标也随之改变</p>
<script type="math/tex; mode=display">
min \sum c_{ui} (p_{ui} - \beta_u - \gamma_i - x_u^T y_i)^2 + \lambda(\sum_{u} (\parallel x_u\parallel ^2  + \beta_u^2)+ \sum_{i} (\parallel y_i \parallel^2 + \gamma_i^2)</script><p>，其中$c_{ui}$称为置信度，通常定义为</p>
<script type="math/tex; mode=display">
c_{ui} = 1 + \alpha r_{ui}</script><p>$\alpha$与$\gamma$同样都是hyper parameter，需要交叉验证来确定，对置信度定性的解释也就是用户对指定商品的行为越多，我们就越有理由认为用户对这个商品是真正感兴趣的，比较类似于统计检验中置信度的概念。设想我们在网购时买一个自己喜欢的东西基本上都要看来看去好多次甚至好多天，但如果是帮人买，多数情况下的行为是与这个商品只接触一次，买完即走。更generalized的想法是，把$c_{ui}$解释为用户对商品的行为权重，对电商而言，如果仅仅浏览过，权重最小，浏览次数大于一定阈值时，权重次之，加购过，权重再次加大，购买过，权重最大。</p>
<p>如果考虑得再复杂一点，可以把$x_u^T y_i$更加具体化，比如加上用户的属性特征$U$，有过行为的商品embedding归一化特征$I$，扩充用户的隐因子矩阵$x$，就变为</p>
<script type="math/tex; mode=display">
(x_u + |N(u)|^{-0.5}\sum_{i \in N(u)}I_i + \sum U_{\alpha})^Ty_i</script><p>对于$y_i$，如果有何必要的话也可以同样进行这种扩充方式处理，从而解决一些数据不足或冷启动的问题。</p>
<h3 id="学习算法"><a href="#学习算法" class="headerlink" title="学习算法"></a>学习算法</h3><p>最小化目标函数的方法主要有梯度下降法和交替最小二乘法（Alternative Least Square, ALS），接下来分别简要说明一下。</p>
<h4 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h4><p>其实严格的来说，矩阵分解的梯度下降应该叫做随机梯度下降（逐个元素更新）。先看最基础的，定义损失函数</p>
<script type="math/tex; mode=display">
e_{ij}^2 = (r_{ij} - \hat r_{ij})^2 = (r_{ij} - \sum_kx_{ik} y_{kj})^2</script><p>，待更新参数为$x<em>{ik}$和$y</em>{kj}$，每一步梯度更新后两个参数分别为</p>
<script type="math/tex; mode=display">
x_{ik}^‘ = x_{ik} - \eta \frac{\partial}{\partial x_{ik}} e_{ij}^2 =  x_{ik} + 2\eta (r_{ij} - \hat r_{ij})y_{kj} =  x_{ik} + 2\eta e_{ij} y_{kj}\\
y_{kj}^‘ = y_{kj} - \eta \frac{\partial}{\partial y_{kj}} e_{ij}^2 =  y_{kj} + 2\eta (r_{ij} - \hat r_{ij})x_{ik} =  y_{kj} + 2\eta e_{ij} x_{ik}</script><p>加上正则项后，形式类似，</p>
<script type="math/tex; mode=display">
x_{ik}^‘ =  x_{ik} + 2\eta (e_{ij} y_{kj} - \lambda x_{ik})\\
y_{kj}^‘ =  y_{kj} + 2\eta (e_{ij} x_{ik} - \lambda y_{ij})</script><p>这里有一点需要注意，并不是$X$和$Y$中的所有元素都需要按顺序依次更新，因为目标矩阵的稀疏性导致大量rating值缺失，所以只需要更新那些$e_{ij}$不为空对应的user和item矩阵元素。</p>
<h5 id="python的简单实现"><a href="#python的简单实现" class="headerlink" title="python的简单实现"></a>python的简单实现</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">matrix_factorization</span><span class="params">(R, X, Y, rank, iter, lr = <span class="number">0.01</span>, gamma =<span class="number">0.02</span>)</span>:</span></div><div class="line">    Y = Y.T</div><div class="line">    <span class="comment"># update user-item latent factor</span></div><div class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> xrange(iter):</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> xrange(R.shape[<span class="number">0</span>]):</div><div class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> xrange(R.shape[<span class="number">1</span>]):</div><div class="line">                <span class="keyword">if</span> R[i][j] &gt; <span class="number">0</span>:</div><div class="line">                    eij = R[i][j] - numpy.dot(X[i,:],Y[:,j])</div><div class="line">                    <span class="keyword">for</span> k <span class="keyword">in</span> xrange(rank):</div><div class="line">                        X[i][k] = X[i][k] + <span class="number">2</span> * lr * (eij * Y[k][j] - gamma * X[i][k])</div><div class="line">                        Y[k][j] = Y[k][j] + <span class="number">2</span> * lr * (eij * X[i][k] - gamma * Y[k][j])</div><div class="line">    <span class="keyword">return</span> X, Y.T</div><div class="line"></div><div class="line"></div><div class="line">R = numpy.array([[<span class="number">5</span>,<span class="number">3</span>,<span class="number">0</span>,<span class="number">1</span>],</div><div class="line">     [<span class="number">4</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>],</div><div class="line">     [<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">5</span>],</div><div class="line">     [<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">4</span>],</div><div class="line">     [<span class="number">0</span>,<span class="number">1</span>,<span class="number">5</span>,<span class="number">4</span>]])</div><div class="line">rank = <span class="number">2</span> </div><div class="line">X = numpy.random.rand(R.shape[<span class="number">0</span>], rank)</div><div class="line">Y = numpy.random.rand(R.shape[<span class="number">1</span>], rank)</div><div class="line">X_new, Y_new = matrix_factorization(R, X, Y, rank, <span class="number">1000</span>)</div><div class="line">R_new = numpy.dot(X_new, Y_new.T)</div><div class="line"><span class="keyword">print</span> <span class="string">"predict matrix:\n"</span>, R_new	</div><div class="line"><span class="keyword">print</span> <span class="string">"raw matrix:\n"</span>, R</div><div class="line">R = R.reshape(<span class="number">-1</span>)</div><div class="line">R_new = R_new.reshape(<span class="number">-1</span>)</div><div class="line">mse = <span class="number">0</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> xrange(R.size):</div><div class="line">    <span class="keyword">if</span> R[i] &gt; <span class="number">0</span>:</div><div class="line">        mse += pow(R[i] - R_new[i], <span class="number">2</span>) </div><div class="line"></div><div class="line"><span class="keyword">print</span> <span class="string">"MSE: "</span>, mse/R.size</div></pre></td></tr></table></figure>
<p>输出结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">predict matrix:</div><div class="line">[[ 4.95397969  2.96469808  4.39008599  1.00365562]</div><div class="line"> [ 3.96264341  2.38721232  3.71827269  1.00055567]</div><div class="line"> [ 1.00646214  0.98090633  5.85147223  4.94890212]</div><div class="line"> [ 0.99598939  0.8966756   4.82086649  3.96964276]</div><div class="line"> [ 1.2039566   1.01871901  4.97353667  3.98151943]]</div><div class="line">raw matrix:</div><div class="line">[[5 3 0 1]</div><div class="line"> [4 0 0 1]</div><div class="line"> [1 1 0 5]</div><div class="line"> [1 0 0 4]</div><div class="line"> [0 1 5 4]]</div><div class="line">MSE:  0.000506024533095</div></pre></td></tr></table></figure>
<h4 id="ALS"><a href="#ALS" class="headerlink" title="ALS"></a>ALS</h4><p>从目标函数$e_{ij}^2$的形式上可以看出来，这是个non-convex function，但却是bi-convex的。也就是说，在已知$x$的情况下，就是个关于$y$的quadratic形式的function，同理在已知$y$的情况下，就是个关于$x$的quadratic形式的function，而quadratic形式是一定能找到全局最优的。针对这种问题，我们就可以固定其中一个变量来更新另一个变量，然后做一次相反的操作，迭代多次，就可以获得最下二乘下的优化解，所以这种方法就称之为交替最小二乘。正是由于在每一次迭代都可以获得真正意义的全局最优，而非梯度下降方法每次迭代只在梯度方向移动一小步，因此ALS在理论上要比gradient descent方法收敛快很多。</p>
<p>开始推导，这里我们只针对带正则项的explicit feedback的目标函数形式，</p>
<script type="math/tex; mode=display">
min \sum (r_{ui} - x_u^T y_i)^2 + \lambda(\sum_{u} \parallel x_u\parallel ^2  + \sum_{i} \parallel y_i \parallel^2 )</script><p>先固定$y_i$，求$x_u$，求导有</p>
<script type="math/tex; mode=display">
\frac{\partial C}{\partial x_u} = -2\sum_i (r_{ui} - x_u^T y_i)y_i +2\lambda x_u
=-2Y_{I_i}R(u, I_i) + 2Y_{I_i} Y_{I_i}^T + 2\lambda x_u</script><p>令上式等于0，直接求最优，可得$x_u$表达式</p>
<script type="math/tex; mode=display">
x_u = (\lambda I + Y_{I_i} Y_{I_i}^T)^{-1}Y_{I_i}R(u, I_i)</script><p>简单解释下各个符号含义。假设用户维度为$N$维，商品维度为$M$维，隐因子的rank为$K$维，用户$u$打过分的商品为$m$个，那么$Y_{I_i}$代表用户$u$打过分的item集合的向量堆叠，即$K \times m$维，$R(u, I_i)$用户$u$打过分的商品得分，为一个$m*1$列向量，$I$为单位向量，维度为$K \times K$，$x_u$即为用户$u$的向量表示，是一个$k \times 1$的列向量。在更新完$x_u$后，同理可以更新$y_i$，由于目标函数是对称的，就不推导了，直接上公式</p>
<script type="math/tex; mode=display">
y_i = (\lambda I + X_{I_u} X_{I_u}^T)^{-1}X_{I_u}R(i, I_u)</script><p>含义和上面的刚好相反。如果商品$i$被$n$个用户打过分，那么$X_{I_u}$代表商品i被打过分的user集合的向量堆叠，即$K \times n$维，$R(i, I_u)$为商品$i$被打过的得分，为一个$n \times 1$的列向量，$I$为单位向量，维度为$K*K$，$y_i$即为商品$i$的向量表示，是一个$k \times 1$的列向量。在实际应用里，由于矩阵的求逆操作计算量很大，通常就通过解线性方程组的方法来求$x_u$和$y_i$。</p>
<script type="math/tex; mode=display">
(\lambda I + Y_{I_i} Y_{I_i}^T) x_u = Y_{I_i}R(u, I_i) \\
(\lambda I + X_{I_u} X_{I_u}^T) y_i = X_{I_u}R(i, I_u)</script><h5 id="python的简单实现-1"><a href="#python的简单实现-1" class="headerlink" title="python的简单实现"></a>python的简单实现</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">matrix_factorization</span><span class="params">(R, rank, iter, gamma =<span class="number">0.02</span>)</span>:</span></div><div class="line">	<span class="comment">#initialization</span></div><div class="line">	X = np.random.rand(R.shape[<span class="number">0</span>], rank)</div><div class="line">	Y = np.random.rand(R.shape[<span class="number">1</span>], rank)</div><div class="line">	I = np.eye(rank)</div><div class="line">	<span class="keyword">for</span> step <span class="keyword">in</span> xrange(iter):</div><div class="line">		<span class="keyword">for</span> user, rating <span class="keyword">in</span> enumerate(R):</div><div class="line">			is_rating_index = np.nonzero(rating)</div><div class="line">			A = np.dot(Y[is_rating_index].T, Y[is_rating_index]) + gamma * I</div><div class="line">			B = np.dot(Y[is_rating_index].T, rating[is_rating_index])</div><div class="line">			<span class="comment"># using solve method instead of calculating inverse matrix</span></div><div class="line">			X[user,:] = np.linalg.solve(A, B)</div><div class="line"></div><div class="line">		<span class="keyword">for</span> item, rating <span class="keyword">in</span> enumerate(R.T):</div><div class="line">			is_rating_index = np.nonzero(rating)</div><div class="line">			A = np.dot(X[is_rating_index].T, X[is_rating_index]) + gamma * I</div><div class="line">			B = np.dot(X[is_rating_index].T, rating[is_rating_index])</div><div class="line">			Y[item,:] = np.linalg.solve(A, B)</div><div class="line"></div><div class="line">	<span class="keyword">return</span> X, Y</div><div class="line"></div><div class="line"></div><div class="line">R = np.array([[<span class="number">5</span>,<span class="number">3</span>,<span class="number">0</span>,<span class="number">1</span>],</div><div class="line">     [<span class="number">4</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>],</div><div class="line">     [<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">5</span>],</div><div class="line">     [<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">4</span>],</div><div class="line">     [<span class="number">0</span>,<span class="number">1</span>,<span class="number">5</span>,<span class="number">4</span>]])</div><div class="line">rank = <span class="number">2</span> </div><div class="line">X, Y = matrix_factorization(R, rank, <span class="number">10</span>)</div><div class="line"></div><div class="line">R_new = X.dot(Y.T)</div><div class="line"><span class="keyword">print</span> <span class="string">"predict matrix:\n"</span>, 	R_new</div><div class="line"><span class="keyword">print</span> <span class="string">"raw matrix:\n"</span>, R</div><div class="line">R = R.reshape(<span class="number">-1</span>)</div><div class="line">R_new = R_new.reshape(<span class="number">-1</span>)</div><div class="line">mse = <span class="number">0</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> xrange(R.size):</div><div class="line">    <span class="keyword">if</span> R[i] &gt; <span class="number">0</span>:</div><div class="line">        mse += pow(R[i] - R_new[i], <span class="number">2</span>) </div><div class="line"></div><div class="line"><span class="keyword">print</span> <span class="string">"MSE: "</span>, mse/R.size</div></pre></td></tr></table></figure>
<p>输出结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">predict matrix:</div><div class="line">[[ 5.0161259   2.9859823  -3.84990048  1.01292524]</div><div class="line"> [ 3.9720155   2.38887852 -2.78909332  0.9972788 ]</div><div class="line"> [ 0.96012708  1.16133008  5.5264977   4.90613676]</div><div class="line"> [ 1.04836492  1.09127496  4.15699704  3.94456546]</div><div class="line"> [ 0.50838622  0.809576    4.99343708  4.15301915]]</div><div class="line">raw matrix:</div><div class="line">[[5 3 0 1]</div><div class="line"> [4 0 0 1]</div><div class="line"> [1 1 0 5]</div><div class="line"> [1 0 0 4]</div><div class="line"> [0 1 5 4]]</div><div class="line">MSE:  0.00514865377649</div></pre></td></tr></table></figure>
<p>至此，矩阵分解的基本原理和求解方法都已经啰嗦完了，但是现实应用中其实远没有这么简单，比如关于隐反馈的定义（用户-商品打分的量化），大数据量的内存问题（spark ALS处理亿量级的用户）等。总的来说坑还是很多的，所以这篇文章也只能算是从入门到高级入门。</p>
<h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p><a href="https://datajobs.com/data-science-repo/Recommender-Systems-%5BNetflix%5D.pdf" target="_blank" rel="external">Matrix factorization techniques for recommender systems</a><br><a href="https://endymecy.gitbooks.io/spark-ml-source-analysis/content/%E6%8E%A8%E8%8D%90/papers/Large-scale%20Parallel%20Collaborative%20Filtering%20the%20Netflix%20Prize.pdf" target="_blank" rel="external">Large-scale parallel collaborative filtering for the netflix prize</a><br><a href="http://yifanhu.net/PUB/cf.pdf" target="_blank" rel="external">Collaborative filtering for implicit feedback datasets</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel  sidebar-panel-active ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.png"
               alt="Nirvanada" />
          <p class="site-author-name" itemprop="name">Nirvanada</p>
          <p class="site-description motion-element" itemprop="description"></p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">18</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">6</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

      </section>

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Nirvanada</span>
</div>

<div class="powered-by">
  Powered by <a class="theme-link" href="http://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Muse
  </a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  



  




  
  
  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
  </script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });
  </script>
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


  

  

</body>
</html>
