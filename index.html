<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta property="og:type" content="website">
<meta property="og:title" content="Andante">
<meta property="og:url" content="https://nirvanada.github.io/index.html">
<meta property="og:site_name" content="Andante">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Andante">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://nirvanada.github.io/"/>





  <title>Andante</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Andante</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://nirvanada.github.io/2018/01/20/Attention/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Nirvanada">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Andante">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/01/20/Attention/" itemprop="url">关于机器翻译中的Attention机制</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-01-20T18:17:15+08:00">
                2018-01-20
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h4 id="NMT"><a href="#NMT" class="headerlink" title="NMT"></a>NMT</h4><p>在Attention机制没有被发明前，最新的Neural Machine Translation（NMT）模型的结构为decoder + encoder的形式。其中encoder负责将原始文本序列的信息压缩成一个fixed-length的向量，decoder负责在给定encoder的向量及转换文本序列当前词的情况下，预测下一个词的概率。可以数学描述为如下过程：</p>
<p>encoder的输出：</p>
<script type="math/tex; mode=display">
h_t = f(x_t, h_{t-1}) \\
c = h_{T_x}</script><p>其中$T_x$为输出文本的长度。decoder输出的条件概率：</p>
<script type="math/tex; mode=display">
p(y_t|\{y_1,...,y_{t-1}\}, c) = g(y_{t-1}, s_t, c) \\
s_t = f(y_{t-1}, s_{t-1}, c)</script><p>这里的$f$都是一个关于RNN的非线性函数，例如LSTM或Bi-LSTM。</p>
<img src="/images/Attention/encoder-decoder.png">
<h4 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h4><p>实际上，Attention机制引入的insight是由上面的建模方法的一些不足而来的。假设现在要对[I love you]做一次翻译任务，中文结果为[我爱你]，那么由上面的结构encoder的输出是包含[I],[love],[you]三个词信息量的向量，当decoder来了之后，[我]、[爱]、[你]三个字的翻译都是由这一个向量作为输入，而我们人类的直觉其实应该是：当翻译[我]的时候，[I]这个词的权重权重应该更大，而其他两个词[爱]和[你]的时候，也应该对应的是[love]和[you]的大权重。因此，从这点考虑出发，encoder的输出对于每一个待翻译的词不应该都是一致的，反之应该是一种动态的，对原始文本每个词（或者理解为time step）输出的向量进行加权的形式，这种机制在人类认知里就可以解释为[注意力]机制，即对不同词翻译的时候我们注意力集中的位置也不一样，于是就有了attention的由来，上面的数学描述就发生了一些变化。由于encoder需要对于不同词产生不同的向量，encoder的输出就变为</p>
<script type="math/tex; mode=display">
c_i = \sum_{j = 1}^{T_x} \alpha_{ij} h_j</script><p>这里$i$，$j$分别表示的是翻译文本和原始文本的词下标。这样，对于翻译文本的每个词，所产生的向量就是一种加权的形式，暂且先不理会这个权重$\alpha$是怎么来的，先看decoder的变化</p>
<script type="math/tex; mode=display">
p(y_t|{y_1,...,y_{t-1}}, c_i) = g(y_{t-1}, s_t, c_i) \\
s_t = f(y_{t-1}, s_{t-1}, c_i)</script><p>其实可以看到，本质上和上面的decoder根本没发生变化，只是将$c$变成了与词下标有关的$c_i$。</p>
<img src="/images/Attention/attention.png">
<p>最后看$\alpha$是怎么得到的，从下标上可以发现，最终得到的$\alpha$应该是一个矩阵，大小为原始文本长度*翻译文本长度，它表示的是原始文本中第$j$个词对翻译文本中第$i$个词的权重大小（重要程度），具体计算方法为</p>
<script type="math/tex; mode=display">
\alpha_{ij}= \frac{exp(e_{ij})}{\sum_{k =1 }^{T_x}exp(e_{ij})} \\
e_{ij} = a(s_{i-1}, h_j)</script><p>函数$a$可以定义为一个full-conneted layer，随着整个网路一起训练，物理意义上是将翻译文本第$i-1$位的翻译词输出的向量与原始文本第$j$个原始词输出向量整合到一起做了一次操作，反映了在准确的翻译成第$i$个目标词的前提下，后者对前者的重要度。</p>
<blockquote>
<p>The probability $\alpha_{ij}$, or its associated energy $e_{ij}$, reflects the importance of the annotation $h_j$ with respect to the previous hidden state $s_{i-1}$ in deciding the next state $s_i$ and generating $y_i$.</p>
</blockquote>
<p>更一般的，$e$的计算方法表达为</p>
<script type="math/tex; mode=display">
e_{ij} = v_a^Ttanh(W_a s_{i-1} + U_a h_j)</script><p>至此，基本的attention方法就结束了，下面一张图可能把整个流程描述的更清楚。值得一提的是，个人理解，attention机制的引入应该对双向RNN的依赖更强，因为翻译任务中某个待翻译词是对原始文本中特定位置的上下文敏感的，因此$h_j$中不仅要包含有上文的信息，也要包含有下文信息。</p>
<img src="/images/Attention/attention_2.png">
<p>参考资料</p>
<ol>
<li><a href="https://github.com/lvapeab/nmt-keras" target="_blank" rel="external">NMT-Keras</a></li>
<li><a href="https://github.com/tensorflow/nmt" target="_blank" rel="external">NMT-TensorFlow</a></li>
<li><a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="external">Neural Machine Translation by Jointly Learning to Align and Translate</a></li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://nirvanada.github.io/2018/01/17/aqi/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Nirvanada">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Andante">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/01/17/aqi/" itemprop="url">2017中国城市空气质量指数（aqi）爬取及分析</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-01-17T21:17:02+08:00">
                2018-01-17
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Data-Analysis/" itemprop="url" rel="index">
                    <span itemprop="name">Data Analysis</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h4 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h4><p>2016是我来北京第一年，这年年末和2017年的年初，雾霾在这个城市肆虐了一个冬天，还记得那时候每天出门都要带着口罩，卡着眼镜戴的特别难受。在公司里大家也时不时的讨论着空气质量、哪款口罩质量比较好、中午要不要出去吃饭等等与雾霾有关的话题。本科四年在哈尔滨，那时候冬天的雾虽然存在，但也没有让人感到窒息，研究生在武汉的三年也未见满大街都带着口罩的壮观景象。所以当时的想法是，连基本的生存都要成问题，还谈何工作、生活？既然无法改变空气质量，那可以做的就是选择一个空气质量说得过去的城市。于是选择了十个城市，在<a href="http://www.air-level.com/" target="_blank" rel="external">空气质量网站</a>上爬了2017年连续一年的空气质量，分别是北京、上海、杭州、珠海、深圳、广州、成都、苏州、厦门、哈尔滨。这些城市主要是以经济发展空间这个维度来选择的，个人认为也是我们这代人比较有倾向性的久居城市（哈尔滨是hometown，纯属关心；武汉呆了几年，觉得以后再也不会去了）。<br>爬虫代码在服务器上crontab每五分钟一次定时执行，最终结果最小粒度为小时，个别时间因为不明原因数据缺失。</p>
<h4 id="爬虫"><a href="#爬虫" class="headerlink" title="爬虫"></a>爬虫</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#!/usr/bin/python</span></div><div class="line"><span class="comment">#-*-coding:utf-8 -*-</span></div><div class="line"></div><div class="line"><span class="comment"># crawl.py</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> urllib</div><div class="line"><span class="keyword">import</span> re</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getHtml</span><span class="params">(url)</span>:</span></div><div class="line">    page = urllib.urlopen(url)</div><div class="line">    html = page.read()</div><div class="line">    <span class="keyword">return</span> html</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getImg</span><span class="params">(html)</span>:</span></div><div class="line">    reg = <span class="string">r'src="(.+?\.png)" class'</span></div><div class="line">    reg_time = <span class="string">r'&lt;h4&gt;更新时间 (.+?)&lt;/h4&gt;'</span></div><div class="line">    imgre = re.compile(reg)</div><div class="line">    titlere = re.compile(reg_time)</div><div class="line"></div><div class="line">    imgurl = re.findall(imgre,html)[<span class="number">0</span>]</div><div class="line">    title = re.findall(titlere, html)[<span class="number">0</span>][<span class="number">0</span>:<span class="number">13</span>]</div><div class="line"></div><div class="line">    urllib.urlretrieve(imgurl,<span class="string">'./aqi/%s.png'</span> %title)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">getaqi</span><span class="params">(html, file)</span>:</span></div><div class="line">    reg = <span class="string">r'aqi-bg aqi-level-(.+?)\"&gt;(.+?)&lt;/span&gt;'</span></div><div class="line">    reg_time = <span class="string">r'label label-info\"&gt;(.+?)发布&lt;/span&gt;'</span></div><div class="line">    aqire = re.compile(reg)</div><div class="line">    timere = re.compile(reg_time)</div><div class="line"></div><div class="line">    aqiurl = re.findall(aqire,html)[<span class="number">0</span>][<span class="number">1</span>]</div><div class="line">    index = aqiurl.split(<span class="string">" "</span>)[<span class="number">0</span>]</div><div class="line">    quality = aqiurl.split(<span class="string">" "</span>)[<span class="number">1</span>]</div><div class="line"></div><div class="line">    time = re.findall(timere, html)[<span class="number">0</span>].replace(<span class="string">"年"</span>,<span class="string">"-"</span>).replace(<span class="string">"月"</span>,<span class="string">"-"</span>).replace(<span class="string">"日"</span>,<span class="string">""</span>)</div><div class="line">    <span class="keyword">with</span> open(file, <span class="string">'r'</span>) <span class="keyword">as</span> f:</div><div class="line">        lines = f.readlines()</div><div class="line">        last_line = lines[<span class="number">-1</span>]</div><div class="line"></div><div class="line">    <span class="keyword">if</span> (last_line[<span class="number">0</span>:<span class="number">16</span>] != time):</div><div class="line">        <span class="keyword">with</span> open(file, <span class="string">"a"</span>) <span class="keyword">as</span> f:</div><div class="line">            f.write(<span class="string">'\n'</span> + time + <span class="string">'\t'</span> + index + <span class="string">'\t'</span> + quality)</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    <span class="comment">#get img</span></div><div class="line">    htmlimg = getHtml(<span class="string">"http://www.air-level.com/"</span>)</div><div class="line">    getImg(htmlimg)</div><div class="line"></div><div class="line">    <span class="comment">#get aqi</span></div><div class="line">    cityarr = (<span class="string">"shanghai"</span>,<span class="string">"beijing"</span>,<span class="string">"hangzhou"</span>,<span class="string">"zhuhai"</span>,<span class="string">"shenzhen"</span>,<span class="string">"guangzhou"</span>,<span class="string">"chengdu"</span>,<span class="string">"haerbin"</span>,<span class="string">"suzhou"</span>,<span class="string">"xiamen"</span>)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> cityarr:</div><div class="line">        htmlaqi = getHtml(<span class="string">"http://www.air-level.com/air/%s/"</span> %i)</div><div class="line">        getaqi(htmlaqi, <span class="string">"./aqi/%s"</span> %i)</div></pre></td></tr></table></figure>
<h4 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</div><div class="line">%matplotlib inline</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">city_lst = [<span class="string">"beijing"</span>, <span class="string">"chengdu"</span>, <span class="string">"guangzhou"</span>, <span class="string">"haerbin"</span>, <span class="string">"hangzhou"</span>, <span class="string">"shanghai"</span>, <span class="string">"shenzhen"</span>, <span class="string">"suzhou"</span>, <span class="string">"xiamen"</span>, <span class="string">"zhuhai"</span>]</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">df_lst = []</div><div class="line"><span class="keyword">for</span> city <span class="keyword">in</span> city_lst:</div><div class="line">    tmp = pd.read_table(city, names = [<span class="string">"tm"</span>, <span class="string">"aqi_"</span> + city, <span class="string">"lv_"</span> + city])</div><div class="line">    tmp[<span class="string">"tm"</span>] = pd.to_datetime(tmp[<span class="string">"tm"</span>].apply(<span class="keyword">lambda</span> x: x + <span class="string">":00"</span>))</div><div class="line">    df_lst.append(tmp)</div><div class="line"></div><div class="line">timeIndex = pd.date_range(<span class="string">"2017-01-07 00:00"</span>, <span class="string">"2018-01-06 23:00"</span>, freq=<span class="string">"H"</span>)</div><div class="line">timeIndex = timeIndex.to_series().to_frame().reset_index(drop = <span class="keyword">True</span>)</div><div class="line">timeIndex.columns = [<span class="string">"tm"</span>]</div><div class="line"></div><div class="line">df = pd.merge(timeIndex, df_lst[<span class="number">0</span>], how = <span class="string">'left'</span>, on = <span class="string">'tm'</span>)</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(city_lst)):</div><div class="line">    df = pd.merge(df, df_lst[i], how = <span class="string">'left'</span>, on = <span class="string">'tm'</span>)</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># hour to day (daily mean)</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">day_scale</span><span class="params">(df)</span>:</span></div><div class="line">    df_scale = pd.DataFrame()</div><div class="line">    df[<span class="string">"date"</span>] = df.tm.dt.date</div><div class="line">    df_scale[<span class="string">"date"</span>] = df[<span class="string">"date"</span>].unique()</div><div class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> [col <span class="keyword">for</span> col <span class="keyword">in</span> df.columns.tolist() <span class="keyword">if</span> <span class="string">"aqi"</span> <span class="keyword">in</span> col]:</div><div class="line">        df_scale[col] = df.groupby([<span class="string">"date"</span>])[col].mean().reset_index()[col]</div><div class="line">    <span class="keyword">return</span> df_scale</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># hour to month (monthly mean)</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">month_scale</span><span class="params">(df)</span>:</span></div><div class="line">    df_scale = pd.DataFrame()</div><div class="line">    df[<span class="string">"month"</span>] = df.index.map(<span class="keyword">lambda</span> x: str(int(x / df.shape[<span class="number">0</span>] * <span class="number">12</span>) + <span class="number">1</span>))</div><div class="line">    df_scale[<span class="string">"month"</span>] = df[<span class="string">"month"</span>].unique()</div><div class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> [col <span class="keyword">for</span> col <span class="keyword">in</span> df.columns.tolist() <span class="keyword">if</span> <span class="string">"aqi"</span> <span class="keyword">in</span> col]:</div><div class="line">        df_scale[col] = df.groupby([<span class="string">"month"</span>])[col].mean().reset_index()[col]</div><div class="line">    <span class="keyword">return</span> df_scale</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># hour scale</span></div><div class="line">plt.figure(figsize=[<span class="number">20</span>,<span class="number">10</span>])</div><div class="line"><span class="keyword">for</span> city <span class="keyword">in</span> city_lst:</div><div class="line">    plt.plot(df[<span class="string">"tm"</span>], df[<span class="string">"aqi_"</span> + city])</div><div class="line">plt.legend(prop=&#123;<span class="string">'size'</span>:<span class="number">16</span>&#125;)</div></pre></td></tr></table></figure>
<img src="/images/aqi/output_8_1.png">
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># day scale</span></div><div class="line">plt.figure(figsize=[<span class="number">20</span>,<span class="number">10</span>])</div><div class="line"><span class="keyword">for</span> city <span class="keyword">in</span> city_lst:</div><div class="line">    plt.plot(day_scale(df)[<span class="string">"date"</span>], day_scale(df)[<span class="string">"aqi_"</span> + city])</div><div class="line">plt.legend(prop=&#123;<span class="string">'size'</span>:<span class="number">16</span>&#125;)</div></pre></td></tr></table></figure>
<img src="/images/aqi/output_9_1.png">
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># month scale</span></div><div class="line">plt.figure(figsize=[<span class="number">20</span>,<span class="number">10</span>])</div><div class="line"><span class="keyword">for</span> city <span class="keyword">in</span> city_lst:</div><div class="line">    plt.plot(month_scale(df)[<span class="string">"month"</span>], month_scale(df)[<span class="string">"aqi_"</span> + city])</div><div class="line">plt.legend(prop=&#123;<span class="string">'size'</span>:<span class="number">16</span>&#125;)</div></pre></td></tr></table></figure>
<img src="/images/aqi/output_10_1.png">
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Moving Average</span></div><div class="line">df_day = day_scale(df)</div><div class="line">df_day.fillna(df_day.median(axis = <span class="number">0</span>), inplace=<span class="keyword">True</span>) <span class="comment">#should be no none value when using moving average method</span></div><div class="line"></div><div class="line">plt.figure(figsize=[<span class="number">20</span>,<span class="number">10</span>])</div><div class="line">plt.title(<span class="string">"30 days moving average"</span>, fontsize = <span class="number">18</span>)</div><div class="line"><span class="keyword">for</span> city <span class="keyword">in</span> city_lst:</div><div class="line">    plt.plot(df_day[<span class="string">"date"</span>], df_day[<span class="string">"aqi_"</span> + city].to_frame().rolling(<span class="number">30</span>).mean())</div><div class="line">plt.legend(city_lst, prop = &#123;<span class="string">'size'</span>:<span class="number">16</span>&#125;)</div><div class="line"></div><div class="line">plt.figure(figsize=[<span class="number">20</span>,<span class="number">10</span>])</div><div class="line">plt.title(<span class="string">"60 days moving average"</span>, fontsize = <span class="number">18</span>)</div><div class="line"><span class="keyword">for</span> city <span class="keyword">in</span> city_lst:</div><div class="line">    plt.plot(df_day[<span class="string">"date"</span>], df_day[<span class="string">"aqi_"</span> + city].to_frame().rolling(<span class="number">60</span>).mean())</div><div class="line">plt.legend(city_lst, prop = &#123;<span class="string">'size'</span>:<span class="number">16</span>&#125;)</div></pre></td></tr></table></figure>
<img src="/images/aqi/output_11_1.png">
<img src="/images/aqi/output_11_2.png">
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">plt.figure(figsize=[<span class="number">20</span>,<span class="number">10</span>])</div><div class="line">sns.boxplot(df[[col <span class="keyword">for</span> col <span class="keyword">in</span> df.columns <span class="keyword">if</span> <span class="string">"aqi"</span> <span class="keyword">in</span> col]])</div><div class="line">plt.ylim([<span class="number">0</span>,<span class="number">200</span>])</div><div class="line">plt.xticks(fontsize = <span class="number">16</span>)</div></pre></td></tr></table></figure>
<img src="/images/aqi/output_12_2.png">
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">lvIndex = pd.DataFrame(df.lv_beijing.dropna().unique()).reset_index(drop = <span class="keyword">True</span>)</div><div class="line">lvIndex.columns = [<span class="string">"lvIndex"</span>]</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">df_lv = df[<span class="string">"lv_"</span> + city_lst[<span class="number">0</span>]].value_counts().reset_index()</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(city_lst)):</div><div class="line">    df_lv = pd.merge(df_lv, df[<span class="string">"lv_"</span> + city_lst[i]].value_counts().reset_index(), how = <span class="string">'left'</span>, on = <span class="string">'index'</span>)</div><div class="line">df_lv = df_lv.fillna(<span class="number">0</span>)</div><div class="line">columns = df_lv[<span class="string">"index"</span>].map(&#123;<span class="string">"优"</span>:<span class="string">"lv_0"</span>, <span class="string">"良"</span>:<span class="string">"lv_1"</span>, <span class="string">"轻度污染"</span>:<span class="string">"lv_2"</span>, <span class="string">"中度污染"</span>:<span class="string">"lv_3"</span>, <span class="string">"重度污染"</span>:<span class="string">"lv_4"</span>, <span class="string">"严重污染"</span>:<span class="string">"lv_5"</span>, <span class="string">"极度污染"</span>:<span class="string">"lv_6"</span>&#125;)</div><div class="line">df_lv = df_lv.transpose().drop([<span class="string">"index"</span>])</div><div class="line">df_lv.columns = columns</div><div class="line">df_lv.plot(kind = <span class="string">'bar'</span>, stacked = <span class="keyword">True</span>, figsize = [<span class="number">20</span>,<span class="number">10</span>], fontsize = <span class="number">16</span>)</div></pre></td></tr></table></figure>
<img src="/images/aqi/output_14_1.png">
<h4 id="结论："><a href="#结论：" class="headerlink" title="结论："></a>结论：</h4><ol>
<li>滑动平均结果显示，夏季空气质量优于冬季</li>
<li>箱型图显示，南方沿海城市空气质量优于北方，且异常值很少且不会很异常，即，再差也差不到哪去</li>
<li>北京2017年冬季空气质量有明显好转，与相关政策干预河北省有关</li>
<li>哈尔滨和北京半斤八两，夏季哈尔滨是赢家，冬季北京是赢家。极度恶劣空气状况频率哈尔滨是赢家</li>
<li>在这个政策主导和经纬度跨度极大的国家，空气质量受到很多因素的影响，过去一年的统计结果未来未必依然具有说服力</li>
</ol>
<p>(待更新)<br>全年中国空气质量分布图的动态展示</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://nirvanada.github.io/2017/12/14/DCN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Nirvanada">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Andante">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/12/14/DCN/" itemprop="url">Deep and Cross Network原理及实现</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-12-14T18:17:15+08:00">
                2017-12-14
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Paper/" itemprop="url" rel="index">
                    <span itemprop="name">Paper</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="https://arxiv.org/pdf/1708.05123.pdf" target="_blank" rel="external">Deep &amp; Cross Network for Ad Click Predictions</a>，是四位在谷歌的中国人放出的一篇文章。题目也一目了然，是用复杂网络结构做CTR预估的，与2016年谷歌的wide and deep model非常相似，于是利用课余时间梳理并简单的实现了一下。</p>
<h4 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h4><img src="/images/DCN/structure.jpeg">
<h5 id="Input"><a href="#Input" class="headerlink" title="Input"></a>Input</h5><p>我们从wide and deep model切入，不论是wide侧还是deep侧，在输入端为embedding feature + cross feature + continous_feature。而这里就会存在一个小问题，在构造cross feature时，我们依然需要依靠hand-craft来生成特征，那么哪些分类特征做cross，连续特征如何做cross，都是需要解决的小麻烦。而来到了deep and cross model，则免去了这些麻烦，因为在输入层面，只有embedding column + continuous column，feature cross的概念都在网络结构中去实现的。</p>
<h5 id="Cross-Network"><a href="#Cross-Network" class="headerlink" title="Cross Network"></a>Cross Network</h5><p>将输入的embedding column + continous column定义为$x_0$（$x_0 \in R^d$），第$l+1$层的cross layer为</p>
<script type="math/tex; mode=display">
x_{l+1} = x_0 x_l^Tw_l + b_l + x_l</script><p>其中$w_l(w_l \in R^d)$和$b_l(b_l \in R^d)$为第$l$层的参数。这么看来，Cross这部分网络的总参数量非常少，仅仅为$layers * d * 2$，每一层的维度也都保持一致，最后的output依然与input维度相等。另一方面，特征交叉的概念体现在每一层，当前层的输出的higher-represented特征都要与第一层输入的原始特征做一次两两交叉。至于为什么要再最后又把$x_l$给加上，我想是借鉴了ResNet的思想，最终模型要去拟合的是$x_{l+1} - x_{l}$这一项残差。</p>
<img src="/images/DCN/cross.jpeg">
<h5 id="Deep-Network"><a href="#Deep-Network" class="headerlink" title="Deep Network"></a>Deep Network</h5><p>这部分就不多说了，和传统DNN一样，input进来，简单的N层full-connected layer的叠加，所以参数量主要还是在deep侧。</p>
<h5 id="Output"><a href="#Output" class="headerlink" title="Output"></a>Output</h5><p>Cross layer和Deep layer出来的输出做一次concat，对于多分类问题，过一个softmax就OK了。</p>
<h4 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h4><p>在实现上主要利用了Keras的Functional API Model，可以比较方便的自定义layer，数据集利用了文章中提到的Forest coverType数据，layer数及nueron数也都根据文章中写死，但有所不同的是文章中貌似没有对categorical feature做embedding ，而数据集中有一个cardinality为40的categorical feature，所以代码里对这个变量做了embedding，embedding维度也按文章中的公式指定。其他超参也所剩无几，几乎都是些weights的初始化方法。最后放上生成的网络结构及代码。</p>
<img src="/images/DCN/model.png">
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> keras.backend <span class="keyword">as</span> K</div><div class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</div><div class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense</div><div class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> Adam</div><div class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Input, Embedding, Reshape, Add</div><div class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Flatten, merge, Lambda</div><div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model</div><div class="line"><span class="keyword">from</span> keras.utils.vis_utils <span class="keyword">import</span> plot_model</div><div class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder, StandardScaler</div><div class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</div><div class="line"><span class="keyword">import</span> random</div><div class="line"></div><div class="line"><span class="comment"># similar to https://github.com/jrzaurin/Wide-and-Deep-Keras/blob/master/wide_and_deep_keras.py</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">feature_generate</span><span class="params">(data)</span>:</span></div><div class="line">    data, label, cate_columns, cont_columns = preprocessing(data)</div><div class="line">    embeddings_tensors = []</div><div class="line">    continuous_tensors = []</div><div class="line">    <span class="keyword">for</span> ec <span class="keyword">in</span> cate_columns:</div><div class="line">        layer_name = ec + <span class="string">'_inp'</span></div><div class="line">        <span class="comment"># For categorical features, we em-bed the features in dense vectors of dimension 6×(category cardinality)**(1/4)</span></div><div class="line">        embed_dim = data[ec].nunique() <span class="keyword">if</span> int(<span class="number">6</span> * np.power(data[ec].nunique(), <span class="number">1</span>/<span class="number">4</span>)) &gt; data[ec].nunique() \</div><div class="line">            <span class="keyword">else</span> int(<span class="number">6</span> * np.power(data[ec].nunique(), <span class="number">1</span>/<span class="number">4</span>))</div><div class="line">        t_inp, t_build = embedding_input(layer_name, data[ec].nunique(), embed_dim)</div><div class="line">        embeddings_tensors.append((t_inp, t_build))</div><div class="line">        <span class="keyword">del</span>(t_inp, t_build)</div><div class="line">    <span class="keyword">for</span> cc <span class="keyword">in</span> cont_columns:</div><div class="line">        layer_name = cc + <span class="string">'_in'</span></div><div class="line">        t_inp, t_build = continous_input(layer_name)</div><div class="line">        continuous_tensors.append((t_inp, t_build))</div><div class="line">        <span class="keyword">del</span>(t_inp, t_build)</div><div class="line">    inp_layer =  [et[<span class="number">0</span>] <span class="keyword">for</span> et <span class="keyword">in</span> embeddings_tensors]</div><div class="line">    inp_layer += [ct[<span class="number">0</span>] <span class="keyword">for</span> ct <span class="keyword">in</span> continuous_tensors]</div><div class="line">    inp_embed =  [et[<span class="number">1</span>] <span class="keyword">for</span> et <span class="keyword">in</span> embeddings_tensors]</div><div class="line">    inp_embed += [ct[<span class="number">1</span>] <span class="keyword">for</span> ct <span class="keyword">in</span> continuous_tensors]</div><div class="line">    <span class="keyword">return</span> data, label, inp_layer, inp_embed</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">embedding_input</span><span class="params">(name, n_in, n_out)</span>:</span></div><div class="line">    inp = Input(shape = (<span class="number">1</span>, ), dtype = <span class="string">'int64'</span>, name = name)</div><div class="line">    <span class="keyword">return</span> inp, Embedding(n_in, n_out, input_length = <span class="number">1</span>)(inp)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">continous_input</span><span class="params">(name)</span>:</span></div><div class="line">    inp = Input(shape=(<span class="number">1</span>, ), dtype = <span class="string">'float32'</span>, name = name)</div><div class="line">    <span class="keyword">return</span> inp, Reshape((<span class="number">1</span>, <span class="number">1</span>))(inp)</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># The optimal hyperparameter settings were 8 cross layers of size 54 and 6 deep layers of size 292 for DCN</span></div><div class="line"><span class="comment"># Embed "Soil_Type" column (embedding dim == 15), we have 8 cross layers of size 29   </span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(inp_layer, inp_embed, X, y)</span>:</span></div><div class="line">    <span class="comment">#inp_layer, inp_embed = feature_generate(X, cate_columns, cont_columns)</span></div><div class="line">    input = merge(inp_embed, mode = <span class="string">'concat'</span>)</div><div class="line">    <span class="comment"># deep layer</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">6</span>):</div><div class="line">        <span class="keyword">if</span> i == <span class="number">0</span>:</div><div class="line">            deep = Dense(<span class="number">272</span>, activation=<span class="string">'relu'</span>)(Flatten()(input))</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            deep = Dense(<span class="number">272</span>, activation=<span class="string">'relu'</span>)(deep)</div><div class="line"></div><div class="line">    <span class="comment"># cross layer</span></div><div class="line">    cross = CrossLayer(output_dim = input.shape[<span class="number">2</span>].value, num_layer = <span class="number">8</span>, name = <span class="string">"cross_layer"</span>)(input)</div><div class="line"></div><div class="line">    <span class="comment">#concat both layers</span></div><div class="line">    output = merge([deep, cross], mode = <span class="string">'concat'</span>)</div><div class="line">    output = Dense(y.shape[<span class="number">1</span>], activation = <span class="string">'softmax'</span>)(output)</div><div class="line">    model = Model(inp_layer, output) </div><div class="line">    print(model.summary())</div><div class="line">    plot_model(model, to_file = <span class="string">'model.png'</span>, show_shapes = <span class="keyword">True</span>)</div><div class="line">    model.compile(Adam(<span class="number">0.01</span>), loss = <span class="string">'categorical_crossentropy'</span>, metrics = [<span class="string">"accuracy"</span>])</div><div class="line">    model.fit([X[c] <span class="keyword">for</span> c <span class="keyword">in</span> X.columns], y, batch_size = <span class="number">256</span>, epochs = <span class="number">10</span>)</div><div class="line">    <span class="keyword">return</span> model</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(X, y, model)</span>:</span></div><div class="line">    y_pred = model.predict([X[c] <span class="keyword">for</span> c <span class="keyword">in</span> X.columns])</div><div class="line">    acc = np.sum(np.argmax(y_pred, <span class="number">1</span>) == np.argmax(y, <span class="number">1</span>)) / y.shape[<span class="number">0</span>]</div><div class="line">    print(<span class="string">"Accuracy: "</span>, acc)</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># https://keras.io/layers/writing-your-own-keras-layers/</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">CrossLayer</span><span class="params">(layers.Layer)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, output_dim, num_layer, **kwargs)</span>:</span></div><div class="line">        self.output_dim = output_dim</div><div class="line">        self.num_layer = num_layer</div><div class="line">        super(CrossLayer, self).__init__(**kwargs)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, input_shape)</span>:</span></div><div class="line">        self.input_dim = input_shape[<span class="number">2</span>]</div><div class="line">        self.W = []</div><div class="line">        self.bias = []</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.num_layer):</div><div class="line">            self.W.append(self.add_weight(shape = [<span class="number">1</span>, self.input_dim], initializer = <span class="string">'glorot_uniform'</span>, name = <span class="string">'w_'</span> + str(i), trainable = <span class="keyword">True</span>))</div><div class="line">            self.bias.append(self.add_weight(shape = [<span class="number">1</span>, self.input_dim], initializer = <span class="string">'zeros'</span>, name = <span class="string">'b_'</span> + str(i), trainable = <span class="keyword">True</span>))</div><div class="line">        self.built = <span class="keyword">True</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, input)</span>:</span></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.num_layer):</div><div class="line">            <span class="keyword">if</span> i == <span class="number">0</span>:</div><div class="line">                cross = Lambda(<span class="keyword">lambda</span> x: Add()([K.sum(self.W[i] * K.batch_dot(K.reshape(x, (<span class="number">-1</span>, self.input_dim, <span class="number">1</span>)), x), <span class="number">1</span>, keepdims = <span class="keyword">True</span>), self.bias[i], x]))(input)</div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                cross = Lambda(<span class="keyword">lambda</span> x: Add()([K.sum(self.W[i] * K.batch_dot(K.reshape(x, (<span class="number">-1</span>, self.input_dim, <span class="number">1</span>)), input), <span class="number">1</span>, keepdims = <span class="keyword">True</span>), self.bias[i], input]))(cross)</div><div class="line">        <span class="keyword">return</span> Flatten()(cross)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_output_shape</span><span class="params">(self, input_shape)</span>:</span></div><div class="line">        <span class="keyword">return</span> (<span class="keyword">None</span>, self.output_dim)</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># modify the embedding columns here</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocessing</span><span class="params">(data)</span>:</span></div><div class="line">    <span class="comment"># inverse transform one-hot to continuous column</span></div><div class="line">    df_onehot = data[[col <span class="keyword">for</span> col <span class="keyword">in</span> data.columns.tolist() <span class="keyword">if</span> <span class="string">"Soil_Type"</span> <span class="keyword">in</span> col]]</div><div class="line">    <span class="comment">#for i in df_onehot.columns.tolist():</span></div><div class="line">    <span class="comment">#    if df_onehot[i].sum() == 0:</span></div><div class="line">    <span class="comment">#        del df_onehot[i]</span></div><div class="line">    data[<span class="string">"Soil"</span>] = df_onehot.dot(np.array(range(df_onehot.columns.size))).astype(int)</div><div class="line">    data.drop([col <span class="keyword">for</span> col <span class="keyword">in</span> data.columns.tolist() <span class="keyword">if</span> <span class="string">"Soil_Type"</span> <span class="keyword">in</span> col], axis = <span class="number">1</span>, inplace = <span class="keyword">True</span>)</div><div class="line">    label = np.array(OneHotEncoder().fit_transform(data[<span class="string">"Cover_Type"</span>].reshape(<span class="number">-1</span>, <span class="number">1</span>)).todense())</div><div class="line">    <span class="keyword">del</span> data[<span class="string">"Cover_Type"</span>]</div><div class="line">    cate_columns = [<span class="string">"Soil"</span>]</div><div class="line">    cont_columns = [col <span class="keyword">for</span> col <span class="keyword">in</span> data.columns <span class="keyword">if</span> col != <span class="string">"Soil"</span>]</div><div class="line">    <span class="comment"># Feature normilization</span></div><div class="line">    scaler = StandardScaler()</div><div class="line">    data_cont = pd.DataFrame(scaler.fit_transform(data[cont_columns]), columns = cont_columns)</div><div class="line">    data_cate = data[cate_columns]</div><div class="line">    data = pd.concat([data_cate, data_cont], axis = <span class="number">1</span>)</div><div class="line">    <span class="keyword">return</span> data, label, cate_columns, cont_columns</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</div><div class="line">    <span class="comment"># data download from https://www.kaggle.com/uciml/forest-cover-type-dataset/data</span></div><div class="line">    data = pd.read_csv(<span class="string">"./data/covtype.csv"</span>)</div><div class="line">    X, y, inp_layer, inp_embed = feature_generate(data)</div><div class="line">    </div><div class="line">    <span class="comment">#random split train and test by 9:1</span></div><div class="line">    train_index = random.sample(range(X.shape[<span class="number">0</span>]), int(X.shape[<span class="number">0</span>] * <span class="number">0.9</span>))</div><div class="line">    test_index = list(set(range(X.shape[<span class="number">0</span>])) - set(train_index))</div><div class="line">    model = fit(inp_layer, inp_embed, X.iloc[train_index], y[train_index, :])</div><div class="line">    evaluate(X.iloc[test_index], y[test_index, :], model)</div></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://nirvanada.github.io/2017/11/14/CapsuleNet/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Nirvanada">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Andante">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/11/14/CapsuleNet/" itemprop="url">Capsule Networks</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-11-14T16:46:15+08:00">
                2017-11-14
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Paper/" itemprop="url" rel="index">
                    <span itemprop="name">Paper</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Hinton上个月在axiv上甩出了一篇文章<a href="https://arxiv.org/abs/1710.09829" target="_blank" rel="external">Dynamic Routing Between Capsules</a>，作者在结构设计上弥补了一些cnn的设计缺陷。虽然作为行业领军人物，研究具有一定权威性，但也依然需要时间的考验和同行的不断推敲。</p>
<h4 id="Capsules"><a href="#Capsules" class="headerlink" title="Capsules"></a>Capsules</h4><p>在与cnn相关的研究中，这些年流行的网络框架都是基于conv+pooling的这种方式搭建的，即使是微软Resnet中引入了residual的概念，子网络依然是以这种方式组织起来。cnn中神经元的概念，无论是否共享权重，其实就是featureMap中的每一个具体的像素，是个标量。而capsules的将神经元扩展了一个维度，变成了一个向量，所以我们可以将capsules简单的理解为向量版的神经元。从网络上来说，cnn的forward过程是神经元与神经元的不断传播关系，而Capsules Network，显然，就是向量神经元之间的传播。</p>
<h4 id="Model-Structure"><a href="#Model-Structure" class="headerlink" title="Model Structure"></a>Model Structure</h4><p>以mnist数据集为例，step by step看model的每个环节都是怎样的输出（第一个维度为batch_size）</p>
<img src="/images/CapsuleNet/structure.jpg">
<h5 id="Layer-One"><a href="#Layer-One" class="headerlink" title="Layer One"></a>Layer One</h5><p><strong>Input</strong>: $[None,28,28,1]$</p>
<p>经过一层$9*9$，strides为1，通道数为256的conv layer</p>
<p><strong>Output</strong>:[None,20,20,256]</p>
<h5 id="Layer-Two"><a href="#Layer-Two" class="headerlink" title="Layer Two"></a>Layer Two</h5><p><strong>Input</strong>:$[None,20,20,256]$</p>
<p>经过一层$9*9$的，strides为2，通道数为128的conv layer，按照这种方式的setting下，输出应为$[None,6,6,32]$。但如果我们将这一层的conv layer复制8份，输出就变成了$[None,6,6,8,32]$，如果我们再将这个结果reshape到$[None,6*6*32,8] = [None,1152,8]$的维度，此时capsules的概念就可以体现出来了，即，这一层共有1152个$[1*8]$的capsules向量。</p>
<p><strong>Output</strong>:$[None,1152,8]$</p>
<h5 id="Layer-Three"><a href="#Layer-Three" class="headerlink" title="Layer Three"></a>Layer Three</h5><p><strong>Input</strong>:$[None,1152,8]$</p>
<p>这一层在维度上的变化略微复杂。对于一个指定index为$i$的$capsule_{i}$，其维度为$[1*8]$，引入一个维度为$[8*16]$的$Weight_{ij}$，那么</p>
<script type="math/tex; mode=display">
\hat{capsule_{j|i}} = Weight_{ij}capsule_i</script><p>上面的式子输出维度为$[1*16]​$，而$i\in(1,1152)​$，$j\in(1,10)​$，所以共有$1152*10​$个$\hat{capsule_{j|i}}​$。直观上的理解是，上面的操作我们令其进行10次，一次操作代表一个类别，加上这样的capsule一共存在1152个，所以$Weight​$的维度为$[1152,10,8,16]​$，所以截止在$\hat{capsule}​$的输出维度为$[None,1152,10,1,16]​$。</p>
<p>接着，再引入一个维度为$[1152,10,1,1]$的bias，首先对bias做一次按第二个维度（10）的softmax压缩，维度保持不变</p>
<script type="math/tex; mode=display">
c_{ij} = \frac{exp(bias_{ij})}{\sum_k exp(bias_{ik})}</script><p>可以看到，这里的$c_{ij}$就是一个标量，用它将所有capsules建立起联系，做一次对1152个capsules的sum，即</p>
<script type="math/tex; mode=display">
s_j = \sum_i c_{ij}\hat{capsule_{j|i}}</script><p>可以看到，由于是按$i$求和，因此维度与$\hat{capsule_{j|i}}$一致（$[1*16]$），每一个$s_j$也都可以看成是capsule。由于Hinton在文中提到，capsule的模长表示概率，方向表示属性（真的很难理解。。），因此在输出层之前还要将每个capsule做一次非线性压缩</p>
<script type="math/tex; mode=display">
v_j = \frac{||s_j||^2}{1+||s_j||^2} \frac{s_j}{||s_j||}</script><p><strong>Output</strong>:$[None,10,16]$</p>
<p>另外可以注意到，文章的题目中有dynamic routing的关键字，这个思想也穿插在上面的pipeline里。从公式2到公式4，可以概化为一个公式，</p>
<script type="math/tex; mode=display">
cupsule_{layer2} = squashing(\sum softmax(bias)*capsule_{layer1})</script><p>这里其实我们只进行了一次上面的操作，即将layer1的capsule transform到layer2的capsule，所以衍生出可以进行多次循环来获取更高level的capsule，而这个循环capsule高阶表达的过程就成为routing。具体的做法就是不断通过输出的高层capsule回传到低层capsule，从而更新bias（只更新bias，其余参数不更新），即</p>
<script type="math/tex; mode=display">
bias_{ij} = bias_{ij} + \hat{capsule_{j|i}} v_j</script><p>剥离上面公式的维度信息，发现$\hat{capsule_{j|i}} v_j$与$v_{ij}$均为$[16*1]$的向量，而$bias_{ij}$为一个标量，完全match，具体routing流程见下图。</p>
<img src="/images/CapsuleNet/routing.jpg">
<p>在这也顺便粘上一句原文的话，说明的是每个layer之间的capsule究竟是用来干嘛的，可以好好体会一下，就一句话已经看的云里雾里了。</p>
<blockquote>
<p>In convolutional capsule layers, each capsule outputs a local grid of vectors to each type of capsule in the layer above using different transformation matrices for each member of the grid as well as for each type of capsule.</p>
</blockquote>
<h5 id="Layer-Four"><a href="#Layer-Four" class="headerlink" title="Layer Four"></a>Layer Four</h5><p><strong>Input:</strong>$[None,10,16]$</p>
<p>上面已经说到了，capsule的模长表示概率，因此直接将上层输入做一次范数即可，输出的维度即为所有类别总数</p>
<p><strong>Output</strong>:$[None,10]$</p>
<h5 id="Reconstruction-Layer"><a href="#Reconstruction-Layer" class="headerlink" title="Reconstruction Layer"></a>Reconstruction Layer</h5><p><strong>Input</strong>:$[None,10,16]$</p>
<p>上面所描述的layer均为正向预测的过程，而通过top layer的capsule还原回原始图片质量的好坏也是能证明模型表达能力的一种评价方式，所以作者又引入了reconstruction layer，来对图像进行重构。首先将Layer Four中的输入拿过来，用真实class对张量进行一次掩模，只保留真实类别对应的vector，维度为$[None,16]$，然后连续接上三个Full Connected Layer，大小分别为512，1024和728，激活函数分别为ReLU，ReLU和Sigmoid，最后再讲728维向量reshape到$[28*28]$，从而还原了原始图像。</p>
<p><strong>Output</strong>:$[None,28,28,1]$</p>
<img src="/images/CapsuleNet/reconstruction.jpg">
<h4 id="Loss-Defination"><a href="#Loss-Defination" class="headerlink" title="Loss Defination"></a>Loss Defination</h4><p>说完了上面的结构，应该可以清晰的看出来，loss由两部分构成，一部分为网络正向传播上层capsule模预测为哪个class的loss-1，另一部分为重构网络还原的图片与真实图片的差异loss-2。其中loss-1使用类似于svm的margin loss，加入了一些小trick，而loss-2则使用的是重构与真实pixel-to-pixel的平方损失。形式为</p>
<script type="math/tex; mode=display">
loss = loss_1 + \alpha loss_2 = T_k max(0, m^+ - ||v_k||)^2+ \lambda(1-T_k)max(0, ||v_k|| - m^-)^2 + \alpha \sum_{28}\sum_{28}(x_{ri}-x_{ti})^2</script><p>其中$T_{k} \in (0,1)$表示图像中是否存在第k个类（这里不太一样的是，学习的图片里允许有多个类别，文章在实验中也加入两个类别分离的实验），$m^+$与$m^-$分别表示类比预测的upperBound和lowerBound，引入$\lambda(==0.5)$的作用是为了减弱不存在类capsule在初始阶段的学习，以防止对其他的capsule影响过大？（没太理解），$x_{ri}$和$x_{ti}$分别表示第$i$个重构像素与真实像素。为了尽量不影响正向构造capsule的能力，重构loss稀疏$\alpha$在文章中取得极小（0.0005）。</p>
<h4 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h4><p>Keras版本：<a href="https://github.com/XifengGuo/CapsNet-Keras" target="_blank" rel="external">https://github.com/XifengGuo/CapsNet-Keras</a></p>
<p>Layer的模型参数<br><img src="/images/CapsuleNet/param.jpg"></p>
<h4 id="Qustions"><a href="#Qustions" class="headerlink" title="Qustions"></a>Qustions</h4><ol>
<li>squashing的形式为何是这样?</li>
<li>多次routing的作用是什么？</li>
<li>为什么采用margin loss？</li>
<li>包含有capsule的网络结构中，激活函数在哪里？</li>
<li>cifar数据集表现不佳的原因？</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://nirvanada.github.io/2017/09/18/AFM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Nirvanada">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Andante">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/09/18/AFM/" itemprop="url">Attentional FM及源码解析</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-09-18T18:17:15+08:00">
                2017-09-18
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h4 id="原理部分"><a href="#原理部分" class="headerlink" title="原理部分"></a>原理部分</h4><p>书接上回，FM模型无论是对于交叉特征的捕捉能力上，还是在工业界大规模数据下的运用方面上，都具有出色的表现。因此，在深度学习火热的大背景下，各种基于FM模型和神经网络模型相结合的方法也开始频频出现。这里要说的是SIGIR 2017和IJCAI 2017的两篇文章<a href="http://www.comp.nus.edu.sg/~xiangnan/papers/sigir17-nfm.pdf" target="_blank" rel="external">Neural Factorization Machine</a>和<a href="http://www.comp.nus.edu.sg/~xiangnan/papers/ijcai17-afm.pdf" target="_blank" rel="external">Attentional Facotrization Machine</a>，无一例外，它们都是在FM的基础上衍生出与深度学习相关的研究。由于两篇文章的是同一团队所著，所以工作有重叠和改进部分，前一篇网络结构相对简单，因此本文主要针对Attentional Facotrization Machine展开，但也会在其中涉及Neural Factorization Machine。不仅如此，作者也同样给出了开源实现，源码解析也会在此基础上进行（<a href="https://github.com/hexiangnan/attentional_factorization_machine" target="_blank" rel="external">Neural FM</a> and <a href="https://github.com/hexiangnan/neural_factorization_machine" target="_blank" rel="external">Attention FM</a>）。</p>
<p>FM模型虽然存在了所有可能的二阶特征交叉项，但本质上，并不是所有的特征都是有效的，比如一个垃圾特征a和一个牛逼特征b做交叉可能会起到一些作用，但a和垃圾特征c再做交叉基本上不会有任何作用，因此Attentional Facotrization Machine解决的问题主要就是——利用目前比较流行的attention的概念，对每个特征交叉分配不同的attention，使没用的交叉特征权重降低，有用的交叉特征权重提高，加强模型预测能力。原始的FM模型，如果忽略一阶项，可以表示为</p>
<script type="math/tex; mode=display">
y_{fm} = \sum_i \sum_{j = i + 1} <v_i, v_j> x_i x_j</script><p>这里我们可以把$v_i$理解为特征$x_i$的feature embedding，而上式中交叉项的表达为embedding向量的内积，因此输入的$y$直接就为标量，不需要再次的映射；但是这种内积操作如果变换为哈达玛积的操作（element-wise product）$(v_i \odot v_j) x_i x_j$，这样从Neural Network的角度去看，每个特征都交叉之后上层就变成了一个embedding layer，然后再对这个layer做一次sum pooling和映射，便可以得到输出值，即</p>
<script type="math/tex; mode=display">
y = p^T\sum_i \sum_{j = i + 1} (v_i \odot v_j) x_i x_j</script><p>至此，Neural Factorization Machine中所描述的已经叙述完毕，就是这么简单。而来到了Attention这篇，思想一致，只是在embedding layer的sum pooling时为每个embedding加上对应的attention，也就是权重，即</p>
<script type="math/tex; mode=display">
y = p^T\sum_i \sum_{j = i + 1} \alpha_{ij}(v_i \odot v_j) x_i x_j</script><p>其中$\alpha$通过对embedding空间做了一次encoding并标准化得到，</p>
<script type="math/tex; mode=display">
\alpha_{ij}^{'} = h^TReLU(W(v_i \odot v_j) x_i x_j + b) \\
\alpha_{ij} = \frac{exp(\alpha_{ij}^{'})}{\sum exp(\alpha_{ij}^{'})}</script><p>另外，由于这种attention的方式对训练数据的拟合表达更充分，但也更容易过拟合，因此作者除了在loss中加入正则项之外，在attention部分加入了dropout。具体模型结构如下图：</p>
<img src="/images/AFM/net.jpeg">
<p>作者在Frappe和MovieLens数据集上进行了实验，AFM的表现beat掉了所有fine tuned的对比模型（LibFm、HOFM、Wide&amp;Deep、DeepCross）。而且，使用这套框架的传统FM也要比LibFM的效果好，原因之一在于dropout的引入降低了过拟合的风险，第二在于LibFM在迭代优化时使用的是SGD方法，对每个参数的学习步长一致，很容易使后期的学习很快的止步，而Adagrad的引入使得学习率可变，因此性能更加优良。另外，attention的引入，一方面加速了收敛速度，另一方面也具有对交叉特征的可解释性（因为就是权重），类似于特征重要性。</p>
<h4 id="代码部分"><a href="#代码部分" class="headerlink" title="代码部分"></a>代码部分</h4><p>截取了一部分关键代码，解析在注释中体现</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_init_graph</span><span class="params">(self)</span>:</span></div><div class="line">    self.graph = tf.Graph()</div><div class="line">    <span class="keyword">with</span> self.graph.as_default(): </div><div class="line">        <span class="comment"># Input data.</span></div><div class="line">        self.train_features = tf.placeholder(tf.int32, shape=[<span class="keyword">None</span>, <span class="keyword">None</span>], name=<span class="string">"train_features_afm"</span>)  <span class="comment"># None * features_M</span></div><div class="line">        self.train_labels = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>, <span class="number">1</span>], name=<span class="string">"train_labels_afm"</span>)  <span class="comment"># None * 1</span></div><div class="line">        self.dropout_keep = tf.placeholder(tf.float32, shape=[<span class="keyword">None</span>], name=<span class="string">"dropout_keep_afm"</span>)</div><div class="line"></div><div class="line">        <span class="comment"># Variables.</span></div><div class="line">        self.weights = self._initialize_weights()</div><div class="line"></div><div class="line">        <span class="comment"># Model.</span></div><div class="line">         <span class="comment"># _________ Embedding part _____________</span></div><div class="line">        <span class="comment"># data为libsvm格式，且非none特征共有M‘个，且特征值均为1。data中每个feature都对应一个K维embedding</span></div><div class="line">        self.nonzero_embeddings = tf.nn.embedding_lookup(self.weights[<span class="string">'feature_embeddings'</span>], self.train_features) <span class="comment"># None * M' * K</span></div><div class="line">        </div><div class="line">        <span class="comment"># 交叉项embedding layer生成，对应equation(2)</span></div><div class="line">        element_wise_product_list = []</div><div class="line">        count = <span class="number">0</span></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, self.valid_dimension):</div><div class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(i+<span class="number">1</span>, self.valid_dimension):</div><div class="line">               element_wise_product_list.append(tf.multiply(self.nonzero_embeddings[:,i,:], self.nonzero_embeddings[:,j,:]))</div><div class="line">                count += <span class="number">1</span></div><div class="line">        self.element_wise_product = tf.stack(element_wise_product_list) <span class="comment"># (M'*(M'-1))/2 * None * K</span></div><div class="line">        self.element_wise_product = tf.transpose(self.element_wise_product, perm=[<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>], name=<span class="string">"element_wise_product"</span>) <span class="comment"># None * (M'*(M'-1))/2 * K</span></div><div class="line">        </div><div class="line">        <span class="comment"># _________ MLP Layer / attention part _____________</span></div><div class="line">        <span class="comment"># self.attention == True时，模型为Attention FM;</span></div><div class="line">        <span class="comment"># self.attention == False时，就退化为Neural FM</span></div><div class="line">        num_interactions = self.valid_dimension*(self.valid_dimension<span class="number">-1</span>)/<span class="number">2</span></div><div class="line">        <span class="keyword">if</span> self.attention:</div><div class="line">            <span class="comment"># 对应equation(4)-1</span></div><div class="line">            self.attention_mul = tf.reshape(tf.matmul(tf.reshape(self.element_wise_product, shape=[<span class="number">-1</span>, self.hidden_factor[<span class="number">1</span>]]),self.weights[<span class="string">'attention_W'</span>]), shape=[<span class="number">-1</span>, num_interactions, self.hidden_factor[<span class="number">0</span>]])</div><div class="line">            self.attention_exp = tf.exp(tf.reduce_sum(tf.multiply(self.weights[<span class="string">'attention_p'</span>], tf.nn.relu(self.attention_mul + self.weights[<span class="string">'attention_b'</span>])), <span class="number">2</span>, keep_dims=<span class="keyword">True</span>)) <span class="comment"># None * (M'*(M'-1)) * 1</span></div><div class="line">            self.attention_sum = tf.reduce_sum(self.attention_exp, <span class="number">1</span>, keep_dims=<span class="keyword">True</span>) <span class="comment"># None * 1 * 1</span></div><div class="line">            <span class="comment"># 对应equation(4)-2</span></div><div class="line">            self.attention_out = tf.div(self.attention_exp, self.attention_sum, name=<span class="string">"attention_out"</span>) <span class="comment"># None * (M'*(M'-1)) * 1</span></div><div class="line">            <span class="comment"># 在attention部分引入dropout</span></div><div class="line">            self.attention_out = tf.nn.dropout(self.attention_out, self.dropout_keep[<span class="number">0</span>]) <span class="comment"># dropout</span></div><div class="line">            self.AFM = tf.reduce_sum(tf.multiply(self.attention_out, self.element_wise_product), <span class="number">1</span>, name=<span class="string">"afm"</span>) <span class="comment"># None * K</span></div><div class="line">        <span class="keyword">else</span>:              <span class="comment"># Neural FM</span></div><div class="line">            self.AFM = tf.reduce_sum(self.element_wise_product, <span class="number">1</span>, name=<span class="string">"afm"</span>) <span class="comment"># None * K</span></div><div class="line">        </div><div class="line">        <span class="comment"># 在全连接（映射）部分二次引入dropout</span></div><div class="line">        self.AFM = tf.nn.dropout(self.AFM, self.dropout_keep[<span class="number">1</span>]) <span class="comment"># dropout</span></div><div class="line"></div><div class="line">        <span class="comment"># _________ out _____________</span></div><div class="line">        self.prediction = tf.matmul(self.AFM, self.weights[<span class="string">'prediction'</span>]) <span class="comment"># None * 1</span></div><div class="line">        <span class="comment"># 引入传统FM中的一阶项和常数项</span></div><div class="line">        Bilinear = tf.reduce_sum(self.prediction, <span class="number">1</span>, keep_dims=<span class="keyword">True</span>)  <span class="comment"># None * 1</span></div><div class="line">        self.Feature_bias = tf.reduce_sum(tf.nn.embedding_lookup(self.weights[<span class="string">'feature_bias'</span>], self.train_features) , <span class="number">1</span>)  <span class="comment"># None * 1</span></div><div class="line">        Bias = self.weights[<span class="string">'bias'</span>] * tf.ones_like(self.train_labels)  <span class="comment"># None * 1</span></div><div class="line">        self.out = tf.add_n([Bilinear, self.Feature_bias, Bias], name=<span class="string">"out_afm"</span>)  <span class="comment"># None * 1</span></div><div class="line"></div><div class="line">        <span class="comment"># Compute the loss.</span></div><div class="line">        <span class="comment"># 在loss中加入L2正则项</span></div><div class="line">        self.loss = tf.nn.l2_loss(tf.subtract(self.train_labels, self.out)) + tf.contrib.layers.l2_regularizer(self.lamda_attention)(self.weights[<span class="string">'attention_W'</span>])  <span class="comment"># regulizer</span></div><div class="line"></div><div class="line">        <span class="comment"># Optimizer.</span></div><div class="line">        <span class="comment"># 传统FM为SGD方式迭代训练，可能会导致训练不充分的情况。Ada系列的迭代方式learning rate可变，会一定程度上缓解这个现象</span></div><div class="line">        <span class="keyword">if</span> self.optimizer_type == <span class="string">'AdamOptimizer'</span>:</div><div class="line">            self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate, beta1=<span class="number">0.9</span>, beta2=<span class="number">0.999</span>, epsilon=<span class="number">1e-8</span>).minimize(self.loss)</div><div class="line">        <span class="keyword">elif</span> self.optimizer_type == <span class="string">'AdagradOptimizer'</span>:</div><div class="line">            self.optimizer = tf.train.AdagradOptimizer(learning_rate=self.learning_rate, initial_accumulator_value=<span class="number">1e-8</span>).minimize(self.loss)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate).minimize(self.loss)</div><div class="line"></div><div class="line">        <span class="comment"># init</span></div><div class="line">        self.saver = tf.train.Saver()</div><div class="line">        init = tf.global_variables_initializer()</div><div class="line">        self.sess = self._init_session()</div><div class="line">        self.sess.run(init)</div></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://nirvanada.github.io/2017/07/14/FM and FFM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Nirvanada">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Andante">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/07/14/FM and FFM/" itemprop="url">FM and FFM</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-07-14T17:11:02+08:00">
                2017-07-14
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>在CTR预估中，LR模型有着举足轻重的地位，而近几年FM（Factorization Machine）和FFM（Field Factorization Machine）开始在各大比赛和互联网公司广泛运用并取得了不俗的成绩和效果。实际上，FM算是LR的扩展，FFM算是FM的扩展，或者说FMM的一种特例是FM，而FM的一种特例就是LR，下面主要围绕着这两个算法的原理部分展开介绍。</p>
<h4 id="FM"><a href="#FM" class="headerlink" title="FM"></a>FM</h4><p>相对于LR模型来说，FM就是考虑了每个特征之间的交互关系，而有些时候在实际场景中这些特征交互往往起到了关键性作用，这也是为什么现在深度学习火的原因之一。打个比方，在广告推荐场景下，一个男性用户可能会有2%的概率点击一条足球属性的广告，而一个年龄在20~40岁之间的男性用户可能会有3%的概率点击，更进一步，满足上面条件的用户在一个给定的上下文条件下，比如世界杯期间、傍晚、甚至在搜索query高度相关的条件下就会有10%的点击概率，这都说明了特征之间的交互作用会对最后的预测结果产生很重要的影响。回到LR模型，它只对每个特征单独赋予一个权重，即</p>
<script type="math/tex; mode=display">
\phi(x) = \sigma(w_0 + \sum_i w_i x_i)</script><p>如果把$\sigma$拿掉的话，只看内部的线性部分，FM模型所能带来的特征交互形式即为</p>
<script type="math/tex; mode=display">
\begin{aligned}
y(x) =& w_0 + \sum_i w_i x_i +\sum_i \sum_{j = i + 1} w_{ij} x_i x_j\\
= &w_0 + \sum_i w_i x_i +\sum_i \sum_{j = i + 1} <v_i, v_j> x_i x_j
\end{aligned}</script><p>实际上，仅仅依靠$w_{ij}$就可以完成对特征的交互，但这里对$W$矩阵的构造并没有采用直接学习的方式，而是借鉴了矩阵分解的形式，构成形式为由$V \cdot V^T$。这样做的好处有两点，第一点是学习的参数量会显著减少。由于在CTR预估的任务中，会存在非常sparse的特征，百万千万维都有可能，如果直接学习$w_{ij}$的参数量是$N^2$，而如果$V\in R^{N*k}$，学习的参数量就是$kN$，而一般来说$k$的选取并不会太大，一方面是为了减少计算量，另一方面则是为了考虑在高维稀疏特征条件下的泛化性能；第二点是如果直接学习$w_{ij}$，会存在“没什么可学的”的窘境，因为如果是采用SGD的优化方式，$w_{ij}$的更新会依赖那些交互项都不为0的特征，而这样的交互极少，参数可能会学习不充分，但以这种矩阵分解的间接构建$w_{ij}$更新参数就不会出现这种问题。</p>
<p>由于上式前半部分和LR一致，所以我们将重点放在后半部分特征交互项上，经过推导，最后一项可以化简为</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\sum_i \sum_{j = i+1} <v_i, v_j> x_i x_j \\
=&\frac{1}{2} \sum_i \sum_{j = 1} <v_i, v_j> x_i x_j - 1/2 \sum_i <v_i, v_i> x_i x_i \\
=&\frac{1}{2}(\sum_i \sum_j \sum_{f=1}^{k} v_{i,f} v_{j, f} x_{i} x_{j} - \sum_i \sum_{f=1}^{k} v_{i, f} v_{i, f} x_i x_i) \\
=&\frac{1}{2}\sum_{f = 1}^{k}((\sum_i v_{i,f} x_i)(\sum_j v_{j,f} x_j)) - \sum_i v_{i,f}^2 x_i^2))  \\
=&\frac{1}{2}\sum_{f = 1}^{k}(( \sum_{i=1}^{N} v_{i,f} x_i)^2 -  \sum_{i=1}^{N} v_{i,f}^2 x_i^2))
\end{aligned}</script><p>这里虽然看上去很绕，但只要自己动手写一写就一目了然了。最最重要的是，上面的式子的计算复杂度是线性的$O(kN)$，相比较于直接计算$W$矩阵的复杂度$O(kN^2)$，明显要高效许多。在优化参数时，采用传统的SGD方法，式中各项参数的梯度可求</p>
<script type="math/tex; mode=display">
\frac {\partial y(x)}{\partial \theta} = \begin{cases} 1 & \text {if $\theta$ is $w_0$}\\
x_i & \text {if $\theta$ is $w_i$}\\
x_i \sum_j v_{j,f}x_j - v_{i,f}x_i^2 & \text {if $\theta$ is $v_{i,f}$} \end{cases}</script><p>最后，假设对于一个具体的分类问题，最终带正则的优化目标就变成了</p>
<script type="math/tex; mode=display">
\begin{aligned}
\theta^* &= argmin_\theta \sum (loss(y(x_i), \hat y_i) + \sum_\theta \lambda \theta^2) \\ 
&= argmin_\theta \sum -\hat y_i log\sigma(y_i) - (1 - \hat y_i) log(1 - \sigma(y_i)) + \sum_\theta \lambda \theta^2)
\end{aligned}</script><h4 id="FFM"><a href="#FFM" class="headerlink" title="FFM"></a>FFM</h4><p>FFM是在FM的基础上引入了Field的概念。从上面的公式中可以看到，FM在处理$x_i$和$x_j$的特征交互时，所使用的隐向量$V$是$v_i$和$v_j$，而在处理$x_i$和$x_k$的特征交互时，所使用的隐向量$V$是$v_i$和$v_k$，这样的局限就是$x_i$和其他所有特征交互时用的都是一个隐向量$v_i$，而FFM对这个问题的处理方法就是引入field的概念，或者说把特征分组，比如年龄特征是一个field，性别特征是一个field，职业特征又是一个field，在FM模型中年龄与性别、职业特征交互项的隐因子$v_{age}$是一样的，但在FFM中性别职业的交互隐因子为$v_{age, sex}$，而性别职业的交互隐因子为$v_{age, prof}$。更一般地，之前的交互项系数的表达为&lt;$v_i, v_j$&gt;，而现在又加入了field这一因素，表达为&lt;$v_{i, f_j}, v_{j, f_i}$&gt;，也就是说在模型的表达式上只需要修改这一项就可以，其余都不变。在实际应用中，我们一般指定分类特征中的每一类特征为一个field，而连续特征每个都为一个field。</p>
<p>我们可以看到，加入field概念之后对特征的交互考虑得更加周全，不同类型的特征交互体现出了多样性，但是由于有这样一个因素的存在，使得我们模型的参数量也随之提高。FM的参数量为$kN$，又由于公式可以化简，因此计算复杂度由初始的$O(kN^2)$变为$O(kN)$，然而FM的参数量则变为了$kfN$，其中$f$为field个数，公式无法化简，计算复杂度为$O(kN^2)$。虽然说由于有field带来的多样性，在$k$的选取上FFM要比FM小，但总体上看FFM的学习速率还是要比FM慢一截。</p>
<p>从FM和FFM在公开数据集上的表现来看，FM和FFM各有千秋，但相比于原始未进行特征交互的LM和直接计算特征交互矩阵$W$的Poly2方法还是有一定程度的提升。</p>
<img src="/images/FM/fm_ffm.png">
<h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><p><a href="http://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf" target="_blank" rel="external">Factorization Machines</a><br><a href="http://www.csie.ntu.edu.tw/~cjlin/papers/ffm.pdf" target="_blank" rel="external">Field-aware Factorization Machines for CTR Prediction</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://nirvanada.github.io/2017/07/14/Tensorflow/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Nirvanada">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Andante">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/07/14/Tensorflow/" itemprop="url">Tensorflow简介</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-07-14T17:08:42+08:00">
                2017-07-14
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>接触Tensorflow也有好一段时间了，但总是对这个项目庞大的代码量和复杂的设计结构望而生畏，一直觉得它可以和某些原生的编程语言相提并论，所以大多时候也都是照着一些现成的模板去套用然后再不断调试，对内部的一些基本概念其实并没有太多了解。最近想着好好把官方给的get started和tutorial过一遍，对以后使用应该也更有帮助。</p>
<h3 id="Tensors"><a href="#Tensors" class="headerlink" title="Tensors"></a>Tensors</h3><p>张量就是高维数组的另一种表达形式，数组是几维的，张量的rank就是几。比如[1,2,3]是一维向量，rank=1；[[1,2,3], [2,3,4]]是二维矩阵，rank=2；[[[1,2,3], [2,3,4]]]是三维张量，rank=3，就这么简单。</p>
<h3 id="Computational-Graph"><a href="#Computational-Graph" class="headerlink" title="Computational Graph"></a>Computational Graph</h3><p>有了tensor，要形成flow，就必须借助图，张量和图起来就基本构成了Tensorflow的基本框架。图中的每个node可以是tensor，也可以是加减乘除之类的算子。其实回想一下，神经网络本质上就是这种神经元之间互相有向连接的结构，所以在我们定义好每个node后把这些node连接成图，理论上就可以搭建任何模式的结构。但是如果要查看某个图最终node的输出结果，还必须要通过一个session的运行环境来run。下面举个简单的小例子就会一目了然：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">node1 = tf.constant(<span class="number">3.0</span>, tf.float32)</div><div class="line">node2 = tf.constant(<span class="number">4.0</span>) <span class="comment"># also tf.float32 implicitly</span></div><div class="line">print(node1, node2)</div><div class="line"><span class="comment">#Output: Tensor("Const:0", shape=(), dtype=float32) Tensor("Const_1:0", shape=(), dtype=float32)</span></div><div class="line">sess = tf.Session()</div><div class="line">print(sess.run([node1, node2]))</div><div class="line"><span class="comment">#Output:[3.0, 4.0]</span></div><div class="line">node3 = tf.add(node1, node2)</div><div class="line">print(<span class="string">"node3: "</span>, node3)</div><div class="line">print(<span class="string">"sess.run(node3): "</span>,sess.run(node3))</div><div class="line"><span class="comment">#Output:7.0</span></div></pre></td></tr></table></figure>
<p>上面的code只是把node1和node2定义为tf.constant()的常量形式，更普遍的我们将输入定义为类似于函数形式的变量，这时就需要利用到tf.placeholder()方法，可以理解为占位符，这类似于python中lambda匿名函数的形式，占位符就是传入匿名函数的输入参数，再看例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">a = tf.placeholder(tf.float32)</div><div class="line">b = tf.placeholder(tf.float32)</div><div class="line">adder_node = a + b  <span class="comment"># + provides a shortcut for tf.add(a, b)</span></div><div class="line">print(sess.run(adder_node, feed_dict = &#123;a: <span class="number">3</span>, b:<span class="number">4.5</span>&#125;))</div><div class="line"><span class="comment">#Output:7.5</span></div><div class="line">print(sess.run(adder_node, feed_dict = &#123;a: [<span class="number">1</span>,<span class="number">3</span>], b: [<span class="number">2</span>, <span class="number">4</span>]&#125;))</div><div class="line"><span class="comment">#Output:[ 3.  7.]</span></div><div class="line">add_and_triple = adder_node * <span class="number">3.</span></div><div class="line">print(sess.run(add_and_triple, feed_dict = &#123;a: <span class="number">3</span>, b:<span class="number">4.5</span>&#125;))</div><div class="line"><span class="comment">#Output:22.5</span></div></pre></td></tr></table></figure>
<p>上面提到的tensor都是不可变，但是在大多machine learning的应用中需要通过不断的迭代来调整参数，比如网络中的weight和bias，这时需要利用tf.Variable()来给定初始值并定义这些需要train的变量。有了trainable的参数和输入数据，最后一步就是定义loss，我们以linear regression为例，定义square loss 为$loss  = (Wx + b - \hat y)^2$，大多数常见的loss在Tensorflow中都已经被封装好，直接调用即可。接下来的代码相对来说复杂一点，但逻辑在Tensorflow的框架下十分清晰：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">W = tf.Variable([<span class="number">.3</span>], tf.float32) <span class="comment">#Trainable param</span></div><div class="line">b = tf.Variable([<span class="number">-.3</span>], tf.float32) <span class="comment">#Trainable param</span></div><div class="line">x = tf.placeholder(tf.float32) <span class="comment">#Input placeholder</span></div><div class="line">y = tf.placeholder(tf.float32) <span class="comment">#Input placeholder</span></div><div class="line"></div><div class="line">linear_model = W * x + b <span class="comment"># Linear Regression</span></div><div class="line">squared_deltas = tf.square(linear_model - y) <span class="comment">#Define square loss</span></div><div class="line">loss = tf.reduce_sum(squared_deltas) <span class="comment">#Add loss on all samples</span></div><div class="line"></div><div class="line">init = tf.global_variables_initializer() <span class="comment"># You must explicitly initilize the variables</span></div><div class="line">sess.run(init) <span class="comment"># Until we call sess.run, the variables are uninitialized</span></div><div class="line">print(sess.run(loss, &#123;x:[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>], y:[<span class="number">0</span>,<span class="number">-1</span>,<span class="number">-2</span>,<span class="number">-3</span>]&#125;))</div><div class="line"><span class="comment">#Output:23.66</span></div></pre></td></tr></table></figure>
<h3 id="Train"><a href="#Train" class="headerlink" title="Train"></a>Train</h3><p>到现在为止，我们还只是简单的手动构建了图，做了几个简单运算，还没有涉及到神经网路的梯度反向传播、参数更新过程。Tensorflow的强大之处在于，在用户构建完成网络结构（图）后，就可以通过简单的函数调用来自动计算梯度。这里我们以最常见的梯度下降为例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">optimizer = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>) <span class="comment"># define gradient descent method</span></div><div class="line">train = optimizer.minimize(loss) <span class="comment"># minize loss using gradient descent</span></div><div class="line"></div><div class="line">sess.run(init) <span class="comment"># reset values to incorrect defaults.</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>): <span class="comment"># run gradient descent for 1000 steps</span></div><div class="line">  sess.run(train, &#123;x:[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>], y:[<span class="number">0</span>,<span class="number">-1</span>,<span class="number">-2</span>,<span class="number">-3</span>]&#125;)</div><div class="line"></div><div class="line">print(sess.run([W, b]))</div><div class="line"><span class="comment">#Output:[array([-0.9999969], dtype=float32), array([ 0.99999082],dtype=float32)]</span></div></pre></td></tr></table></figure>
<h3 id="Reading-Data"><a href="#Reading-Data" class="headerlink" title="Reading Data"></a>Reading Data</h3><p>Tensorflow读取数据的方式大体上说有三种，第一种是和上述例子中一样的方式，先在图中定义好placeholder，再通过feed_dict方式在run session时传入placeholder对应的数据；第二种适用于小数据量，直接将数据定义为constant或variable，个人感觉本质上和第一种没有太大区别；第三种在数据量比较大时候十分常用，就是直接从文件中按照队列的方式进行读取，以目前一般企业级机器学习的数据量来看，前两种方式会受限于内存和图的大小，因此简单介绍下这种比较通用的方式。</p>
<p>首先将要读取的小文件名形成一个列表，然后通过<a href="https://www.tensorflow.org/api_docs/python/tf/train/string_input_producer" target="_blank" rel="external">tf.train.string_input_producer</a>函数建立一个关于这些文件名的队列queue。接下来会有若干个reader实体去从这个queue中读取文件名，然后对于不同形式的文件（比如csv、二进制等）再通过不同的decoder对当前文件名下的内容进行解析，这样就可以不断的从文件列表中的每个文件读数，下面是一个涉及到batch read的例子。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_my_file_format</span><span class="params">(filename_queue)</span>:</span></div><div class="line">  reader = tf.SomeReader() <span class="comment"># define reader</span></div><div class="line">  key, record_string = reader.read(filename_queue) <span class="comment"># read file from file_queue using reader, each execution of read reads a single line from the file</span></div><div class="line">  example, label = tf.some_decoder(record_string) <span class="comment"># decode file</span></div><div class="line">  processed_example = some_processing(example) <span class="comment"># do some preprocessing</span></div><div class="line">  <span class="keyword">return</span> processed_example, label</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">input_pipeline</span><span class="params">(filenames, batch_size, num_epochs=None)</span>:</span></div><div class="line">  filename_queue = tf.train.string_input_producer(</div><div class="line">      filenames, num_epochs=num_epochs, shuffle=<span class="keyword">True</span>)</div><div class="line">  example, label = read_my_file_format(filename_queue)</div><div class="line">  min_after_dequeue = <span class="number">10000</span> <span class="comment"># define how big a buffer we will randomly sample from</span></div><div class="line">  capacity = min_after_dequeue + <span class="number">3</span> * batch_size <span class="comment"># larger than min_after_dequeue</span></div><div class="line">  example_batch, label_batch = tf.train.shuffle_batch(</div><div class="line">      [example, label], batch_size=batch_size, capacity=capacity,</div><div class="line">      min_after_dequeue=min_after_dequeue) <span class="comment"># make batch sample</span></div><div class="line">  <span class="keyword">return</span> example_batch, label_batch</div></pre></td></tr></table></figure>
<p>一般来说牵扯到了queue，我们就必须要在tf.Session()中在任务run之前利用tf.train.start_queue_runners和tf.train.Coordinator来填充队列和管理线程，不然read进程就会因为从queue中等待读取文件名而发生阻塞。</p>
<p>上面只是对tf做了一个非常非常浅显的介绍，但烂熟于心之后基本上也可以无障碍阅读github上大多数的tensorflow tutorial，而在实际的程序中依然有大量的坑和N多封装好的方法等待被发掘。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://nirvanada.github.io/2017/05/05/Session_rec/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Nirvanada">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Andante">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/05/05/Session_rec/" itemprop="url">利用RNN做session-based推荐</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-05-05T15:05:42+08:00">
                2017-05-05
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Paper/" itemprop="url" rel="index">
                    <span itemprop="name">Paper</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>最近在做站内新用户推荐相关的项目，由于新用户历史行为相对较少，传统的neighborhood method和factor method models上对于user profile的利用少之又少，而如果当做一个传统的二分类问题，即预测购买/不购买，同样也存在相似的问题——特征不足，model能力上限有瓶颈，因此想着挖掘一下会话的time series pattern，从而变成一个基于session的预测问题，主要参考了<a href="https://arxiv.org/abs/1511.06939" target="_blank" rel="external">ICLR 2016的一篇文章</a>，个人感觉这篇文章可以算是利用RNN做基于session推荐比较早期的方法，而且作者也开源了<a href="https://github.com/hidasib/GRU4Rec" target="_blank" rel="external">基于theano的实现</a>，后续一些improved版本，包括网易在考拉上的session推荐实践也都借鉴了这篇文章的一些思想，接下来对这篇文章进行一个简单的梳理。</p>
<p>文章中解决的问题是：给定一个session中的item点击流（click streaming events），来预测这个点击流后的下一个点击会是什么item，并最终由目标item完成推荐。需要注意的是，作者虽然用到了recsys 2015的数据来做model evaluation，但实际上并没有用到购买数据，因此这篇文章所解决的问题只是通过session的item序列模式挖掘可能感兴趣的item，并不是最终会发生购买行为的item。按照这个模型结构来看，输入数据就是某个session的当前状态，即前N个item click sequences所携带的序列信息，输出就是当前session第N+1个item。所以在一个点击流中的一个特定点击事件的网络结构可以表达为如下图所示，输入为已点击item的one-hot编码，经过一层embedding层来进行降维，再通过多层GRU（Gated Recurrent Unit）单元，最后通过一个前馈映射层来输出下一个item的likelihood。</p>
<p>简要描述一下GRU的原理（这里作者用GRU的原因是在GRU、LSTM和传统RNN单元中发现GRU效果最好）。传统RNN的hidden state数学表达为</p>
<script type="math/tex; mode=display">
h_t = g(Wx_t + Uh_{t-1})</script><p>也就是某一个时间点上的hidden state与当前时间点的输入和上一个时间点的hidden state有关，并且是二者线性组合的形式，但当时间序列过长时，会面临严重的梯度消失问题，而GRU和LSTM由于网络结构的变化可以有效的解决传统RNN中的时间维度梯度消失问题。其中GRU引入了update gate $z_t$和reset gate $r_t$的概念，两个gate控制当前时刻输入和上一时刻的记忆信息哪部分需要保留，哪部分需要丢弃，表达形式一致，分别表示为</p>
<script type="math/tex; mode=display">
z_t = \sigma(W_z x_t + U_z h_{t-1}) \\
r_t = \sigma(W_r x_t + U_r h_{t-1})</script><p>而输出的hidden state为</p>
<script type="math/tex; mode=display">
\hat h_t = tanh(W x_t + U(r_t \odot h_{t-1}))\\
h_t = (1 - z_t)h_{t-1} + z_t \hat h_t</script><p>在构造数据集时，文章采用了session后补齐再划分mini-batch的方法（如下图）。具体来讲，就是当一个session结束时，用一个新的session接到当前session的末尾，当这个事件发生时，hidden state就被重新置零。</p>
<img src="/images/Session_Rec/fig1.png">
<p>虽然可以把上述问题作为一个多分类问题，但最后文章在实验中发现用cross-entropy做loss在100个左右hidden units时效果不错，但一旦units数量增大效果就不如ranking loss的形式，因此采用了pairwise ranking loss作为损失函数，并分别列举了BPR（Bayesian Personalized Ranking）和TOP1，其中某一个特定session中的一个点击预测loss为</p>
<script type="math/tex; mode=display">
L_S^{BPR} = -\frac{1}{N_S} \sum_{j=1}^{N_S} log(\sigma(\hat{r}_{S,i} - \hat{r}_{S,j})) \\
L_S^{TOP1} = -\frac{1}{N_S} \sum_{j=1}^{N_S} log(\sigma(\hat{r}_{S,i} - \hat{r}_{S,j})) + \sigma(\hat{r}_{S,j}^2)</script><p>，这里$\hat {r}_{S,j}$为第$j$个negative sample item的score，$\hat {r}_{S,i}$为postive sample item的score，值得注意的是这里对负样本做了一定程度的负采样，以消除一些用户可能感兴趣但并没有展示商品的影响。</p>
<p>实验在Recsys 2015数据集和另一个视频点击数据集上进行，以MRR@20和Recall@20作为指标评估，分别比较了不同mini-batch大小、dropout大小、learning-rate大小和momentum和loss形式下的效果，并与一些传统流行的推荐算法baseline进行对比，均有提升，细节不详谈，有兴趣的可以自己去看一下。</p>
<img src="/images/Session_Rec/fig2.png">
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://nirvanada.github.io/2017/04/10/DSSM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Nirvanada">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Andante">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/04/10/DSSM/" itemprop="url">Multi-Rate Deep Learning for Temporal Recommendation解读</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-04-10T20:47:53+08:00">
                2017-04-10
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Paper/" itemprop="url" rel="index">
                    <span itemprop="name">Paper</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>在推荐系统里有一个比较重要的问题：如何能把一个user的长期兴趣和短期兴趣综合起来考虑进行内容推荐？一个人的兴趣总会随着时间发生变化，特定日期、事件都会使长期兴趣发生波动从而促生短期兴趣。一个比较直观的例子：如果在一个世界杯期间的新闻推荐场景下，推足球新闻很有可能比推荐长期兴趣的内容更使人满意。而目前的很多推荐方法都没有考虑与时间有关的短期信息，因此这篇文章主要针对将长短兴趣结合来提高推荐效果。</p>
<p>模型的基础利用deep semantic structured model (DSSM)。简言之，DSSM可以视为把从两个或多个角度所构建的神经网络模型整合到一个角度进行学习。假设一个两个角度的DSSM，其中一个网络表示query，另一个网络表示document，两个网络的输入可以是由各自角度所代表的特征（query-based features和document-based features），输出为比输入维度低的embedding vectors，而两个网络综合学习的目标是最大化两个输出vectors的cosine相似度。在具体的训练过程中，在每个mini-batch中随机负采样后再与正样本组合，然后再最小化正样本上的cosine loss（使正样本中的两个网络输出vectors最匹配）。在推荐场景里，可以认为其中一个网络是user的query history，即用户特征，另一个网络为系统中item的隠反馈，比如新闻的点击或是app的下载。</p>
<p>上面所说的是DSSM的基本概念，但是这里有一个问题，就是user相关的features都是不随时间发生改变的，于是作者引入了Temporal的概念，也就是将user的feautures进行拆分和细化，分为static和temporal，分别代表长期兴趣和短期兴趣。 下图可以很好的表示整个推荐系统的架构，其中item features和user static features全都利用全连接的神经网络构造embeddings，而user temporal features则利用LSTM构造embedding（文中的实验表明GRU的效果并不理想），然后将static和temporal的vectors通过函数$f$做成组合向量（$f$可以是multiplication或concatenation），再与item的向量做cosine similarity生成一个user-item相似度，预测的时候取与user相似度最大的topK item进行推荐。（这里有个问题，如果考虑item的temporal features会是什么样？）</p>
<img src="/images/DSSM/1.png">
<p>给出的优化目标为似然形式，即使给定user和时刻t时item概率最大,</p>
<script type="math/tex; mode=display">
min_{W_{user}, W_{item}} -log \prod_{user, item^+, t_i} p(item^+|user, t_i) \\
p(item^+|user, t_i) = \frac{exp(cos(E(user, t_i), E_{item^+}))}{\sum_{item} exp(cos(E(user, t_i), E_{item}))}</script><p>这里概率依然还是由softmax得到，只不过用user的temporal+static向量和item向量之间的相似度来表现。</p>
<p>不过仔细想一下，模型在细节上还是有些问题，那就是如何选择时间窗口t。选大了，兴趣不够“短期”，选小了，模型参数太多，训练不来，因此作者又引入了multi-rate的概念，也就是选择几个窗口，分别代表短期兴趣和中短期兴趣，然后再训练不同的LSTM，称为“Fast-RNNs”和“Slow-RNNs”，然后将几个LSTMs用全连接层串到一起就OK了。不过这样RNN所带来的训练参数还是太多，文中采用的方法是在训练之前先用上文提到基本的DSSM做pre-train。</p>
<p>理论部分基本就是这些，就是在DSSM基础上引入了Temporal的概念——解决用户短期兴趣的问题，再引入了multi-rate的概念——对短期兴趣的粒度和模型训练效率做trade-off，因此称为MR-TDSSM。说实话，这篇文章并没有DSSM那篇惊艳，只能算是前者的进一步扩展，但实验结果确实很不赖，在新闻数据上多个指标上能够碾压传统推荐算法和DSSM。值得一提的是，实现工具是keras（一直以为微软的工程师都会用自己的轮子），一些传统方法的baseline用的都是LibRec，可以看到这个开源工具还是挺流行的。</p>
<img src="/images/DSSM/2.png">
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://nirvanada.github.io/2017/03/22/w_n_d/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Nirvanada">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Andante">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/03/22/w_n_d/" itemprop="url">Wide and Deep Learning for Recommender Systems解读</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-03-22T22:59:53+08:00">
                2017-03-22
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Paper/" itemprop="url" rel="index">
                    <span itemprop="name">Paper</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Google在去年6月份在arxiv上放出了”Wide &amp; Deep Learning for Recommender Systems”这篇文章，应用场景是Google Play上App安装预测，线上效果提升显著（相比较于Wide only和Deep only的model）。与此同时，Google也开源了这一模型框架并将其集成在Tensorflow的高级封装Tflearn中，今年开年的谷歌开发者大会上也专门有一个section是讲wide and deep model的，作者的意图也很明显，一方面推广tensorflow，另一方面是显示模型的强大。</p>
<p>通读文章后，发现其实模型的基本原理很简单，就是wide model + deep model。分别来讲，wide model就是一个LR，在特征方面除了原始特征外还有分类特征稀疏表达后的交叉特征，例如将分类特征做完one-hot后再进行cross product。个人理解，这里一方面是利用类似于FM模型原理来增强分类特征的特征交互（co-occurrence），另一方面是利用LR对高维稀疏特征的学习能力，而作者把wide model所具备的能力称为“memorization”；而deep model则是一个DNN，特征上除了原始特征还增加了分类特征的embedding，这个embedding在模型中属于独立的一层，embedding后的向量也是通过不断迭代学习出来的。将高维稀疏分类特征映射到低维embedding特征这种方式有助于模型进行“generalization”。Memerization和Generalization这两个概念中文还真没找到特别合适的诠释，如果非要翻译一下，我觉得应该是推理和演绎，一个是通过特征交互关系来训练浅层模型，另一个则是通过特征在映射空间中的信息训练深层模型。</p>
<p>模型结构采用joint的方式而非传统的ensemble方式。如果是ensemble方式，那么这两个模型就针对label进行单独训练，然后再加权到一起；而joint方式则是将这两个模型的输出加起来，然后再针对label进行联合训练。这样的好处是在train model的时候可以同时最优化两个model的参数，而且两个model可以起到互相补充的作用。下面的公式也很好的解释了wide and deep model的结构原理，即两个model的output在通过sigmoid函数之前把结果相加，然后再经过sigmoid实现分类。这里$x$指原始特征，$\phi(x)$分别表示wide模型的cross product feature和deep模型的embedding feature，而$w_{deep}$则泛指DNN各层weights和bias集合表示。</p>
<script type="math/tex; mode=display">
 P(Y=1|x) = \sigma(w_{wide}[x, \phi_1(x)] + w_{deep}[x, \phi_2(x)])</script><img src="/images/w_n_d/1.png">
<p>最终模型在离线评测上效果并不明显，但在在线评测上提升还算显著。前几天在电梯里听到广告部同事说他们也在搞这个模型，离线效果提升了10%+，但在线serving技术上目前比较头疼，但也不知道具体是什么指标提升了10%+。但在我们团队内部客户拉新预测问题的离线应用上，同样的数据效果只和xgboost持平。另外，官方提供的tutorial原生代码并不能很好的应用于大数据量，于是进行了改写，读取数据方式变成了队列读取，也是参考了stackoverflow上的一些反馈。核心就是wide_and_deep函数，cross product的实现方式是直接在预处理数据时对分类特征进行字符串拼接，然后再做one-hot，相对contrib中crossed_column的实现方式略显复杂，但以个人能力也只能先这样做，日后再去探索。另外，由于目前团队业务只涉及线下模型，因此对于模型的线上更新并没有太多关注，但大的技术框架来看应该也是用tf serving的方式实现。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">import</span> tensorflow.contrib.learn <span class="keyword">as</span> tf_learn</div><div class="line"><span class="keyword">import</span> tensorflow.contrib.layers <span class="keyword">as</span> tf_layers</div><div class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</div><div class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</div><div class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</div><div class="line"><span class="keyword">import</span> os</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</div><div class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">from</span> feat <span class="keyword">import</span> CONTINUOUS_COLUMNS</div><div class="line"><span class="keyword">from</span> feat <span class="keyword">import</span> CATEGORICAL_COLUMNS</div><div class="line"><span class="keyword">from</span> feat <span class="keyword">import</span> COLUMNS</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_columns</span><span class="params">(x)</span>:</span></div><div class="line">    <span class="keyword">return</span> <span class="string">':'</span>.join(x)</div><div class="line"></div><div class="line"><span class="comment"># Define the column names for the data sets.</span></div><div class="line">LABEL_COLUMN = <span class="string">'target'</span></div><div class="line"><span class="comment">#second order</span></div><div class="line">CROSSED_COLUMNS = []</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(CATEGORICAL_COLUMNS) <span class="number">-1</span>):</div><div class="line">  <span class="keyword">for</span> j <span class="keyword">in</span> range(i+<span class="number">1</span>, len(CATEGORICAL_COLUMNS)):</div><div class="line">     CROSSED_COLUMNS.append([CATEGORICAL_COLUMNS[i], CATEGORICAL_COLUMNS[j]])</div><div class="line"></div><div class="line">CATEGORICAL_COLUMNS_DNN = CATEGORICAL_COLUMNS[:]</div><div class="line">CATEGORICAL_COLUMNS += map(add_columns, CROSSED_COLUMNS)</div><div class="line">CATEGORICAL_ID_COLUMNS = [col + <span class="string">'_ids'</span> <span class="keyword">for</span> col <span class="keyword">in</span> CATEGORICAL_COLUMNS]</div><div class="line"></div><div class="line"></div><div class="line">HIDDEN_UNITS = [<span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>]</div><div class="line">CATEGORICAL_EMBED_SIZE = <span class="number">10</span></div><div class="line"></div><div class="line">LABEL_ENCODERS = &#123;&#125;</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">pandas_input_fn</span><span class="params">(X, y=None, batch_size=<span class="number">1024</span>, num_epochs=None)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">input_fn</span><span class="params">()</span>:</span></div><div class="line">        <span class="keyword">if</span> y <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">            X[<span class="string">'target'</span>] = y</div><div class="line">        queue = tf_learn.dataframe.queues.feeding_functions.enqueue_data(</div><div class="line">            X, <span class="number">1000</span>, shuffle=num_epochs <span class="keyword">is</span> <span class="keyword">None</span>, num_epochs=num_epochs)</div><div class="line">        <span class="keyword">if</span> num_epochs <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">            features = queue.dequeue_many(batch_size)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            features = queue.dequeue_up_to(batch_size)</div><div class="line"></div><div class="line">        features = dict(zip([<span class="string">'index'</span>] + list(X.columns), features))</div><div class="line"></div><div class="line">        <span class="keyword">if</span> y <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">            target = features.pop(<span class="string">'target'</span>)</div><div class="line">            <span class="keyword">return</span> features, target</div><div class="line">        <span class="keyword">return</span> features</div><div class="line"></div><div class="line">    <span class="keyword">return</span> input_fn</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">encode_categorical_cross</span><span class="params">(df)</span>:</span></div><div class="line">    <span class="keyword">global</span> LABEL_ENCODERS</div><div class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> CATEGORICAL_COLUMNS:</div><div class="line">        <span class="keyword">if</span> <span class="string">":"</span> <span class="keyword">in</span> col:</div><div class="line">            df[col] = df[col.split(<span class="string">":"</span>)[<span class="number">0</span>]].fillna(<span class="number">-1</span>).astype(str) + <span class="string">":"</span> + df[col.split(<span class="string">":"</span>)[<span class="number">1</span>]].fillna(<span class="number">-1</span>).astype(str)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            df[col] = df[col].fillna(<span class="number">-1</span>).astype(str)</div><div class="line">        encoder = LabelEncoder().fit(df[col])</div><div class="line">        df[col + <span class="string">'_ids'</span>] = encoder.transform(df[col])</div><div class="line">        LABEL_ENCODERS[col] = encoder</div><div class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> CATEGORICAL_COLUMNS:</div><div class="line">        df.pop(col)</div><div class="line">    <span class="keyword">return</span> df, LABEL_ENCODERS</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_input_df</span><span class="params">(df)</span>:</span></div><div class="line">    df, label_encoders = encode_categorical_cross(df)</div><div class="line">	<span class="keyword">for</span> col <span class="keyword">in</span> CATEGORICAL_COLUMNS:</div><div class="line">    	y = df.pop(LABEL_COLUMN)</div><div class="line">    	X = df[CATEGORICAL_ID_COLUMNS + CONTINUOUS_COLUMNS].fillna(<span class="number">0</span>)</div><div class="line">    <span class="keyword">return</span> X, y</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">wide_and_deep</span><span class="params">(features, target, hidden_units=HIDDEN_UNITS)</span>:</span></div><div class="line">    <span class="keyword">global</span> LABEL_ENCODERS</div><div class="line">    target = tf.one_hot(target, <span class="number">2</span>, <span class="number">1.0</span>, <span class="number">0.0</span>)</div><div class="line"></div><div class="line">    <span class="comment"># DNN</span></div><div class="line">    final_features_nn = [tf.expand_dims(tf.cast(features[col], tf.float32), <span class="number">1</span>) <span class="keyword">for</span></div><div class="line">                      col <span class="keyword">in</span> CONTINUOUS_COLUMNS]</div><div class="line"></div><div class="line">    <span class="comment"># Embed categorical variables into distributed representation.</span></div><div class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> CATEGORICAL_COLUMNS_DNN:</div><div class="line">        feature_tmp = tf_learn.ops.categorical_variable(</div><div class="line">            features[col + <span class="string">'_ids'</span>],</div><div class="line">            len(LABEL_ENCODERS[col].classes_),</div><div class="line">            embedding_size=CATEGORICAL_EMBED_SIZE,</div><div class="line">            name=col)</div><div class="line">        final_features_nn.append(feature_tmp)</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">    <span class="comment"># Concatenate all features into one vector.</span></div><div class="line">    features_nn = tf.concat(<span class="number">1</span>, final_features_nn)</div><div class="line"></div><div class="line">    logits_nn = tf_layers.stack(features_nn,</div><div class="line">                             tf_layers.fully_connected,</div><div class="line">                             stack_args=hidden_units,</div><div class="line">                             activation_fn=tf.nn.relu)</div><div class="line"></div><div class="line">    <span class="comment"># LR</span></div><div class="line">    final_features_lr = [tf.expand_dims(tf.cast(features[col], tf.float32), <span class="number">1</span>) <span class="keyword">for</span></div><div class="line">                      col <span class="keyword">in</span> CONTINUOUS_COLUMNS]</div><div class="line"></div><div class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> CATEGORICAL_COLUMNS:</div><div class="line">        final_features_lr.append(tf.one_hot(features[col + <span class="string">'_ids'</span>],</div><div class="line">            len(LABEL_ENCODERS[col].classes_),</div><div class="line">            on_value = <span class="number">1.0</span>,</div><div class="line">            off_value = <span class="number">0.0</span>))</div><div class="line"></div><div class="line"></div><div class="line">    logits_lr = tf_layers.stack(tf.concat(<span class="number">1</span>, final_features_lr),</div><div class="line">                             tf_layers.fully_connected,</div><div class="line">                             stack_args=[<span class="number">1</span>],</div><div class="line">                             activation_fn=<span class="keyword">None</span>)</div><div class="line"></div><div class="line">    <span class="comment"># add logits</span></div><div class="line">    logits = logits_lr + logits_nn</div><div class="line"></div><div class="line">    prediction, loss = tf_learn.models.logistic_regression(logits, target)</div><div class="line">    train_op = tf_layers.optimize_loss(loss,</div><div class="line">                                       tf.contrib.framework.get_global_step(),</div><div class="line">                                       optimizer=<span class="string">'Adam'</span>,</div><div class="line">                                       learning_rate=<span class="number">0.001</span>)</div><div class="line">    <span class="keyword">return</span> prediction[:,<span class="number">1</span>], loss, train_op</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(X, y, steps=<span class="number">100</span>)</span>:</span></div><div class="line">    print(<span class="string">"model dir: "</span>, model_dir)</div><div class="line">    classifier = tf_learn.Estimator(model_fn=wide_and_deep, model_dir=model_dir)</div><div class="line">    classifier.fit(input_fn=pandas_input_fn(X, y), steps=steps)</div><div class="line"></div><div class="line">    <span class="keyword">return</span> classifier</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(classifier, X)</span>:</span></div><div class="line">    <span class="keyword">return</span> list(classifier.predict(input_fn=pandas_input_fn(X, num_epochs=<span class="number">1</span>),</div><div class="line">                                   as_iterable=<span class="keyword">True</span>))</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    model_dir = <span class="string">"./wnd"</span></div><div class="line">    os.system(<span class="string">"rm -rf ./wnd"</span>)</div><div class="line"></div><div class="line">    trainFile = <span class="string">'train.csv'</span></div><div class="line">    testFile = <span class="string">'test.csv'</span></div><div class="line"></div><div class="line">    data_train = pd.read_csv(trainFile, names=COLUMNS)  <span class="comment"># LOAD DATA</span></div><div class="line">    data_test = pd.read_csv(testFile, names=COLUMNS)  <span class="comment"># LOAD DATA</span></div><div class="line"></div><div class="line">    train_size = data_train.shape[<span class="number">0</span>]</div><div class="line"></div><div class="line">    X_train, y_train = process_input_df(data_train)</div><div class="line">    X_test, y_test = process_input_df(data_test)</div><div class="line"></div><div class="line">    <span class="comment"># data scale</span></div><div class="line">    data_continuous = pd.concat([X_train[CONTINUOUS_COLUMNS], X_test[CONTINUOUS_COLUMNS]])</div><div class="line">    scaler = StandardScaler()</div><div class="line">    data_continuous_scale = scaler.fit_transform(data_continuous)</div><div class="line">    X_train[CONTINUOUS_COLUMNS] = pd.DataFrame(data_continuous_scale[:train_size])</div><div class="line">    X_test[CONTINUOUS_COLUMNS] = pd.DataFrame(data_continuous_scale[train_size:])</div><div class="line"></div><div class="line">    classifier = train(X_train, y_train, steps=<span class="number">5000</span>)</div><div class="line">    pred = predict(classifier, X_test)</div><div class="line">    print(<span class="string">"auc"</span>, roc_auc_score(y_test, np.array(pred)))</div></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.png"
                alt="Nirvanada" />
            
              <p class="site-author-name" itemprop="name">Nirvanada</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">22</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Nirvanada</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
