<!doctype html>



  


<html class="theme-next muse use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.1" />






<meta name="description" content="引言矩阵分解可以视为无监督学习的一种形式，成名于Netflix Prize推荐比赛，由Yehuda Koren引申出诸多变种，在推荐系统中可谓是简单粗暴，非常有效。
矩阵分解，顾名思义，就是把已有的目标矩阵分解为两个矩阵相乘的形式，那么目标矩阵是什么？在推荐系统里一般我们可以理解为是用户（user）-商品（item）-评分（rating）所构成的矩阵。矩阵的每一行代表一个user，每一列代表ite">
<meta property="og:type" content="article">
<meta property="og:title" content="矩阵分解：从入门到高级入门">
<meta property="og:url" content="https://nirvanada.github.io/2017/03/01/mf/index.html">
<meta property="og:site_name" content="Andante">
<meta property="og:description" content="引言矩阵分解可以视为无监督学习的一种形式，成名于Netflix Prize推荐比赛，由Yehuda Koren引申出诸多变种，在推荐系统中可谓是简单粗暴，非常有效。
矩阵分解，顾名思义，就是把已有的目标矩阵分解为两个矩阵相乘的形式，那么目标矩阵是什么？在推荐系统里一般我们可以理解为是用户（user）-商品（item）-评分（rating）所构成的矩阵。矩阵的每一行代表一个user，每一列代表ite">
<meta property="og:updated_time" content="2017-03-01T15:24:08.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="矩阵分解：从入门到高级入门">
<meta name="twitter:description" content="引言矩阵分解可以视为无监督学习的一种形式，成名于Netflix Prize推荐比赛，由Yehuda Koren引申出诸多变种，在推荐系统中可谓是简单粗暴，非常有效。
矩阵分解，顾名思义，就是把已有的目标矩阵分解为两个矩阵相乘的形式，那么目标矩阵是什么？在推荐系统里一般我们可以理解为是用户（user）-商品（item）-评分（rating）所构成的矩阵。矩阵的每一行代表一个user，每一列代表ite">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Muse',
    sidebar: {"position":"right","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: '博主'
    }
  };
</script>




  <link rel="canonical" href="https://nirvanada.github.io/2017/03/01/mf/"/>

  <title> 矩阵分解：从入门到高级入门 | Andante </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-right page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">Andante</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle"></p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                矩阵分解：从入门到高级入门
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-03-01T21:48:42+08:00" content="2017-03-01">
              2017-03-01
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>矩阵分解可以视为无监督学习的一种形式，成名于Netflix Prize推荐比赛，由Yehuda Koren引申出诸多变种，在推荐系统中可谓是简单粗暴，非常有效。</p>
<p>矩阵分解，顾名思义，就是把已有的目标矩阵分解为两个矩阵相乘的形式，那么目标矩阵是什么？在推荐系统里一般我们可以理解为是用户（user）-商品（item）-评分（rating）所构成的矩阵。矩阵的每一行代表一个user，每一列代表item，矩阵元素$r_{ij}$代表第i个user为第j个item打得分数，可以是浏览数、加购数、购买数、评论数等等，如果第i个user对第j个item没有任何行为，我们令该矩阵元素为空。而我们的目标就是将这些空元素补齐，用以推断每个user对没有行为的这些item的兴趣度大小究竟是多少，从而找到用户可能最感兴趣的top-K个商品进行推荐。</p>
<h3 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h3><p>假设矩阵$R$维度为$M  \times N$，也就是用户为$M$个，item为$N$个，目标是将$R$分解为$X \cdot Y$的形式，其中$X$维度为$M  \times K$，$Y$维度为$K \times N$，$K &lt;&lt; M,N$，这里的$K$称为隐因子维度。我们可以把user的隐因子解释为用户的兴趣维度，把item的隐因子解释为商品的属性维度，分解出的结果同样也可以基于cosine similarity计算用户之间的相似度和商品之间的相似度。其实传统的SVD同样也是这种思想，但是由于这里我们的场景有大量的缺失值SVD无法操作，盲目的填补缺失值也会造成模型的过拟合，因此也衍生出了大量诸如SVD++，RSVD等SVD加强版。事实上最基础和原始的矩阵分解优化目标形式非常简洁，思想就是尽可能的可以复原出已有矩阵中非空元素，使预测元素与原始元素保持一致，即</p>
<script type="math/tex; mode=display">
min \sum_{u,i \in ratings} (r_{ui} - \hat r_{ui})^2 = min \sum (r_{ui} - x_u^T y_i)^2</script><p>当然，还少不了regulation term，以防止overfitting，这样目标就变得复杂一点</p>
<script type="math/tex; mode=display">
min \sum (r_{ui} - x_u^T y_i)^2 + \lambda(\sum_{u} \parallel x_u\parallel ^2  + \sum_{i} \parallel y_i \parallel^2 )</script><h4 id="Bias"><a href="#Bias" class="headerlink" title="Bias"></a>Bias</h4><p>有了这个基础版本的目标函数，其实就可以在上面进行修修改改，逐步完善。考虑到在实际推荐系统中user和item的多样性，我们还可以在此基础上加上user、item的bias项。举个直观例子来解释这个概念，假如现在要预测用户A对商品a的打分，已经知道的是在所有的商品里均值得分为4分，而a相比所有商品来说质量和口碑要相对好一些，因此均值要高出1分，但用户A又是一个比较较真和挑剔的用户，所以他对每个商品的打分要比所有人打分均值低0.5分，因此如果暂且不考虑兴趣因素，最终A对a的打分可以粗略估计为4+1-0.5=4.5，这里这些均值偏移数据就是所谓的bias项修正。如此目标就又复杂一些</p>
<script type="math/tex; mode=display">
min \sum (r_{ui} - \beta_u - \gamma_i - x_u^T y_i)^2 + \lambda(\sum_{u} (\parallel x_u\parallel ^2  + \beta_u^2)+ \sum_{i} (\parallel y_i \parallel^2 + \gamma_i^2)</script><h4 id="Implicit-Feedback"><a href="#Implicit-Feedback" class="headerlink" title="Implicit Feedback"></a>Implicit Feedback</h4><p>上面说了一大堆，但是有比较核心的一点被忽略了，那就是在现实的应用场景中，“rating”，也就是打分这个概念我们几乎是获取不到的。这相当于是用户给出最积极的反馈，比如豆瓣电影的主动打分，但是大多数用户在网站网行的行为也不过是点击、浏览，在电商场景中可能更多一些，比如关注、加购等，在一些音乐、影视平台上可能有收听、观看和评论等。但总的说来，这些都不能够让我们具体的概化为一个“打分”，或者称为显示反馈，所以需要引入隐式反馈这个概念来进一步刻画矩阵分解的优化目标。</p>
<p>首先引入一个二值变量，定义</p>
<script type="math/tex; mode=display">
p_{ui}= \begin{cases} 1, & \text {if $r_{ui}$ > 0} \\ 0, & \text{if $r_{ui}$ = 0} \end{cases}</script><p>这里的$r_{ui}$就不再是打分数据了，而是用户对商品有过任意行为的次数，而二值变量可以理解为一个用户对一个商品有过是/否有过任何行为。接下来优化目标也随之改变</p>
<script type="math/tex; mode=display">
min \sum c_{ui} (p_{ui} - \beta_u - \gamma_i - x_u^T y_i)^2 + \lambda(\sum_{u} (\parallel x_u\parallel ^2  + \beta_u^2)+ \sum_{i} (\parallel y_i \parallel^2 + \gamma_i^2)</script><p>，其中$c_{ui}$称为置信度，通常定义为</p>
<script type="math/tex; mode=display">
c_{ui} = 1 + \alpha r_{ui}</script><p>$\alpha$与$\gamma$同样都是hyper parameter，需要交叉验证来确定，对置信度定性的解释也就是用户对指定商品的行为越多，我们就越有理由认为用户对这个商品是真正感兴趣的，比较类似于统计检验中置信度的概念。设想我们在网购时买一个自己喜欢的东西基本上都要看来看去好多次甚至好多天，但如果是帮人买，多数情况下的行为是与这个商品只接触一次，买完即走。更generalized的想法是，把$c_{ui}$解释为用户对商品的行为权重，对电商而言，如果仅仅浏览过，权重最小，浏览次数大于一定阈值时，权重次之，加购过，权重再次加大，购买过，权重最大。</p>
<p>如果考虑得再复杂一点，可以把$x_u^T y_i$更加具体化，比如加上用户的属性特征$U$，有过行为的商品embedding归一化特征$I$，扩充用户的隐因子矩阵$x$，就变为</p>
<script type="math/tex; mode=display">
(x_u + |N(u)|^{-0.5}\sum_{i \in N(u)}I_i + \sum U_{\alpha})^Ty_i</script><p>对于$y_i$，如果有何必要的话也可以同样进行这种扩充方式处理，从而解决一些数据不足或冷启动的问题。</p>
<h3 id="学习算法"><a href="#学习算法" class="headerlink" title="学习算法"></a>学习算法</h3><p>最小化目标函数的方法主要有梯度下降法和交替最小二乘法（Alternative Least Square, ALS），接下来分别简要说明一下。</p>
<h4 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h4><p>其实严格的来说，矩阵分解的梯度下降应该叫做随机梯度下降（逐个元素更新）。先看最基础的，定义损失函数</p>
<script type="math/tex; mode=display">
e_{ij}^2 = (r_{ij} - \hat r_{ij})^2 = (r_{ij} - \sum_kx_{ik} y_{kj})^2</script><p>，待更新参数为$x<em>{ik}$和$y</em>{kj}$，每一步梯度更新后两个参数分别为</p>
<script type="math/tex; mode=display">
x_{ik}^‘ = x_{ik} - \eta \frac{\partial}{\partial x_{ik}} e_{ij}^2 =  x_{ik} + 2\eta (r_{ij} - \hat r_{ij})y_{kj} =  x_{ik} + 2\eta e_{ij} y_{kj}\\
y_{kj}^‘ = y_{kj} - \eta \frac{\partial}{\partial y_{kj}} e_{ij}^2 =  y_{kj} + 2\eta (r_{ij} - \hat r_{ij})x_{ik} =  y_{kj} + 2\eta e_{ij} x_{ik}</script><p>加上正则项后，形式类似，</p>
<script type="math/tex; mode=display">
x_{ik}^‘ =  x_{ik} + 2\eta (e_{ij} y_{kj} - \lambda x_{ik})\\
y_{kj}^‘ =  y_{kj} + 2\eta (e_{ij} x_{ik} - \lambda y_{ij})</script><p>这里有一点需要注意，并不是$X$和$Y$中的所有元素都需要按顺序依次更新，因为目标矩阵的稀疏性导致大量rating值缺失，所以只需要更新那些$e_{ij}$不为空对应的user和item矩阵元素。</p>
<h5 id="python的简单实现"><a href="#python的简单实现" class="headerlink" title="python的简单实现"></a>python的简单实现</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">matrix_factorization</span><span class="params">(R, X, Y, rank, iter, lr = <span class="number">0.01</span>, gamma =<span class="number">0.02</span>)</span>:</span></div><div class="line">    Y = Y.T</div><div class="line">    <span class="comment"># update user-item latent factor</span></div><div class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> xrange(iter):</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> xrange(R.shape[<span class="number">0</span>]):</div><div class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> xrange(R.shape[<span class="number">1</span>]):</div><div class="line">                <span class="keyword">if</span> R[i][j] &gt; <span class="number">0</span>:</div><div class="line">                    eij = R[i][j] - numpy.dot(X[i,:],Y[:,j])</div><div class="line">                    <span class="keyword">for</span> k <span class="keyword">in</span> xrange(rank):</div><div class="line">                        X[i][k] = X[i][k] + <span class="number">2</span> * lr * (eij * Y[k][j] - gamma * X[i][k])</div><div class="line">                        Y[k][j] = Y[k][j] + <span class="number">2</span> * lr * (eij * X[i][k] - gamma * Y[k][j])</div><div class="line">    <span class="keyword">return</span> X, Y.T</div><div class="line"></div><div class="line"></div><div class="line">R = numpy.array([[<span class="number">5</span>,<span class="number">3</span>,<span class="number">0</span>,<span class="number">1</span>],</div><div class="line">     [<span class="number">4</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>],</div><div class="line">     [<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">5</span>],</div><div class="line">     [<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">4</span>],</div><div class="line">     [<span class="number">0</span>,<span class="number">1</span>,<span class="number">5</span>,<span class="number">4</span>]])</div><div class="line">rank = <span class="number">2</span> </div><div class="line">X = numpy.random.rand(R.shape[<span class="number">0</span>], rank)</div><div class="line">Y = numpy.random.rand(R.shape[<span class="number">1</span>], rank)</div><div class="line">X_new, Y_new = matrix_factorization(R, X, Y, rank, <span class="number">1000</span>)</div><div class="line">R_new = numpy.dot(X_new, Y_new.T)</div><div class="line"><span class="keyword">print</span> <span class="string">"predict matrix:\n"</span>, R_new	</div><div class="line"><span class="keyword">print</span> <span class="string">"raw matrix:\n"</span>, R</div><div class="line">R = R.reshape(<span class="number">-1</span>)</div><div class="line">R_new = R_new.reshape(<span class="number">-1</span>)</div><div class="line">mse = <span class="number">0</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> xrange(R.size):</div><div class="line">    <span class="keyword">if</span> R[i] &gt; <span class="number">0</span>:</div><div class="line">        mse += pow(R[i] - R_new[i], <span class="number">2</span>) </div><div class="line"></div><div class="line"><span class="keyword">print</span> <span class="string">"MSE: "</span>, mse/R.size</div></pre></td></tr></table></figure>
<p>输出结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">predict matrix:</div><div class="line">[[ 4.95397969  2.96469808  4.39008599  1.00365562]</div><div class="line"> [ 3.96264341  2.38721232  3.71827269  1.00055567]</div><div class="line"> [ 1.00646214  0.98090633  5.85147223  4.94890212]</div><div class="line"> [ 0.99598939  0.8966756   4.82086649  3.96964276]</div><div class="line"> [ 1.2039566   1.01871901  4.97353667  3.98151943]]</div><div class="line">raw matrix:</div><div class="line">[[5 3 0 1]</div><div class="line"> [4 0 0 1]</div><div class="line"> [1 1 0 5]</div><div class="line"> [1 0 0 4]</div><div class="line"> [0 1 5 4]]</div><div class="line">MSE:  0.000506024533095</div></pre></td></tr></table></figure>
<h4 id="ALS"><a href="#ALS" class="headerlink" title="ALS"></a>ALS</h4><p>从目标函数$e_{ij}^2$的形式上可以看出来，这是个non-convex function，但却是bi-convex的。也就是说，在已知$x$的情况下，就是个关于$y$的quadratic形式的function，同理在已知$y$的情况下，就是个关于$x$的quadratic形式的function，而quadratic形式是一定能找到全局最优的。针对这种问题，我们就可以固定其中一个变量来更新另一个变量，然后做一次相反的操作，迭代多次，就可以获得最下二乘下的优化解，所以这种方法就称之为交替最小二乘。正是由于在每一次迭代都可以获得真正意义的全局最优，而非梯度下降方法每次迭代只在梯度方向移动一小步，因此ALS在理论上要比gradient descent方法收敛快很多。</p>
<p>开始推导，这里我们只针对带正则项的explicit feedback的目标函数形式，</p>
<script type="math/tex; mode=display">
min \sum (r_{ui} - x_u^T y_i)^2 + \lambda(\sum_{u} \parallel x_u\parallel ^2  + \sum_{i} \parallel y_i \parallel^2 )</script><p>先固定$y_i$，求$x_u$，求导有</p>
<script type="math/tex; mode=display">
\frac{\partial C}{\partial x_u} = -2\sum_i (r_{ui} - x_u^T y_i)y_i +2\lambda x_u
=-2Y_{I_i}R(u, I_i) + 2Y_{I_i} Y_{I_i}^T + 2\lambda x_u</script><p>令上式等于0，直接求最优，可得$x_u$表达式</p>
<script type="math/tex; mode=display">
x_u = (\lambda I + Y_{I_i} Y_{I_i}^T)^{-1}Y_{I_i}R(u, I_i)</script><p>简单解释下各个符号含义。假设用户维度为$N$维，商品维度为$M$维，隐因子的rank为$K$维，用户$u$打过分的商品为$m$个，那么$Y_{I_i}$代表用户$u$打过分的item集合的向量堆叠，即$K \times m$维，$R(u, I_i)$用户$u$打过分的商品得分，为一个$m*1$列向量，$I$为单位向量，维度为$K \times K$，$x_u$即为用户$u$的向量表示，是一个$k \times 1$的列向量。在更新完$x_u$后，同理可以更新$y_i$，由于目标函数是对称的，就不推导了，直接上公式</p>
<script type="math/tex; mode=display">
y_i = (\lambda I + X_{I_u} X_{I_u}^T)^{-1}X_{I_u}R(i, I_u)</script><p>含义和上面的刚好相反。如果商品$i$被$n$个用户打过分，那么$X_{I_u}$代表商品i被打过分的user集合的向量堆叠，即$K \times n$维，$R(i, I_u)$为商品$i$被打过的得分，为一个$n \times 1$的列向量，$I$为单位向量，维度为$K*K$，$y_i$即为商品$i$的向量表示，是一个$k \times 1$的列向量。在实际应用里，由于矩阵的求逆操作计算量很大，通常就通过解线性方程组的方法来求$x_u$和$y_i$。</p>
<script type="math/tex; mode=display">
(\lambda I + Y_{I_i} Y_{I_i}^T) x_u = Y_{I_i}R(u, I_i) \\
(\lambda I + X_{I_u} X_{I_u}^T) y_i = X_{I_u}R(i, I_u)</script><h5 id="python的简单实现-1"><a href="#python的简单实现-1" class="headerlink" title="python的简单实现"></a>python的简单实现</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">matrix_factorization</span><span class="params">(R, rank, iter, gamma =<span class="number">0.02</span>)</span>:</span></div><div class="line">	<span class="comment">#initialization</span></div><div class="line">	X = np.random.rand(R.shape[<span class="number">0</span>], rank)</div><div class="line">	Y = np.random.rand(R.shape[<span class="number">1</span>], rank)</div><div class="line">	I = np.eye(rank)</div><div class="line">	<span class="keyword">for</span> step <span class="keyword">in</span> xrange(iter):</div><div class="line">		<span class="keyword">for</span> user, rating <span class="keyword">in</span> enumerate(R):</div><div class="line">			is_rating_index = np.nonzero(rating)</div><div class="line">			A = np.dot(Y[is_rating_index].T, Y[is_rating_index]) + gamma * I</div><div class="line">			B = np.dot(Y[is_rating_index].T, rating[is_rating_index])</div><div class="line">			<span class="comment"># using solve method instead of calculating inverse matrix</span></div><div class="line">			X[user,:] = np.linalg.solve(A, B)</div><div class="line"></div><div class="line">		<span class="keyword">for</span> item, rating <span class="keyword">in</span> enumerate(R.T):</div><div class="line">			is_rating_index = np.nonzero(rating)</div><div class="line">			A = np.dot(X[is_rating_index].T, X[is_rating_index]) + gamma * I</div><div class="line">			B = np.dot(X[is_rating_index].T, rating[is_rating_index])</div><div class="line">			Y[item,:] = np.linalg.solve(A, B)</div><div class="line"></div><div class="line">	<span class="keyword">return</span> X, Y</div><div class="line"></div><div class="line"></div><div class="line">R = np.array([[<span class="number">5</span>,<span class="number">3</span>,<span class="number">0</span>,<span class="number">1</span>],</div><div class="line">     [<span class="number">4</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>],</div><div class="line">     [<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">5</span>],</div><div class="line">     [<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">4</span>],</div><div class="line">     [<span class="number">0</span>,<span class="number">1</span>,<span class="number">5</span>,<span class="number">4</span>]])</div><div class="line">rank = <span class="number">2</span> </div><div class="line">X, Y = matrix_factorization(R, rank, <span class="number">10</span>)</div><div class="line"></div><div class="line">R_new = X.dot(Y.T)</div><div class="line"><span class="keyword">print</span> <span class="string">"predict matrix:\n"</span>, 	R_new</div><div class="line"><span class="keyword">print</span> <span class="string">"raw matrix:\n"</span>, R</div><div class="line">R = R.reshape(<span class="number">-1</span>)</div><div class="line">R_new = R_new.reshape(<span class="number">-1</span>)</div><div class="line">mse = <span class="number">0</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> xrange(R.size):</div><div class="line">    <span class="keyword">if</span> R[i] &gt; <span class="number">0</span>:</div><div class="line">        mse += pow(R[i] - R_new[i], <span class="number">2</span>) </div><div class="line"></div><div class="line"><span class="keyword">print</span> <span class="string">"MSE: "</span>, mse/R.size</div></pre></td></tr></table></figure>
<p>输出结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">predict matrix:</div><div class="line">[[ 5.0161259   2.9859823  -3.84990048  1.01292524]</div><div class="line"> [ 3.9720155   2.38887852 -2.78909332  0.9972788 ]</div><div class="line"> [ 0.96012708  1.16133008  5.5264977   4.90613676]</div><div class="line"> [ 1.04836492  1.09127496  4.15699704  3.94456546]</div><div class="line"> [ 0.50838622  0.809576    4.99343708  4.15301915]]</div><div class="line">raw matrix:</div><div class="line">[[5 3 0 1]</div><div class="line"> [4 0 0 1]</div><div class="line"> [1 1 0 5]</div><div class="line"> [1 0 0 4]</div><div class="line"> [0 1 5 4]]</div><div class="line">MSE:  0.00514865377649</div></pre></td></tr></table></figure>
<p>至此，矩阵分解的基本原理和求解方法都已经啰嗦完了，但是现实应用中其实远没有这么简单，比如关于隐反馈的定义（用户-商品打分的量化），大数据量的内存问题（spark ALS处理亿量级的用户）等。总的来说坑还是很多的，所以这篇文章也只能算是从入门到高级入门。</p>

      
    </div>

    <div>
      
        
      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/02/23/dl-trick/" rel="next" title="Deep Learning的一些tricks">
                <i class="fa fa-chevron-left"></i> Deep Learning的一些tricks
              </a>
            
          </div>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.png"
               alt="Nirvanada" />
          <p class="site-author-name" itemprop="name">Nirvanada</p>
          <p class="site-description motion-element" itemprop="description"></p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">7</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">3</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

      </section>

      
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">
            
              
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#引言"><span class="nav-number">1.</span> <span class="nav-text">引言</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#目标函数"><span class="nav-number">2.</span> <span class="nav-text">目标函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Bias"><span class="nav-number">2.1.</span> <span class="nav-text">Bias</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Implicit-Feedback"><span class="nav-number">2.2.</span> <span class="nav-text">Implicit Feedback</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#学习算法"><span class="nav-number">3.</span> <span class="nav-text">学习算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#梯度下降"><span class="nav-number">3.1.</span> <span class="nav-text">梯度下降</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#python的简单实现"><span class="nav-number">3.1.1.</span> <span class="nav-text">python的简单实现</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ALS"><span class="nav-number">3.2.</span> <span class="nav-text">ALS</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#python的简单实现-1"><span class="nav-number">3.2.1.</span> <span class="nav-text">python的简单实现</span></a></li></ol></li></ol></li></ol></div>
            
          </div>
        </section>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Nirvanada</span>
</div>

<div class="powered-by">
  Powered by <a class="theme-link" href="http://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Muse
  </a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.0.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.0.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  



  




  
  
  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
  </script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });
  </script>
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


  

  

</body>
</html>
